===========Repository Name===========
ansible-role-container-registry
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-container-registry\tox.ini
===========File Type===========
.ini
===========File Content===========
[tox]
minversion = 1.6
envlist = docs, linters
skipdist = True

[testenv]
usedevelop = True
install_command = pip install -c{env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt} {opts} {packages}
setenv = VIRTUAL_ENV={envdir}
deps = -r{toxinidir}/test-requirements.txt
whitelist_externals = bash

[testenv:bindep]
basepython = python3
# Do not install any requirements. We want this to be fast and work even if
# system dependencies are missing, since it's used to tell you what system
# dependencies are missing! This also means that bindep must be installed
# separately, outside of the requirements files.
deps = bindep
commands = bindep test

[testenv:pep8]
basepython = python3
commands =
    # Run hacking/flake8 check for all python files
    bash -c "git ls-files | grep -v releasenotes |  xargs grep --binary-files=without-match \
        --files-with-match '^.!.*python$' \
        --exclude-dir .tox \
        --exclude-dir .git \
        --exclude-dir .eggs \
        --exclude-dir *.egg-info \
        --exclude-dir dist \
        --exclude-dir *lib/python* \
        --exclude-dir doc \
        | xargs flake8 --verbose"

[testenv:ansible-lint]
basepython=python3
commands =
  bash ci-scripts/ansible-lint.sh

[testenv:linters]
basepython = python3
deps =
    -r{toxinidir}/test-requirements.txt
    -r{toxinidir}/ansible-requirements.txt
commands =
    {[testenv:pep8]commands}
    {[testenv:ansible-lint]commands}

[testenv:releasenotes]
basepython = python3
whitelist_externals = bash
commands = bash -c ci-scripts/releasenotes_tox.sh

[testenv:venv]
basepython = python3
commands = {posargs}

[flake8]
# E123, E125 skipped as they are invalid PEP-8.
# E265 deals withs paces inside of comments
show-source = True
ignore = E123,E125,E265
builtins = _




===========Repository Name===========
ansible-role-container-registry
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-container-registry\tests\inventory
===========File Type===========

===========File Content===========
localhost





===========Repository Name===========
ansible-role-container-registry
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-container-registry\tests\test.yml
===========File Type===========
.yml
===========File Content===========
- hosts: localhost
  become: true
  roles:
    - container-registry




===========Repository Name===========
ansible-role-python_venv_build
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-python_venv_build\tox.ini
===========File Type===========
.ini
===========File Content===========
[tox]
minversion = 2.0
skipsdist = True
envlist = docs,linters,functional


[testenv]
usedevelop = True
install_command =
    pip install -c{env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt} {opts} {packages}
commands =
    /usr/bin/find . -type f -name "*.pyc" -delete
passenv =
    COMMON_TESTS_PATH
    HOME
    http_proxy
    HTTP_PROXY
    https_proxy
    HTTPS_PROXY
    no_proxy
    NO_PROXY
    TESTING_BRANCH
    TESTING_HOME
    USER
whitelist_externals =
    bash
setenv =
    PYTHONUNBUFFERED=1
    ROLE_NAME=python_venv_build
    VIRTUAL_ENV={envdir}
    WORKING_DIR={toxinidir}
    # TODO(odyssey4me): remove after debugging is completed
    ANSIBLE_PARAMETERS=-v


[testenv:docs]
basepython = python3
deps = -r{toxinidir}/doc/requirements.txt
commands=
    bash -c "rm -rf doc/build"
    doc8 doc
    sphinx-build -b html doc/source doc/build/html


[doc8]
# Settings for doc8:
extensions = .rst


[testenv:releasenotes]
basepython = python3
deps = -r{toxinidir}/doc/requirements.txt
commands =
    sphinx-build -a -E -W -d releasenotes/build/doctrees -b html releasenotes/source releasenotes/build/html


# environment used by the -infra templated docs job
[testenv:venv]
basepython = python3
commands =
    {posargs}


[testenv:pep8]
basepython = python3
commands =
    bash -c "{toxinidir}/tests/common/test-pep8.sh"


[flake8]
# Ignores the following rules due to how ansible modules work in general
#     F403 'from ansible.module_utils.basic import *' used;
#          unable to detect undefined names
ignore=F403


[testenv:bashate]
commands =
    bash -c "{toxinidir}/tests/common/test-bashate.sh"


# The deps URL should be set to the appropriate git URL.
# In the tests repo itself, the variable is uniquely set to
# the toxinidir so that the role is able to test itself, but
# the tox config is exactly the same as other repositories.
#
# The value for other repositories must be:
# http://git.openstack.org/cgit/openstack/openstack-ansible-tests/plain/test-ansible-deps.txt
# or for a stable branch:
# http://git.openstack.org/cgit/openstack/openstack-ansible-tests/plain/test-ansible-deps.txt?h=stable/queens


[testenv:ansible-syntax]
commands =
    bash -c "{toxinidir}/tests/common/test-ansible-syntax.sh"


[testenv:ansible-lint]
commands =
    bash -c "{toxinidir}/tests/common/test-ansible-lint.sh"


[testenv:functional]
commands =
    bash -c "{toxinidir}/tests/common/test-ansible-functional.sh"


[testenv:linters]
basepython = python3
commands =
    bash -c "{toxinidir}/tests/common/test-ansible-env-prep.sh"
    {[testenv:pep8]commands}
    {[testenv:bashate]commands}
    {[testenv:ansible-lint]commands}
    {[testenv:ansible-syntax]commands}




===========Repository Name===========
ansible-role-python_venv_build
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-python_venv_build\tests\ansible-role-requirements.yml
===========File Type===========
.yml
===========File Content===========
- name: apt_package_pinning
  src: https://git.openstack.org/openstack/openstack-ansible-apt_package_pinning
  scm: git
  version: master
- name: pip_install
  src: https://git.openstack.org/openstack/openstack-ansible-pip_install
  scm: git
  version: master
- name: openstack_hosts
  src: https://git.openstack.org/openstack/openstack-ansible-openstack_hosts
  scm: git
  version: master
- name: lxc_hosts
  src: https://git.openstack.org/openstack/openstack-ansible-lxc_hosts
  scm: git
  version: master
- name: lxc_container_create
  src: https://git.openstack.org/openstack/openstack-ansible-lxc_container_create
  scm: git
  version: master




===========Repository Name===========
ansible-role-python_venv_build
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-python_venv_build\tests\inventory
===========File Type===========

===========File Content===========
[all]
localhost
container1
container2
container3

[all_containers]
container1
container2
container3




===========Repository Name===========
ansible-role-python_venv_build
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-python_venv_build\tests\test.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Prepare the host/containers
  import_playbook: common/test-setup-host.yml

- name: Prepare web server on localhost to serve python packages
  hosts: localhost
  connection: local
  become: yes
  any_errors_fatal: yes
  tasks:
    - name: Set venv_build_archive_path and venv_install_source_path
      set_fact:
        venv_build_host_wheel_path: >-
          {%- if ansible_distribution == "Ubuntu" %}
          {%-   set _path = "/var/www/html" %}
          {%- elif ansible_distribution == "CentOS" %}
          {%-   set _path = "/usr/share/nginx/html" %}
          {%- else %}
          {%-   set _path = "/srv/www/htdocs" %}
          {%- endif %}
          {{- _path }}

    - name: Install EPEL gpg keys
      rpm_key:
        key: "http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7"
        state: present
      when:
        - ansible_pkg_mgr in ['yum', 'dnf']
      register: _add_yum_keys
      until: _add_yum_keys  is success
      retries: 5
      delay: 2

    - name: Install the EPEL repository
      yum_repository:
        name: epel-nginx
        baseurl: "{{ (centos_epel_mirror | default ('http://download.fedoraproject.org/pub/epel')) ~ '/' ~ ansible_distribution_major_version ~ '/' ~ ansible_architecture }}"
        description: 'Extra Packages for Enterprise Linux 7 - $basearch'
        gpgcheck: yes
        enabled: yes
        state: present
        includepkgs: 'nginx*'
      when:
        - ansible_pkg_mgr in ['yum', 'dnf']
      register: install_epel_repo
      until: install_epel_repo  is success
      retries: 5
      delay: 2

    - name: Install distro packages
      package:
        name: "nginx"
        update_cache: "{{ (ansible_pkg_mgr in ['apt', 'zypper']) | ternary('yes', omit) }}"
      register: install
      until: install  is success
      retries: 5
      delay: 2

    - name: Enable and start nginx
      service:
        name: nginx
        enabled: yes
        daemon_reload: yes
        state: restarted

- name: Verify not using a build host
  hosts: "container1"
  remote_user: root
  any_errors_fatal: yes
  vars:
    venv_pip_packages:
      - "Jinja2==2.10"
    venv_install_destination_path: "/openstack/venvs/test-venv"
  tasks:

    - name: Execute venv install
      include_role:
        name: "python_venv_build"
        private: yes
      vars:
        venv_facts_when_changed:
          - section: "{{ inventory_hostname }}"
            option: "test"
            value: True

    - name: refresh local facts
      setup:
        filter: ansible_local
        gather_subset: "!all"

    - name: Show the ansible_local facts
      debug:
        var: ansible_local

    - name: Verify that the facts were set
      assert:
        that:
          - ansible_local['openstack_ansible'][inventory_hostname]['test'] | bool

    - name: Find files/folders on targets
      find:
        file_type: directory
        get_checksum: no
        recurse: no
        paths:
          - "{{ venv_install_destination_path | dirname }}"
      register: _target_folders

    - name: Compile the folder list from the targets
      set_fact:
        _target_folder_list: "{{ _target_folders['files'] | map(attribute='path') | list }}"

    - name: Show the files/folder from the targets
      debug:
        var: _target_folder_list

    - name: Verify the folder list from the targets
      assert:
        that:
          - "{{ venv_install_destination_path in _target_folder_list }}"

- name: Verify using a build host
  hosts: "container2:container3"
  remote_user: root
  any_errors_fatal: yes
  vars:
    venv_default_pip_packages:
      - "elasticsearch>=6.0.0,<7.0.0"
    venv_pip_packages:
      - "Jinja2==2.10"
    venv_install_destination_path: "/openstack/venvs/test-venv"
    venv_pip_install_args: >-
      --find-links http://{{ hostvars['localhost'].ansible_default_ipv4.address }}
      --trusted-host {{ hostvars['localhost'].ansible_default_ipv4.address }}
    venv_build_host: localhost
    venv_build_host_wheel_path: "{{ hostvars['localhost']['venv_build_host_wheel_path'] }}"
  tasks:

    - name: Execute venv install
      include_role:
        name: "python_venv_build"
        private: yes
      vars:
        venv_facts_when_changed:
          - section: "{{ inventory_hostname }}"
            option: "test"
            value: True

    - name: refresh local facts
      setup:
        filter: ansible_local
        gather_subset: "!all"

    - name: Show the ansible_local facts
      debug:
        var: ansible_local

    - name: Verify that the facts were set
      assert:
        that:
          - ansible_local['openstack_ansible'][inventory_hostname]['test'] | bool

    - name: Find files/folders on targets
      find:
        file_type: directory
        get_checksum: no
        recurse: no
        paths:
          - "{{ venv_install_destination_path | dirname }}"
      register: _target_folders

    - name: Compile the folder list from the targets
      set_fact:
        _target_folder_list: "{{ _target_folders['files'] | map(attribute='path') | list }}"

    - name: Show the files/folder from the targets
      debug:
        var: _target_folder_list

    - name: Verify the folder list from the targets
      assert:
        that:
          - "{{ venv_install_destination_path in _target_folder_list }}"




===========Repository Name===========
ansible-role-python_venv_build
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-python_venv_build\tests\group_vars\all_containers.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

container_networks:
  management_address:
    address: "{{ ansible_host }}"
    bridge: "br-mgmt"
    interface: "eth1"
    netmask: "255.255.252.0"
    type: "veth"
physical_host: localhost
properties:
  service_name: "{{ inventory_hostname }}"

# NOTE(cloudnull): The lxc-openstack AA profile for is used to ensure general
#                  container functionality typical to the integrated build.
lxc2_container_config_list:
  - 'lxc.aa_profile=lxc-openstack'

lxc3_container_config_list:
  - 'lxc.apparmor.profile=lxc-openstack'

lxc_container_config_list: "{{ lookup('pipe', 'lxc-info --version || echo 2.0.0') is version_compare('3.0.0', 'lt') | ternary(lxc2_container_config_list, lxc3_container_config_list) }}"




===========Repository Name===========
ansible-role-python_venv_build
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-python_venv_build\tests\host_vars\container1.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

ansible_host: 10.1.0.2
ansible_become: True
ansible_user: root
container_name: container1




===========Repository Name===========
ansible-role-python_venv_build
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-python_venv_build\tests\host_vars\container2.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

ansible_host: 10.1.0.3
ansible_become: True
ansible_user: root
container_name: container2




===========Repository Name===========
ansible-role-python_venv_build
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-python_venv_build\tests\host_vars\container3.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

ansible_host: 10.1.0.4
ansible_become: True
ansible_user: root
container_name: container3




===========Repository Name===========
ansible-role-python_venv_build
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-python_venv_build\tests\host_vars\localhost.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

bridges:
  - name: "br-mgmt"
    ip_addr: "10.1.0.1"

ansible_python_interpreter: "/usr/bin/python2"




===========Repository Name===========
ansible-role-systemd_mount
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_mount\tox.ini
===========File Type===========
.ini
===========File Content===========
[tox]
minversion = 2.0
skipsdist = True
envlist = docs,linters,functional


[testenv]
usedevelop = True
install_command =
    pip install -c{env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt} {opts} {packages}
commands =
    /usr/bin/find . -type f -name "*.pyc" -delete
passenv =
    COMMON_TESTS_PATH
    HOME
    http_proxy
    HTTP_PROXY
    https_proxy
    HTTPS_PROXY
    no_proxy
    NO_PROXY
    TESTING_BRANCH
    TESTING_HOME
    USER
whitelist_externals =
    bash
setenv =
    PYTHONUNBUFFERED=1
    ROLE_NAME=systemd_mount
    TEST_IDEMPOTENCE=false
    VIRTUAL_ENV={envdir}
    WORKING_DIR={toxinidir}


[testenv:docs]
basepython = python3
deps = -r{toxinidir}/doc/requirements.txt
commands=
    bash -c "rm -rf doc/build"
    doc8 doc
    sphinx-build -b html doc/source doc/build/html


[doc8]
# Settings for doc8:
extensions = .rst


[testenv:releasenotes]
basepython = python3
deps = -r{toxinidir}/doc/requirements.txt
commands =
    sphinx-build -a -E -W -d releasenotes/build/doctrees -b html releasenotes/source releasenotes/build/html


# environment used by the -infra templated docs job
[testenv:venv]
basepython = python3
commands =
    {posargs}


[testenv:pep8]
basepython = python3
commands =
    bash -c "{toxinidir}/tests/common/test-pep8.sh"


[flake8]
# Ignores the following rules due to how ansible modules work in general
#     F403 'from ansible.module_utils.basic import *' used;
#          unable to detect undefined names
ignore=F403


[testenv:bashate]
commands =
    bash -c "{toxinidir}/tests/common/test-bashate.sh"


[testenv:ansible-syntax]
commands =
    bash -c "{toxinidir}/tests/common/test-ansible-syntax.sh"


[testenv:ansible-lint]
commands =
    bash -c "{toxinidir}/tests/common/test-ansible-lint.sh"


[testenv:functional]
commands =
    bash -c "{toxinidir}/tests/common/test-ansible-functional.sh"


[testenv:linters]
basepython = python3
commands =
    bash -c "{toxinidir}/tests/common/test-ansible-env-prep.sh"
    {[testenv:pep8]commands}
    {[testenv:bashate]commands}
    {[testenv:ansible-lint]commands}
    {[testenv:ansible-syntax]commands}
    {[testenv:docs]commands}




===========Repository Name===========
ansible-role-systemd_mount
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_mount\tests\ansible-role-requirements.yaml
===========File Type===========
.yaml
===========File Content===========




===========Repository Name===========
ansible-role-systemd_mount
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_mount\tests\inventory
===========File Type===========

===========File Content===========
[all]
localhost




===========Repository Name===========
ansible-role-systemd_mount
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_mount\tests\systemd_init-overrides.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

tempest_run: yes
tempest_venv_tag: "{{ tempest_git_install_branch }}"
tempest_venv_bin: "/opt/tempest_{{ tempest_venv_tag }}/bin"
tempest_log_dir: "/var/log/"
tempest_test_whitelist:
  - tempest.scenario.test_server_basic_ops.TestServerBasicOps.test_server_basic_ops

neutron_provider_networks:
  network_types: "vxlan,flat"
  network_mappings: "flat:eth12"
  network_vxlan_ranges: "1:1000"




===========Repository Name===========
ansible-role-systemd_mount
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_mount\tests\test-create-btrfs-dev.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2017, BBC R&D
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Configure BTRFS sparse file
  hosts: localhost
  user: root
  become: true
  connection: local
  tasks:
    - name: Install BTRFS packages
      package:
        name: "{{ btrfs_package[ansible_pkg_mgr | lower] }}"
        state: present

    - name: Create base directories
      file:
        path: "/var/lib"
        state: "directory"

    - name: Create sparse file
      command: "truncate -s 1024G /var/lib/sparse-file.img"
      args:
        creates: /var/lib/sparse-file.img
      register: sparse_file

    - name: Format the sparse file
      filesystem:
        fstype: btrfs
        dev: /var/lib/sparse-file.img
      when:
        - sparse_file  is changed
  vars:
    btrfs_package:
      apt: "btrfs-tools"
      yum: "btrfs-progs"
      zypper: "btrfsprogs"




===========Repository Name===========
ansible-role-systemd_mount
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_mount\tests\test-create-nfs-dev.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2017, BBC R&D
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Create an NFS backing store
  hosts: localhost
  user: root
  become: true
  connection: local
  tasks:
    - name: Install NFS packages
      package:
        name: "{{ nfs_package[ansible_distribution.split()[0] | lower] }}"
        state: present

    - name: create the system group for nfs
      group:
        name: "nfs-user"
        gid: "10000"
        state: "present"
        system: "yes"

    - name: Create the system user for nfs
      user:
        name: "nfs-user"
        uid: "10000"
        group: "nfs-user"
        comment: "nfs-user"
        shell: "/bin/false"
        system: "yes"
        createhome: "yes"
        home: "/srv/nfs"

    - name: Create base directories
      file:
        path: "{{ item }}"
        state: "directory"
        owner: "nfs-user"
        group: "nfs-user"
      with_items:
        - "/srv/nfs/test"

    - name: Create exports file
      lineinfile:
        path: /etc/exports
        line: '{{ item }} 127.0.0.1/255.0.0.0(rw,sync,no_subtree_check,insecure,all_squash,anonuid=10000,anongid=10000)'
        owner: root
        group: root
        mode: 0644
        create: yes
      with_items:
        - "/srv/nfs/test"
      register: nfs_exportfs

    - name: Restart nfs-server
      systemd:
        daemon_reload: yes
        name: "nfs-server"
        enabled: "yes"
        state: "restarted"
      when:
        - nfs_exportfs  is changed

    - name: Export NFS
      command: exportfs -rav
      tags:
        - skip_ansible_lint
  vars:
    nfs_package:
      ubuntu: "nfs-kernel-server"
      centos: "nfs-utils"
      opensuse: "nfs-kernel-server"




===========Repository Name===========
ansible-role-systemd_mount
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_mount\tests\test-create-swap-dev.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2017, BBC R&D
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Configure swap sparse file
  hosts: localhost
  user: root
  become: true
  connection: local
  tasks:
    - name: Create swap file
      command: "dd if=/dev/zero of=/var/lib/test-swap.img bs=1M count=128"
      args:
        creates: /var/lib/test-swap.img
      register: create_swap

    - name: Format the swap file
      command: mkswap /var/lib/test-swap.img
      failed_when: false
      when:
        - create_swap  is changed




===========Repository Name===========
ansible-role-systemd_mount
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_mount\tests\test.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
# Copyright 2018, Logan Vig <logan2211@gmail.com>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- import_playbook: test-create-swap-dev.yml

- import_playbook: test-create-nfs-dev.yml

- import_playbook: test-create-btrfs-dev.yml

- name: Playbook for role testing
  hosts: localhost
  connection: local
  user: root
  become: true
  roles:
    - role: "systemd_mount"

  post_tasks:
    - name: Ensure mount are mounted
      command: grep -w '{{ item }}' /proc/mounts
      with_items:
        - /var/lib/sparse-file
        - /var/lib/test
      tags:
        - skip_ansible_lint

    - name: Ensure swap is enabled
      shell: swapon | grep -w '/var/lib/test-swap.img'
      tags:
        - skip_ansible_lint

  vars:
    systemd_mounts:
      - what: '/var/lib/sparse-file.img'
        where: '/var/lib/sparse-file'
        type: 'btrfs'
        options: 'loop'
        state: 'started'
        enabled: true
        config_overrides:
          Unit:
            ConditionPathExists: '/var/lib/sparse-file.img'

      - what: "/var/lib/test-swap.img"
        priority: "0"
        options: "%%"
        type: "swap"
        state: 'started'
        enabled: true

      - what: "127.0.0.1:/srv/nfs/test"
        where: "/var/lib/test"
        type: "nfs"
        options: "_netdev,auto"
        state: 'started'
        enabled: true
        config_overrides:
          Unit:
            After:
              ? network.target
              ? network-online.target
            Wants: network-online.target




===========Repository Name===========
ansible-role-systemd_mount
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_mount\tests\group_vars\all_containers.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

container_networks:
  management_address:
    address: "{{ ansible_host }}"
    bridge: "br-mgmt"
    interface: "eth1"
    netmask: "255.255.255.0"
    type: "veth"

physical_host: localhost
properties:
  service_name: "{{ inventory_hostname }}"




===========Repository Name===========
ansible-role-systemd_mount
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_mount\tests\host_vars\localhost.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

neutron_provider_networks:
  network_types: "vxlan,flat"
  network_mappings: "flat:eth12"
  network_vxlan_ranges: "1:1000"

neutron_local_ip:  10.1.2.1

ansible_python_interpreter: "/usr/bin/python2"

bridges:
  - name: "br-mgmt"
    ip_addr: "10.1.1.1"




===========Repository Name===========
ansible-role-systemd_networkd
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_networkd\tox.ini
===========File Type===========
.ini
===========File Content===========
[tox]
minversion = 2.0
skipsdist = True
envlist = docs,linters,functional


[testenv]
usedevelop = True
install_command =
    pip install -c{env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt} {opts} {packages}
commands =
    /usr/bin/find . -type f -name "*.pyc" -delete
passenv =
    COMMON_TESTS_PATH
    HOME
    http_proxy
    HTTP_PROXY
    https_proxy
    HTTPS_PROXY
    no_proxy
    NO_PROXY
    TESTING_BRANCH
    TESTING_HOME
    USER
whitelist_externals =
    bash
setenv =
    PYTHONUNBUFFERED=1
    ROLE_NAME=systemd_networkd
    TEST_IDEMPOTENCE=false
    VIRTUAL_ENV={envdir}
    WORKING_DIR={toxinidir}


[testenv:docs]
basepython = python3
deps = -r{toxinidir}/doc/requirements.txt
commands=
    bash -c "rm -rf doc/build"
    doc8 doc
    sphinx-build -b html doc/source doc/build/html


[doc8]
# Settings for doc8:
extensions = .rst


[testenv:releasenotes]
basepython = python3
deps = -r{toxinidir}/doc/requirements.txt
commands =
    sphinx-build -a -E -W -d releasenotes/build/doctrees -b html releasenotes/source releasenotes/build/html


# environment used by the -infra templated docs job
[testenv:venv]
basepython = python3
commands =
    {posargs}


[testenv:pep8]
basepython = python3
commands =
    bash -c "{toxinidir}/tests/common/test-pep8.sh"


[flake8]
# Ignores the following rules due to how ansible modules work in general
#     F403 'from ansible.module_utils.basic import *' used;
#          unable to detect undefined names
ignore=F403


[testenv:bashate]
basepython = python3
commands =
    bash -c "{toxinidir}/tests/common/test-bashate.sh"


[testenv:ansible-syntax]
basepython = python3
commands =
    bash -c "{toxinidir}/tests/common/test-ansible-syntax.sh"


[testenv:ansible-lint]
basepython = python3
commands =
    bash -c "{toxinidir}/tests/common/test-ansible-lint.sh"


[testenv:functional]
commands =
    bash -c "{toxinidir}/tests/common/test-ansible-functional.sh"


[testenv:linters]
basepython = python3
commands =
    bash -c "{toxinidir}/tests/common/test-ansible-env-prep.sh"
    {[testenv:pep8]commands}
    {[testenv:bashate]commands}
    {[testenv:ansible-lint]commands}
    {[testenv:ansible-syntax]commands}
    {[testenv:docs]commands}




===========Repository Name===========
ansible-role-systemd_networkd
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_networkd\tests\ansible-role-requirements.yaml
===========File Type===========
.yaml
===========File Content===========




===========Repository Name===========
ansible-role-systemd_networkd
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_networkd\tests\inventory
===========File Type===========

===========File Content===========
[all]
localhost




===========Repository Name===========
ansible-role-systemd_networkd
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_networkd\tests\systemd_init-overrides.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

tempest_run: yes
tempest_venv_tag: "{{ tempest_git_install_branch }}"
tempest_venv_bin: "/opt/tempest_{{ tempest_venv_tag }}/bin"
tempest_log_dir: "/var/log/"
tempest_test_whitelist:
  - tempest.scenario.test_server_basic_ops.TestServerBasicOps.test_server_basic_ops

neutron_provider_networks:
  network_types: "vxlan,flat"
  network_mappings: "flat:eth12"
  network_vxlan_ranges: "1:1000"




===========Repository Name===========
ansible-role-systemd_networkd
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_networkd\tests\test.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
# Copyright 2018, Logan Vig <logan2211@gmail.com>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Playbook for role testing
  hosts: localhost
  connection: local
  become: true
  gather_facts: true
  roles:
    - role: "systemd_networkd"
  vars:
    systemd_run_networkd: yes
    systemd_resolved:
      DNS: "208.67.222.222"
      FallbackDNS: "8.8.8.8"
      Cache: yes
    systemd_netdevs:
      - NetDev:
          Name: dummy0
          Kind: dummy
      - NetDev:
          Name: dummy1
          Kind: dummy
      - NetDev:
          Name: bond0
          Kind: bond
        Bond:
          Mode: 802.3ad
          TransmitHashPolicy: layer3+4
          MIIMonitorSec: 1s
          LACPTransmitRate: fast
      - NetDev:
          Name: br-dummy
          Kind: bridge
      - NetDev:
          Name: dummy2
          Kind: dummy
      - NetDev:
          Name: br-test
          Kind: bridge
    systemd_networks:
      - interface: "dummy0"
        bond: "bond0"
        mtu: 9000
      - interface: "dummy1"
        bond: "bond0"
        mtu: 9000
      - interface: "bond0"
        bridge: "br-dummy"
        mtu: 9000
      - interface: "br-dummy"
        address: "10.0.0.100"
        netmask: "255.255.255.0"
        gateway: "10.0.0.1"
        mtu: 9000
        usedns: true
        static_routes:
          - gateway: "10.1.0.1"
            cidr: "10.1.0.0/24"
        config_overrides:
          Network:
            ConfigureWithoutCarrier: true
      - interface: "dummy2"
        bridge: "br-test"
      - interface: "br-test"
        address: "10.1.0.1"
        netmask: "255.255.255.0"


- name: Test networkd
  hosts: localhost
  connection: local
  become: true
  gather_facts: true
  tasks:
    - name: Interface check
      assert:
        that:
          - ansible_dummy0['active'] == true
          - ansible_dummy0['type'] == 'ether'
          - ansible_dummy0['mtu'] == 9000
          - ansible_dummy1['active'] == true
          - ansible_dummy1['type'] == 'ether'
          - ansible_dummy1['mtu'] == 9000
          - ansible_dummy2['active'] == true
          - ansible_dummy2['type'] == 'ether'
    - name: Bond check
      assert:
        that:
          - ansible_bond0['active'] == true
          - ansible_bond0['type'] == 'bonding'
          - ansible_bond0['mtu'] == 9000
    - name: Bridge check
      assert:
        that:
          - ansible_br_dummy['active'] == true
          - ansible_br_dummy['type'] == 'bridge'
          - ansible_br_dummy['ipv4']['address'] == '10.0.0.100'
          - ansible_br_dummy['ipv4']['netmask'] == '255.255.255.0'
    - name: Bridge check
      assert:
        that:
          - ansible_br_test['active'] == true
          - ansible_br_test['type'] == 'bridge'
          - ansible_br_test['ipv4']['address'] == '10.1.0.1'
          - ansible_br_test['ipv4']['netmask'] == '255.255.255.0'


- name: Playbook for role testing with cleanup
  hosts: localhost
  connection: local
  become: true
  gather_facts: true
  roles:
    - role: "systemd_networkd"
  post_tasks:
    - name: Interface check
      assert:
        that:
          - ansible_br_test is defined
          - ansible_dummy2['active'] == true
          - ansible_dummy2['type'] == 'ether'
    - name: Bridge check
      assert:
        that:
          - ansible_br_test['active'] == true
          - ansible_br_test['type'] == 'bridge'
          - ansible_br_test['ipv4']['address'] == '10.1.0.1'
          - ansible_br_test['ipv4']['netmask'] == '255.255.255.0'
  vars:
    systemd_interface_cleanup: true
    systemd_run_networkd: yes
    systemd_netdevs:
      - NetDev:
          Name: dummy2
          Kind: dummy
      - NetDev:
          Name: br-test
          Kind: bridge
    systemd_networks:
      - interface: "dummyX"
        bridge: "br-test"
      - interface: "br-test"
        address: "10.1.0.1"
        netmask: "255.255.255.0"




===========Repository Name===========
ansible-role-systemd_networkd
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_networkd\tests\group_vars\all_containers.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

container_networks:
  management_address:
    address: "{{ ansible_host }}"
    bridge: "br-mgmt"
    interface: "eth1"
    netmask: "255.255.255.0"
    type: "veth"

physical_host: localhost
properties:
  service_name: "{{ inventory_hostname }}"




===========Repository Name===========
ansible-role-systemd_networkd
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-systemd_networkd\tests\host_vars\localhost.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

neutron_provider_networks:
  network_types: "vxlan,flat"
  network_mappings: "flat:eth12"
  network_vxlan_ranges: "1:1000"

neutron_local_ip:  10.1.2.1

ansible_python_interpreter: "/usr/bin/python2"

bridges:
  - name: "br-mgmt"
    ip_addr: "10.1.1.1"




===========Repository Name===========
ansible-role-tripleo-modify-image
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ansible-role-tripleo-modify-image\tox.ini
===========File Type===========
.ini
===========File Content===========
[tox]
minversion = 2.0
envlist = docs, linters
skipdist = True

[testenv]
usedevelop = True
install_command = pip install -c{env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt} {opts} {packages}
setenv = VIRTUAL_ENV={envdir}
deps = -r{toxinidir}/test-requirements.txt
whitelist_externals = bash

[testenv:bindep]
basepython = python3
# Do not install any requirements. We want this to be fast and work even if
# system dependencies are missing, since it's used to tell you what system
# dependencies are missing! This also means that bindep must be installed
# separately, outside of the requirements files.
deps = bindep
commands = bindep test

[testenv:pep8]
basepython = python3
commands =
    # Run hacking/flake8 check for all python files
    bash -c "git ls-files | grep -v releasenotes |  xargs grep --binary-files=without-match \
        --files-with-match '^.!.*python$' \
        --exclude-dir .tox \
        --exclude-dir .git \
        --exclude-dir .eggs \
        --exclude-dir *.egg-info \
        --exclude-dir dist \
        --exclude-dir *lib/python* \
        --exclude-dir doc \
        | xargs flake8 --verbose"

[testenv:ansible-lint]
basepython=python3
commands =
  bash ci-scripts/ansible-lint.sh

[testenv:linters]
basepython = python3
deps =
    -r{toxinidir}/test-requirements.txt
    -r{toxinidir}/ansible-requirements.txt
commands =
    {[testenv:pep8]commands}
    {[testenv:ansible-lint]commands}

[testenv:releasenotes]
basepython = python3
whitelist_externals = bash
commands = bash -c ci-scripts/releasenotes_tox.sh

[testenv:venv]
basepython = python3
commands = {posargs}

[flake8]
# E123, E125 skipped as they are invalid PEP-8.
# E265 deals with spaces inside of comments
show-source = True
ignore = E123,E125,E265
builtins = _




===========Repository Name===========
bifrost
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\bifrost\tox.ini
===========File Type===========
.ini
===========File Content===========
[tox]
minversion = 2.0
envlist = py35,py27,docs,pep8
skipsdist = True

[testenv]
usedevelop = True
install_command = pip install -U -c{env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt} {opts} {packages}
setenv =
   VIRTUAL_ENV={envdir}
   PYTHONWARNINGS=default::DeprecationWarning
deps = -r{toxinidir}/requirements.txt
       -r{toxinidir}/test-requirements.txt
commands = python setup.py test --slowest --testr-args='{posargs}'

[testenv:pep8]
basepython = python3
commands = flake8
           doc8 doc/source releasenotes/source README.rst CONTRIBUTING.rst MISSION.rst HACKING.rst

[testenv:venv]
basepython = python3
deps =
  -r{toxinidir}/test-requirements.txt
  -r{toxinidir}/doc/requirements.txt
commands = {posargs}

[testenv:cover]
basepython = python3
commands = python setup.py test --coverage --testr-args='{posargs}'

[testenv:docs]
basepython = python3
deps =
  -c{env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt}
  -r{toxinidir}/requirements.txt
  -r{toxinidir}/doc/requirements.txt
commands = sphinx-build -W -b html doc/source doc/build/html

[testenv:debug]
basepython = python3
commands = oslo_debug_helper -t bifrost/tests {posargs}

[testenv:releasenotes]
basepython = python3
deps =
  -c{env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt}
  -r{toxinidir}/doc/requirements.txt
commands = sphinx-build -a -E -d releasenotes/build/doctrees -b html releasenotes/source releasenotes/build/html

[testenv:debug-py27]
basepython = python2.7
commands = oslo_debug_helper -t bifrost/tests {posargs}

[testenv:debug-py35]
basepython = python3.5
commands = oslo_debug_helper -t bifrost/tests {posargs}

[flake8]
show-source = True
ignore = F403,H102,H303
exclude=.venv,.git,.tox,dist,doc,*lib/python*,*egg,build,os_ironic.py,os_ironic_node.py,os_ironic_inspect.py,os_keystone_service.py

[testenv:lower-constraints]
basepython = python3
deps =
  -c{toxinidir}/lower-constraints.txt
  -r{toxinidir}/test-requirements.txt
  -r{toxinidir}/requirements.txt




===========Repository Name===========
bifrost
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\bifrost\bifrost\tests\base.py
===========File Type===========
.py
===========File Content===========
# -*- coding: utf-8 -*-

# Copyright 2010-2011 OpenStack Foundation
# Copyright (c) 2013 Hewlett-Packard Development Company, L.P.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.


import testtools


class TestCase(testtools.TestCase):
    """Test case base class for all unit tests."""

    def setUp(self):
        super(TestCase, self).setUp()




===========Repository Name===========
bifrost
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\bifrost\bifrost\tests\utils.py
===========File Type===========
.py
===========File Content===========
# Copyright (c) 2015 Hewlett-Packard Development Company, L.P.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import contextlib
import tempfile

from bifrost import inventory


@contextlib.contextmanager
def temporary_file(file_data):
    file = None
    file = tempfile.NamedTemporaryFile(mode='w')
    file.write(file_data)
    file.flush()

    try:
        yield file.name
    finally:
        if file is not None:
            file.close()


def bifrost_csv_conversion(csv_data):
    # TODO(TheJulia): To call prep feels like a bug and should be fixed.
    (groups, hostvars) = inventory._prepare_inventory()
    with temporary_file(csv_data) as file:
        (groups, hostvars) = inventory._process_baremetal_csv(
            file,
            groups,
            hostvars)
    # NOTE(TheJulia): Returning separately so the file is closed first
    return (groups, hostvars)


def bifrost_data_conversion(data):
    (groups, hostvars) = inventory._prepare_inventory()
    with temporary_file(data) as file:
        (groups, hostvars) = inventory._process_baremetal_data(
            file,
            groups,
            hostvars)
    return (groups, hostvars)




===========Repository Name===========
bifrost
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\bifrost\bifrost\tests\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
bifrost
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\bifrost\bifrost\tests\functional\test_inventory_functional.py
===========File Type===========
.py
===========File Content===========
# -*- coding: utf-8 -*-

# Copyright (c) 2015 Hewlett-Packard Development Company, L.P.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import yaml

from bifrost.tests import base
from bifrost.tests import utils


class TestBifrostInventoryFunctional(base.TestCase):

    def setUp(self):
        self.maxDiff = None
        super(TestBifrostInventoryFunctional, self).setUp()

    def test_csv_file_conversion_multiline_general(self):
        # NOTE(TheJulia): While this is a massive amount of input
        # and resulting output that is parsed as part of this
        # and similar tests, we need to ensure consistency,
        # and that is largely what this test is geared to ensure.
        CSV = """00:01:02:03:04:05,root,undefined,192.0.2.2,1,8192,512,
unused,,00000000-0000-0000-0000-000000000001,hostname0,
192.168.1.2,,,,|
00:01:02:03:04:06,root,undefined,192.0.2.3,2,8192,1024,
unused,,00000000-0000-0000-0000-000000000002,hostname1,
192.168.1.3,,,,,ipmi""".replace('\n', '').replace('|', '\n')
        expected_hostvars = """{"hostname1":
 {"uuid": "00000000-0000-0000-0000-000000000002", "driver": "ipmi",
 "name": "hostname1", "ipv4_address": "192.168.1.3",
 "provisioning_ipv4_address": "192.168.1.3" ,"ansible_ssh_host":
 "192.168.1.3", "driver_info": {"power": {"ipmi_address": "192.0.2.3",
 "ipmi_password": "undefined", "ipmi_username": "root",
 "ipmi_target_address": null, "ipmi_target_channel": null,
 "ipmi_transit_address": null, "ipmi_transit_channel": null}}, "nics":
 [{"mac": "00:01:02:03:04:06"}], "properties": {"ram": "8192", "cpu_arch":
 "x86_64", "disk_size": "1024", "cpus": "2"}, "host_groups": ["baremetal"]},
 "hostname0":
 {"uuid": "00000000-0000-0000-0000-000000000001", "driver": "ipmi",
 "name": "hostname0", "ipv4_address": "192.168.1.2",
 "provisioning_ipv4_address": "192.168.1.2", "ansible_ssh_host":
 "192.168.1.2", "driver_info": {"power": {"ipmi_address": "192.0.2.2",
 "ipmi_password": "undefined", "ipmi_username": "root",
 "ipmi_target_address": null, "ipmi_target_channel": null,
 "ipmi_transit_address": null, "ipmi_transit_channel": null}}, "nics":
 [{"mac": "00:01:02:03:04:05"}], "properties": {"ram": "8192",
 "cpu_arch": "x86_64", "disk_size": "512", "cpus": "1"},
 "host_groups": ["baremetal"]}}""".replace('\n', '')
        expected_groups = """{"baremetal": {"hosts": ["hostname0",
 "hostname1"]}, "localhost": {"hosts": ["127.0.0.1"]}}""".replace('\n', '')

        (groups, hostvars) = utils.bifrost_csv_conversion(CSV)
        self.assertDictEqual(json.loads(str(expected_hostvars)), hostvars)
        self.assertDictEqual(json.loads(expected_groups), groups)

    def test_csv_file_conversion_ipmi_dual_bridging(self):
        CSV = """00:01:02:03:04:06,root,undefined,192.0.2.3,2,8192,1024,
unused,,00000000-0000-0000-0000-000000000002,hostname1,
192.168.1.3,10,20,30,40,ipmi""".replace('\n', '').replace('|', '\n')

        expected_hostvars = """{"hostname1":
 {"uuid": "00000000-0000-0000-0000-000000000002", "driver": "ipmi",
 "name": "hostname1", "ipv4_address": "192.168.1.3",
 "provisioning_ipv4_address": "192.168.1.3", "ansible_ssh_host":
 "192.168.1.3", "driver_info": {"power": {"ipmi_address": "192.0.2.3",
 "ipmi_password": "undefined", "ipmi_username": "root",
 "ipmi_target_address": "20", "ipmi_target_channel": "10",
 "ipmi_transit_address": "40", "ipmi_transit_channel": "30",
 "ipmi_bridging": "dual"}}, "nics":
 [{"mac": "00:01:02:03:04:06"}], "properties": {"ram": "8192", "cpu_arch":
 "x86_64", "disk_size": "1024", "cpus": "2"},
 "host_groups": ["baremetal"]}}""".replace('\n', '')

        expected_groups = """{"baremetal": {"hosts": ["hostname1"]},
 "localhost": {"hosts": ["127.0.0.1"]}}""".replace('\n', '')

        (groups, hostvars) = utils.bifrost_csv_conversion(CSV)
        self.assertDictEqual(json.loads(str(expected_hostvars)), hostvars)
        self.assertDictEqual(json.loads(expected_groups), groups)

    def test_csv_file_conversion_ipmi_single_bridging(self):
        CSV = """00:01:02:03:04:06,root,undefined,192.0.2.3,2,8192,1024,
unused,,00000000-0000-0000-0000-000000000002,hostname1,
192.168.1.3,10,20,,,ipmi""".replace('\n', '').replace('|', '\n')

        expected_hostvars = """{"hostname1":
 {"uuid": "00000000-0000-0000-0000-000000000002", "driver": "ipmi",
 "name": "hostname1", "ipv4_address": "192.168.1.3",
 "provisioning_ipv4_address": "192.168.1.3", "ansible_ssh_host":
 "192.168.1.3", "driver_info": {"power": {"ipmi_address": "192.0.2.3",
 "ipmi_password": "undefined", "ipmi_username": "root",
 "ipmi_target_address": "20", "ipmi_target_channel": "10",
 "ipmi_transit_address": null, "ipmi_transit_channel": null,
 "ipmi_bridging": "single"}}, "nics":
 [{"mac": "00:01:02:03:04:06"}], "properties": {"ram": "8192", "cpu_arch":
 "x86_64", "disk_size": "1024", "cpus": "2"},
 "host_groups": ["baremetal"]}}""".replace('\n', '')

        (groups, hostvars) = utils.bifrost_csv_conversion(CSV)
        self.assertDictEqual(json.loads(str(expected_hostvars)), hostvars)

    def test_csv_file_conversion_dhcp(self):
        CSV = """00:01:02:03:04:06,root,undefined,192.0.2.3,2,8192,1024,
unused,,00000000-0000-0000-0000-000000000002,hostname1
,,,,,,ipmi""".replace('\n', '').replace('|', '\n')

        expected_hostvars = """{"hostname1":
 {"uuid": "00000000-0000-0000-0000-000000000002", "driver": "ipmi",
 "name": "hostname1", "addressing_mode": "dhcp", "ipv4_address": null,
 "provisioning_ipv4_address": null,
 "driver_info": {"power": {"ipmi_address": "192.0.2.3", "ipmi_password":
 "undefined", "ipmi_username": "root", "ipmi_target_address": null,
 "ipmi_target_channel": null, "ipmi_transit_address": null,
 "ipmi_transit_channel": null}}, "nics":
 [{"mac": "00:01:02:03:04:06"}], "properties": {"ram": "8192", "cpu_arch":
 "x86_64", "disk_size": "1024", "cpus": "2"},
 "host_groups": ["baremetal"]}}""".replace('\n', '')

        (groups, hostvars) = utils.bifrost_csv_conversion(CSV)
        self.assertDictEqual(json.loads(str(expected_hostvars)), hostvars)

    def test_csv_json_reconsumability_dhcp(self):
        # Note(TheJulia) This intentionally takes CSV data, converts it
        # and then attempts reconsumption of the same data through the
        # JSON/YAML code path of Bifrost to ensure that the output
        # is identical.
        CSV = """00:01:02:03:04:06,root,undefined,192.0.2.3,2,8192,1024,
unused,,00000000-0000-0000-0000-000000000002,hostname1
,,,,,,ipmi""".replace('\n', '')

        expected_hostvars = """{"hostname1":
 {"uuid": "00000000-0000-0000-0000-000000000002", "driver": "ipmi",
 "name": "hostname1", "addressing_mode": "dhcp", "ipv4_address": null,
 "provisioning_ipv4_address": null,
 "driver_info": {"power": {"ipmi_address": "192.0.2.3", "ipmi_password":
 "undefined", "ipmi_username": "root", "ipmi_target_address": null,
 "ipmi_target_channel": null, "ipmi_transit_address": null,
 "ipmi_transit_channel": null}}, "nics":
 [{"mac": "00:01:02:03:04:06"}], "properties": {"ram": "8192", "cpu_arch":
 "x86_64", "disk_size": "1024", "cpus": "2"},
 "host_groups": ["baremetal"]}}""".replace('\n', '')

        (groups, hostvars) = utils.bifrost_csv_conversion(CSV)
        self.assertDictEqual(json.loads(str(expected_hostvars)), hostvars)
        (groups, hostvars) = utils.bifrost_data_conversion(
            json.dumps(hostvars))
        self.assertDictEqual(json.loads(str(expected_hostvars)), hostvars)

    def test_csv_json_reconsumability_general(self):
        CSV = """00:01:02:03:04:05,root,undefined,192.0.2.2,1,8192,512,
unused,,00000000-0000-0000-0000-000000000001,hostname0,
192.168.1.2,,,,|
00:01:02:03:04:06,root,undefined,192.0.2.3,2,8192,1024,
unused,,00000000-0000-0000-0000-000000000002,hostname1,
192.168.1.3,,,,,ipmi""".replace('\n', '').replace('|', '\n')
        expected_hostvars = """{"hostname1":
 {"uuid": "00000000-0000-0000-0000-000000000002", "driver": "ipmi",
 "name": "hostname1", "ipv4_address": "192.168.1.3", "ansible_ssh_host":
 "192.168.1.3", "provisioning_ipv4_address": "192.168.1.3",
 "driver_info": {"power": {"ipmi_address": "192.0.2.3",
 "ipmi_password": "undefined", "ipmi_username": "root",
 "ipmi_target_address": null, "ipmi_target_channel": null,
 "ipmi_transit_address": null, "ipmi_transit_channel": null}}, "nics":
 [{"mac": "00:01:02:03:04:06"}], "properties": {"ram": "8192", "cpu_arch":
 "x86_64", "disk_size": "1024", "cpus": "2"}, "host_groups": ["baremetal"]},
 "hostname0":
 {"uuid": "00000000-0000-0000-0000-000000000001", "driver": "ipmi",
 "name": "hostname0", "ipv4_address": "192.168.1.2", "ansible_ssh_host":
 "192.168.1.2", "provisioning_ipv4_address": "192.168.1.2",
 "driver_info": {"power": {"ipmi_address": "192.0.2.2",
 "ipmi_password": "undefined", "ipmi_username": "root",
 "ipmi_target_address": null, "ipmi_target_channel": null,
 "ipmi_transit_address": null, "ipmi_transit_channel": null}}, "nics":
 [{"mac": "00:01:02:03:04:05"}], "properties": {"ram": "8192",
 "cpu_arch": "x86_64", "disk_size": "512", "cpus": "1"},
 "host_groups": ["baremetal"]}}""".replace('\n', '')

        (groups, hostvars) = utils.bifrost_csv_conversion(CSV)
        self.assertDictEqual(json.loads(str(expected_hostvars)), hostvars)
        (groups, hostvars) = utils.bifrost_data_conversion(
            json.dumps(hostvars))
        self.assertDictEqual(json.loads(str(expected_hostvars)), hostvars)

    def test_yaml_to_json_conversion(self):
        # Note(TheJulia) Ultimately this is just ensuring
        # that we get the same output when we pass something
        # in as YAML
        expected_hostvars = """{"hostname1":
 {"uuid": "00000000-0000-0000-0000-000000000002", "driver": "ipmi",
 "name": "hostname1", "ipv4_address": "192.168.1.3", "ansible_ssh_host":
 "192.168.1.3", "provisioning_ipv4_address": "192.168.1.3",
 "driver_info": {"power": {"ipmi_address": "192.0.2.3",
 "ipmi_password": "undefined", "ipmi_username": "root",
 "ipmi_target_address": null, "ipmi_target_channel": null,
 "ipmi_transit_address": null, "ipmi_transit_channel": null}}, "nics":
 [{"mac": "00:01:02:03:04:06"}], "properties": {"ram": "8192", "cpu_arch":
 "x86_64", "disk_size": "1024", "cpus": "2"}, "host_groups":
 ["baremetal", "nova"]}, "hostname0":
 {"uuid": "00000000-0000-0000-0000-000000000001", "driver": "ipmi",
 "name": "hostname0", "ipv4_address": "192.168.1.2", "ansible_ssh_host":
 "192.168.1.2", "provisioning_ipv4_address": "192.168.1.2",
 "driver_info": {"power": {}}, "nics":
 [{"mac": "00:01:02:03:04:05"}], "properties": {"ram": "8192",
 "cpu_arch": "x86_64", "disk_size": "512", "cpus": "1"},
 "host_groups": ["baremetal", "nova"]}}""".replace('\n', '')
        (groups, hostvars) = utils.bifrost_data_conversion(
            yaml.safe_dump(json.loads(str(expected_hostvars))))
        self.assertDictEqual(json.loads(str(expected_hostvars)), hostvars)

    def test_minimal_json(self):
        input_json = """{"h0000-01":{"uuid":
"00000000-0000-0000-0001-bad00000010","name":"h0000-01","driver_info"
:{"power":{"ipmi_address":"10.0.0.78","ipmi_username":"ADMIN","
ipmi_password":"ADMIN"}},"driver":"ipmi"}}""".replace('\n', '')
        expected_json = """{"h0000-01":{"uuid":
"00000000-0000-0000-0001-bad00000010","name":"h0000-01","driver_info"
:{"power":{"ipmi_address":"10.0.0.78","ipmi_username":"ADMIN","
ipmi_password":"ADMIN"}},"driver":"ipmi","addressing_mode":
"dhcp","host_groups": ["baremetal"]}}""".replace('\n', '')
        (groups, hostvars) = utils.bifrost_data_conversion(input_json)
        self.assertDictEqual(json.loads(str(expected_json)), hostvars)




===========Repository Name===========
bifrost
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\bifrost\bifrost\tests\functional\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
bifrost
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\bifrost\bifrost\tests\unit\test_inventory.py
===========File Type===========
.py
===========File Content===========
# -*- coding: utf-8 -*-

# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

"""
test_inventory
----------------------------------

Tests for `inventory` module.
"""

import mock

from bifrost import inventory

from bifrost.tests import base


class TestBifrostInventoryUnit(base.TestCase):

    def test_inventory_preparation(self):
        (groups, hostvars) = inventory._prepare_inventory()
        self.assertIn("baremetal", groups)
        self.assertIn("localhost", groups)
        self.assertDictEqual(hostvars, {})
        localhost_value = dict(hosts=["127.0.0.1"])
        self.assertDictEqual(localhost_value, groups['localhost'])

    def test__val_or_none(self):
        array = ['no', '', 'yes']
        self.assertEqual('no', inventory._val_or_none(array, 0))
        self.assertIsNone(inventory._val_or_none(array, 1))
        self.assertEqual('yes', inventory._val_or_none(array, 2))
        self.assertIsNone(inventory._val_or_none(array, 4))

    def test__process_shade(self):
        inventory.shade = mock_shade = mock.Mock()
        inventory.SHADE_LOADED = True
        (groups, hostvars) = inventory._prepare_inventory()
        mock_cloud = mock_shade.operator_cloud.return_value
        mock_cloud.list_machines.return_value = [
            {
                'driver_info': {
                    'ipmi_address': '1.2.3.4',
                },
                'links': [],
                'name': 'node1',
                'ports': [],
                'properties': {
                    'cpus': 42,
                },
                'uuid': 'f3fbf7c6-b4e9-4dd2-8ca0-c74a50f8be45',
            },
        ]
        mock_cloud.list_nics_for_machine.return_value = [
            {
                'address': '00:11:22:33:44:55',
                'uuid': 'e2be93b5-a8f6-46a2-bec7-571b8ecf2938',
            },
        ]
        (groups, hostvars) = inventory._process_shade(groups, hostvars)
        mock_shade.operator_cloud.assert_called_once_with(
            auth_type='None', auth={'endpoint': 'http://localhost:6385/'})
        mock_cloud.list_machines.assert_called_once_with()
        mock_cloud.list_nics_for_machine.assert_called_once_with(
            'f3fbf7c6-b4e9-4dd2-8ca0-c74a50f8be45')
        self.assertIn('baremetal', groups)
        self.assertIn('hosts', groups['baremetal'])
        self.assertEqual(groups['baremetal'], {'hosts': ['node1']})
        self.assertIn('node1', hostvars)
        expected_machine = {
            'addressing_mode': 'dhcp',
            'driver_info': {
                'ipmi_address': '1.2.3.4',
            },
            'name': 'node1',
            'nics': [
                {
                    'mac': '00:11:22:33:44:55',
                },
            ],
            'properties': {
                'cpus': 42,
            },
            'uuid': 'f3fbf7c6-b4e9-4dd2-8ca0-c74a50f8be45',
        }
        self.assertEqual(hostvars['node1'], expected_machine)

    def test__process_shade_multiple_nics(self):
        inventory.shade = mock_shade = mock.Mock()
        inventory.SHADE_LOADED = True
        (groups, hostvars) = inventory._prepare_inventory()
        mock_cloud = mock_shade.operator_cloud.return_value
        mock_cloud.list_machines.return_value = [
            {
                'driver_info': {
                    'ipmi_address': '1.2.3.4',
                },
                'links': [],
                'name': 'node1',
                'ports': [],
                'properties': {
                    'cpus': 42,
                },
                'uuid': 'f3fbf7c6-b4e9-4dd2-8ca0-c74a50f8be45',
            },
        ]
        mock_cloud.list_nics_for_machine.return_value = [
            {
                'address': '00:11:22:33:44:55',
                'uuid': 'e2be93b5-a8f6-46a2-bec7-571b8ecf2938',
            },
            {
                'address': '00:11:22:33:44:56',
                'uuid': '59e8cd37-4f71-4ca1-a264-93c2ca7de0f7',
            },
        ]
        (groups, hostvars) = inventory._process_shade(groups, hostvars)
        mock_shade.operator_cloud.assert_called_once_with(
            auth_type='None', auth={'endpoint': 'http://localhost:6385/'})
        mock_cloud.list_machines.assert_called_once_with()
        mock_cloud.list_nics_for_machine.assert_called_once_with(
            'f3fbf7c6-b4e9-4dd2-8ca0-c74a50f8be45')
        self.assertIn('baremetal', groups)
        self.assertIn('hosts', groups['baremetal'])
        self.assertEqual(groups['baremetal'], {'hosts': ['node1']})
        self.assertIn('node1', hostvars)
        expected_machine = {
            'addressing_mode': 'dhcp',
            'driver_info': {
                'ipmi_address': '1.2.3.4',
            },
            'name': 'node1',
            'nics': [
                {
                    'mac': '00:11:22:33:44:55',
                },
                {
                    'mac': '00:11:22:33:44:56',
                },
            ],
            'properties': {
                'cpus': 42,
            },
            'uuid': 'f3fbf7c6-b4e9-4dd2-8ca0-c74a50f8be45',
        }
        self.assertEqual(hostvars['node1'], expected_machine)




===========Repository Name===========
bifrost
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\bifrost\bifrost\tests\unit\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
browbeat
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\browbeat\tox.ini
===========File Type===========
.ini
===========File Content===========
[tox]
minversion = 2.0
envlist = py27,py35,py36,py37,linters,dist,insights
skipsdist = True

[testenv]
usedevelop = True
install_command = pip install -U {opts} {packages}
setenv =
   VIRTUAL_ENV={envdir}
deps = -r{toxinidir}/test-requirements.txt
commands = python setup.py test

[testenv:linters]
# py3 linters are able to stop more than py2 ones
basepython = python3
whitelist_externals = bash
extras = insights
commands =
  {[testenv:pep8]commands}
  bash -c "cd ansible; find . -type f -regex '.*.y[a]?ml' -print0 | xargs -t -n1 -0 \
    ansible-lint \
    -x ANSIBLE0012,ANSIBLE0006,ANSIBLE0007,ANSIBLE0016,ANSIBLE0019" \
    --exclude=rally
  pykwalify -d browbeat-config.yaml -s browbeat/schema/browbeat.yml
  pykwalify -d browbeat-complete.yaml -s browbeat/schema/browbeat.yml
  bash -c "set -e; for config in $(ls conf/); do \
    echo conf/$config; pykwalify -d conf/$config -s browbeat/schema/browbeat.yml; done"

[testenv:dist]
basepython = python3
# reuse linters environment to lower footprint on dev machines
envdir = {toxworkdir}/linters
# test that we can build a valid package
commands =
  python setup.py sdist bdist_wheel
  python -m twine check dist/*

[testenv:insights]
commands =
  pip check
  pip install .[insights]
  pip check

[testenv:pep8]
basepython = python3
commands = flake8 {posargs}

[testenv:venv]
basepython = python3
commands = {posargs}

[testenv:py27]
basepython = python2.7
commands = pytest {posargs}

[testenv:py35]
basepython = python3.5
commands = pytest {posargs}

[testenv:py36]
basepython = python3.6
commands = pytest {posargs}

[testenv:py37]
basepython = python3.7
commands = pytest {posargs}

[testenv:cover]
commands = python setup.py test --coverage --testr-args={posargs}

[testenv:docs]
basepython = python3
commands = python setup.py build_sphinx

[testenv:debug]
basepython = python3
commands = oslo_debug_helper {posargs}

[flake8]
# E123, E125 skipped as they are invalid PEP-8.
show-source = True
ignore = E123,E125,E226,E302,E41,E231,E203,H233,H306,H238,H236,H404,H405,W504
max-line-length = 100
builtins = _
exclude=.venv,.git,.tox,dist,doc,*lib/python*,*egg,build,ansible/*,.browbeat-venv,.perfkit-venv,.rally-venv,.shaker-venv




===========Repository Name===========
browbeat
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\browbeat\tests\test_config.py
===========File Type===========
.py
===========File Content===========
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.

import pytest
import yaml

from browbeat.config import load_browbeat_config
from browbeat.config import _validate_yaml

test_browbeat_configs = {
    "tests/data/valid_browbeat.yml": True,
    "tests/data/invalid_browbeat.yml": False,
    "tests/data/invalid_browbeat_workload.yml": False
}


@pytest.mark.parametrize("config", test_browbeat_configs.keys())
def test_load_browbeat_config(config):
    """Tests valid and invalid Browbeat configuration."""
    if test_browbeat_configs[config]:
        # Valid configuration (No exception)
        loaded_config = load_browbeat_config(config)
        assert loaded_config["browbeat"]["cloud_name"] == "browbeat-test"
    else:
        # Invalid configuration, test for exception
        with pytest.raises(Exception) as exception_data:
            load_browbeat_config(config)
        assert "SchemaError" in str(exception_data)


@pytest.mark.parametrize("schema", ["perfkit", "rally", "shaker", "yoda"])
def test__validate_yaml(schema):
    """Tests valid and invalid Browbeat workload configurations."""
    with open("tests/data/workloads.yml", "r") as config_file:
        config_data = yaml.safe_load(config_file)

    for workload_config in config_data[schema]:
        if workload_config["valid"]:
            # Valid configuration (No exception)
            _validate_yaml(schema, workload_config["data"])
        else:
            # Invalid configuration, test for exception
            with pytest.raises(Exception) as exception_data:
                _validate_yaml(schema, workload_config["data"])
            assert "SchemaError" in str(exception_data)




===========Repository Name===========
browbeat
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\browbeat\tests\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
browbeat
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\browbeat\tests\bootstrap\test_bootstrap.py
===========File Type===========
.py
===========File Content===========
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.

import os
import sys
import pytest
sys.path.append(os.path.abspath('ansible'))
import bootstrap  # noqa


def test_bootstrap_help(capsys):
    """Tests to see if bootstrap.py help text is correct and that it loads sample/tripleo plugins"""
    help_text = ("usage: bootstrap.py [-h] [-d] {sample,tripleo} ...\n\n"
                 "Browbeat bootstrap Ansible. Generates files for Ansible interactions to the\n"
                 "OpenStack Cloud.\n\n"
                 "positional arguments:\n"
                 "  {sample,tripleo}\n\n"
                 "optional arguments:\n"
                 "  -h, --help        show this help message and exit\n"
                 "  -d, --debug       Enable Debug messages\n")
    with pytest.raises(SystemExit) as pytest_wrapped_e:
        bootstrap.main(["-h",])
        assert pytest_wrapped_e.type == SystemExit
        assert pytest_wrapped_e.value.code == 0
    out, err = capsys.readouterr()
    assert out == help_text

def test_bootstrap_tripleo_help(capsys):
    """Tests to see if bootstrap.py tripleo plugin help text is correct."""
    help_text = ("usage: bootstrap.py tripleo [-h] [-i TRIPLEO_IP] [-u USER]\n\n"
                 "Bootstrap implementation for tripleo clouds\n\n"
                 "optional arguments:\n"
                 "  -h, --help            show this help message and exit\n"
                 "  -i TRIPLEO_IP, --tripleo-ip TRIPLEO_IP\n"
                 "                        IP address of tripleo undercloud. Defaults to\n"
                 "                        'localhost'. Currently only localhost is supported.\n"
                 "  -u USER, --user USER  User used for tripleo install. Defaults to 'stack'.\n")

    with pytest.raises(SystemExit) as pytest_wrapped_e:
        bootstrap.main(["tripleo", "-h"])
        assert pytest_wrapped_e.type == SystemExit
        assert pytest_wrapped_e.value.code == 0
    out, err = capsys.readouterr()
    assert out == help_text




===========Repository Name===========
browbeat
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\browbeat\tests\bootstrap\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
browbeat
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\browbeat\tests\data\invalid_browbeat.yml
===========File Type===========
.yml
===========File Content===========
# Invalid due to invalid_flag key
browbeat:
  cloud_name: browbeat-test
  rerun: 1
  rerun_type: complete
  invalid_flag: invalid
ansible:
  hosts: ansible/hosts
  metadata_playbook: ansible/gather/site.yml
  ssh_config: ansible/ssh-config
elasticsearch:
  enabled: false
  host: browbeat.test.com
  port: 9200
  regather: false
  metadata_files:
    - name: hardware-metadata
      file: metadata/hardware-metadata.json
    - name: environment-metadata
      file: metadata/environment-metadata.json
    - name: software-metadata
      file: metadata/software-metadata.json
    - name: version
      file: metadata/version.json
grafana:
  enabled: true
  host: browbeat.test.com
  port: 3000
  dashboards:
    - openstack-general-system-performance
perfkit:
  sleep_before: 0
  sleep_after: 0
  default:
    image: centos7
    machine_type: m1.small
    os_type: rhel
    openstack_image_username: centos
    openstack_floating_ip_pool: browbeat_public
    openstack_network: browbeat_private
    timing_measurements: runtimes
    ignore_package_requirements: true
rally:
  sleep_before: 0
  sleep_after: 0
shaker:
  server: 1.1.1.1
  port: 5555
  flavor: m1.small
  join_timeout: 600
  sleep_before: 0
  sleep_after: 0
  shaker_region: regionOne
  external_host: 2.2.2.2
yoda:
  instackenv: "/home/stack/instackenv.json"
  stackrc: "/home/stack/stackrc"

workloads:
  - name: browbeat-test-perfkit-ping
    enabled: false
    type: perfkit
    benchmarks: ping
  - name: browbeat-test-authenticate
    enabled: false
    type: rally
    concurrency:
      - 1
    times: 1
    scenarios:
      - name: browbeat-test-authentic-keystone
        enabled: false
        file: rally/authenticate/keystone-cc.yml
  - name: browbeat-test-shaker-l2
    enabled: false
    type: shaker
    density: 1
    compute: 1
    progression: linear
    time: 60
    file: lib/python2.7/site-packages/shaker/scenarios/openstack/dense_l2.yaml
  - name: browbeat-test-introspect-batch
    enabled: false
    type: yoda
    yoda_type: introspection
    method: individual
    times: 3
    timeout: 900
    batch_size: 2




===========Repository Name===========
browbeat
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\browbeat\tests\data\invalid_browbeat_workload.yml
===========File Type===========
.yml
===========File Content===========
# Invalid due to workload invalid
browbeat:
  cloud_name: browbeat-test
  rerun: 1
  rerun_type: complete
ansible:
  hosts: ansible/hosts
  metadata_playbook: ansible/gather/site.yml
  ssh_config: ansible/ssh-config
elasticsearch:
  enabled: false
  host: browbeat.test.com
  port: 9200
  regather: false
  metadata_files:
    - name: hardware-metadata
      file: metadata/hardware-metadata.json
    - name: environment-metadata
      file: metadata/environment-metadata.json
    - name: software-metadata
      file: metadata/software-metadata.json
    - name: version
      file: metadata/version.json
grafana:
  enabled: true
  host: browbeat.test.com
  port: 3000
  dashboards:
    - openstack-general-system-performance
perfkit:
  sleep_before: 0
  sleep_after: 0
  default:
    image: centos7
    machine_type: m1.small
    os_type: rhel
    openstack_image_username: centos
    openstack_floating_ip_pool: browbeat_public
    openstack_network: browbeat_private
    timing_measurements: runtimes
    ignore_package_requirements: true
rally:
  sleep_before: 0
  sleep_after: 0
shaker:
  server: 1.1.1.1
  port: 5555
  flavor: m1.small
  join_timeout: 600
  sleep_before: 0
  sleep_after: 0
  shaker_region: regionOne
  external_host: 2.2.2.2
yoda:
  instackenv: "/home/stack/instackenv.json"
  stackrc: "/home/stack/stackrc"

workloads:
  # Missing name
  - enabled: false
    type: rally
    concurrency:
      - 1
    times: 1
    scenarios:
      - name: browbeat-test-authentic-keystone
        enabled: false
        file: rally/authenticate/keystone-cc.yml




===========Repository Name===========
browbeat
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\browbeat\tests\data\valid_browbeat.yml
===========File Type===========
.yml
===========File Content===========
# Valid Browbeat Config
browbeat:
  cloud_name: browbeat-test
  rerun: 1
  rerun_type: complete
ansible:
  hosts: ansible/hosts
  metadata_playbook: ansible/gather/site.yml
  ssh_config: ansible/ssh-config
elasticsearch:
  enabled: false
  host: browbeat.test.com
  port: 9200
  regather: false
  metadata_files:
    - name: hardware-metadata
      file: metadata/hardware-metadata.json
    - name: environment-metadata
      file: metadata/environment-metadata.json
    - name: software-metadata
      file: metadata/software-metadata.json
    - name: version
      file: metadata/version.json
grafana:
  enabled: true
  host: browbeat.test.com
  port: 3000
  dashboards:
    - openstack-general-system-performance
perfkit:
  sleep_before: 0
  sleep_after: 0
  default:
    image: centos7
    machine_type: m1.small
    os_type: rhel
    openstack_image_username: centos
    openstack_floating_ip_pool: browbeat_public
    openstack_network: browbeat_private
    timing_measurements: runtimes
    ignore_package_requirements: true
rally:
  sleep_before: 0
  sleep_after: 0
shaker:
  server: 1.1.1.1
  port: 5555
  flavor: m1.small
  join_timeout: 600
  sleep_before: 0
  sleep_after: 0
  shaker_region: regionOne
  external_host: 2.2.2.2
yoda:
  instackenv: "/home/stack/instackenv.json"
  stackrc: "/home/stack/stackrc"

workloads:
  - name: browbeat-test-perfkit-ping
    enabled: false
    type: perfkit
    benchmarks: ping
  - name: browbeat-test-authenticate
    enabled: false
    type: rally
    concurrency:
      - 1
    times: 1
    scenarios:
      - name: browbeat-test-authentic-keystone
        enabled: false
        file: rally/authenticate/keystone-cc.yml
  - name: browbeat-test-shaker-l2
    enabled: false
    type: shaker
    density: 1
    compute: 1
    progression: linear
    time: 60
    file: lib/python2.7/site-packages/shaker/scenarios/openstack/dense_l2.yaml
  - name: browbeat-test-introspect-batch
    enabled: false
    type: yoda
    yoda_type: introspection
    method: individual
    times: 3
    timeout: 900
    batch_size: 2




===========Repository Name===========
browbeat
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\browbeat\tests\data\workloads.yml
===========File Type===========
.yml
===========File Content===========
# Valid and invalid workload schemas for testing per workload
perfkit:
  - valid: true
    data:
      name: valid-test-perfkit-ping
      enabled: false
      type: perfkit
      benchmarks: ping
  - valid: false
    data:
      opps_name: invalid-test-perfkit-ping
      enabled: false
      type: perfkit
      benchmarks: ping
rally:
  - valid: true
    data:
      name: valid-test-authenticate-01
      enabled: true
      type: rally
      concurrency:
        - 1
      times: 1
      scenarios:
        - name: valid-test-authentic-keystone
          enabled: true
          file: rally/authenticate/keystone-cc.yml
        - name: valid-test-authentic-neutron
          enabled: false
          file: rally/authenticate/validate_neutron-cc.yml
  - valid: false
    data:
      name: invalid-test-authenticate-01
      enabled: true
      type: rally-incorrect
      concurrency:
        - 1
      times: 1
      scenarios:
        - name: invalid-test-authentic-keystone
          enabled: true
          file: rally/authenticate/keystone-cc.yml
        - name: invalid-test-authentic-neutron
          enabled: false
          file: rally/authenticate/validate_neutron-cc.yml
shaker:
  - valid: true
    data:
      name: valid-test-shaker-l2
      enabled: false
      type: shaker
      density: 1
      compute: 1
      progression: linear
      time: 60
      file: lib/python2.7/site-packages/shaker/scenarios/openstack/dense_l2.yaml
  - valid: false
    data:
      name: invalid-test-shaker-l2
      enabled: false
      type: shaker
      density: 1
      compute: 1
      progression: linear
      time: 60
      opps_file: lib/python2.7/site-packages/shaker/scenarios/openstack/dense_l2.yaml
yoda:
  - valid: true
    data:
      name: valid-test-yoda-introspection
      enabled: false
      type: yoda
      yoda_type: introspection
      method: individual
      times: 3
      timeout: 900
      batch_size: 2
  - valid: false
    data:
      name: invalid-test-yoda-introspection
      enabled: false
      type: yoda
      fake_yoda_type: introspection
      method: individual
      times: 3
      timeout: 900
      batch_size: 2




===========Repository Name===========
fuel-ccp-installer
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\fuel-ccp-installer\tox.ini
===========File Type===========
.ini
===========File Content===========
[tox]
minversion = 1.6
skipsdist = True
envlist = bashate, pep8

[testenv]
deps =
    -r{toxinidir}/requirements.txt
    -r{toxinidir}/test-requirements.txt

[testenv:doc8]
commands = doc8 doc

[testenv:docs]
whitelist_externals = /bin/rm
commands =
  /bin/rm -rf doc/build
  python setup.py build_sphinx

[doc8]
# Settings for doc8:
# Ignore target directories
ignore-path = doc/build*
# File extensions to use
extensions = .rst,.txt
# Maximal line length should be 79 but we have some overlong lines.
# Let's not get far more in.
max-line-length = 80
# Disable some doc8 checks:
# D000: Check RST validity (cannot handle lineos directive)
ignore = D000

[testenv:bashate]
whitelist_externals = bash
commands = bash -c "find {toxinidir} -type f -name '*.sh' -not -path '*/.tox/*' -print0 | xargs -0 bashate -v"

[testenv:pep8]
usedevelop = False
whitelist_externals = bash
commands =
    bash -c "find {toxinidir}/* -type f -name '*.py' -print0 | xargs -0 flake8"

[testenv:venv]
commands = {posargs}

[flake8]
show-source = true
builtins = _
exclude=.venv,.git,.tox,dist,doc,*lib/python*,*egg,tools




===========Repository Name===========
ironic-lib
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ironic-lib\tox.ini
===========File Type===========
.ini
===========File Content===========
[tox]
minversion = 1.8
skipsdist = True
envlist = py3,py27,pep8

[testenv]
install_command = pip install {opts} {packages}
usedevelop = True
setenv = VIRTUAL_ENV={envdir}
         PYTHONDONTWRITEBYTECODE = 1
         LANGUAGE=en_US
         TESTS_DIR=./ironic_lib/tests/
deps =
  -c{env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt}
  -r{toxinidir}/test-requirements.txt
  -r{toxinidir}/requirements.txt
commands = stestr run {posargs}

[flake8]
show-source = True
ignore = E129
exclude = .venv,.tox,dist,doc,*.egg,.update-venv
import-order-style = pep8
application-import-names = ironic_lib
# [H106] Don't put vim configuration in source files.
# [H203] Use assertIs(Not)None to check for None.
# [H204] Use assert(Not)Equal to check for equality.
# [H205] Use assert(Greater|Less)(Equal) for comparison.
# [H210] Require 'autospec', 'spec', or 'spec_set' in mock.patch/mock.patch.object calls
# [H904] Delay string interpolations at logging calls.
enable-extensions=H106,H203,H204,H205,H210,H904

[testenv:pep8]
basepython = python3
commands =
    flake8 {posargs}
    doc8 README.rst doc/source --ignore D001

[testenv:cover]
basepython = python3
setenv = VIRTUALENV={envdir}
         LANGUAGE=en_US
         PYTHON=coverage run --source ironic_lib --omit='*tests*' --parallel-mode
commands =
  coverage erase
  stestr run {posargs}
  coverage combine
  coverage report --omit='*tests*'
  coverage html -d ./cover --omit='*tests*'

[testenv:venv]
basepython = python3
commands = {posargs}

[testenv:docs]
basepython = python3
setenv = PYTHONHASHSEED=0
sitepackages = False
envdir = {toxworkdir}/venv
commands =
  python setup.py build_sphinx

[testenv:lower-constraints]
basepython = python3
deps =
  -c{toxinidir}/lower-constraints.txt
  -r{toxinidir}/test-requirements.txt
  -r{toxinidir}/requirements.txt




===========Repository Name===========
ironic-lib
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ironic-lib\ironic_lib\tests\base.py
===========File Type===========
.py
===========File Content===========
# Copyright 2017 Cisco Systems, Inc
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Common utilities and classes across all unit tests."""

import subprocess

from oslo_concurrency import processutils
from oslo_config import fixture as config_fixture
from oslotest import base as test_base

from ironic_lib import utils


class IronicLibTestCase(test_base.BaseTestCase):
    """Test case base class for all unit tests except callers of utils.execute.

    This test class prevents calls to the utils.execute() /
    processutils.execute() and similar functions.
    """

    # By default block execution of utils.execute() and related functions.
    block_execute = True

    def setUp(self):
        super(IronicLibTestCase, self).setUp()

        # Make sure config overrides do not leak for test to test.
        self.cfg_fixture = self.useFixture(config_fixture.Config())

        # Ban running external processes via 'execute' like functions. If the
        # patched function is called, an exception is raised to warn the
        # tester.
        if self.block_execute:
            # NOTE(jlvillal): Intentionally not using mock as if you mock a
            # mock it causes things to not work correctly. As doing an
            # autospec=True causes strangeness. By using a simple function we
            # can then mock it without issue.
            self.patch(processutils, 'execute', do_not_call)
            self.patch(subprocess, 'call', do_not_call)
            self.patch(subprocess, 'check_call', do_not_call)
            self.patch(subprocess, 'check_output', do_not_call)
            self.patch(utils, 'execute', do_not_call)

            # subprocess.Popen is a class
            self.patch(subprocess, 'Popen', DoNotCallPopen)


def do_not_call(*args, **kwargs):
    """Helper function to raise an exception if it is called"""
    raise Exception(
        "Don't call ironic_lib.utils.execute() / "
        "processutils.execute() or similar functions in tests!")


class DoNotCallPopen(object):
    """Helper class to mimic subprocess.popen()

    It's job is to raise an exception if it is called. We create stub functions
    so mocks that use autospec=True will work.
    """
    def __init__(self, *args, **kwargs):
        do_not_call(*args, **kwargs)

    def communicate(self, input=None):
        pass

    def kill(self):
        pass

    def poll(self):
        pass

    def terminate(self):
        pass

    def wait(self):
        pass




===========Repository Name===========
ironic-lib
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ironic-lib\ironic_lib\tests\test_base.py
===========File Type===========
.py
===========File Content===========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import subprocess

import mock
from oslo_concurrency import processutils

from ironic_lib.tests import base
from ironic_lib import utils


class BlockExecuteTestCase(base.IronicLibTestCase):
    """Test to ensure we block access to the 'execute' type functions"""

    def test_exception_raised_for_execute(self):
        execute_functions = (processutils.execute, subprocess.Popen,
                             subprocess.call, subprocess.check_call,
                             subprocess.check_output, utils.execute)

        for function_name in execute_functions:
            exc = self.assertRaises(
                Exception,
                function_name,
                ["echo", "%s" % function_name])  # noqa
            # Have to use 'noqa' as we are raising plain Exception and we will
            # get H202 error in 'pep8' check.

            self.assertEqual(
                "Don't call ironic_lib.utils.execute() / "
                "processutils.execute() or similar functions in tests!",
                "%s" % exc)

    @mock.patch.object(utils, "execute", autospec=True)
    def test_can_mock_execute(self, mock_exec):
        # NOTE(jlvillal): We had discovered an issue where mocking wasn't
        # working because we had used a mock to block access to the execute
        # functions. This caused us to "mock a mock" and didn't work correctly.
        # We want to make sure that we can mock our execute functions even with
        # our "block execute" code.
        utils.execute("ls")
        utils.execute("echo")
        self.assertEqual(2, mock_exec.call_count)

    @mock.patch.object(processutils, "execute", autospec=True)
    def test_exception_raised_for_execute_parent_mocked(self, mock_exec):
        # Make sure that even if we mock the parent execute function, that we
        # still get an exception for a child. So in this case
        # ironic_lib.utils.execute() calls processutils.execute(). Make sure an
        # exception is raised even though we mocked processutils.execute()
        exc = self.assertRaises(
            Exception,
            utils.execute,
            "ls")  # noqa
        # Have to use 'noqa' as we are raising plain Exception and we will get
        # H202 error in 'pep8' check.

        self.assertEqual(
            "Don't call ironic_lib.utils.execute() / "
            "processutils.execute() or similar functions in tests!",
            "%s" % exc)


class DontBlockExecuteTestCase(base.IronicLibTestCase):
    """Ensure we can turn off blocking access to 'execute' type functions"""

    # Don't block the execute function
    block_execute = False

    @mock.patch.object(processutils, "execute", autospec=True)
    def test_no_exception_raised_for_execute(self, mock_exec):
        # Make sure we can call ironic_lib.utils.execute() even though we
        # didn't mock it. We do mock processutils.execute() so we don't
        # actually execute anything.
        utils.execute("ls")
        utils.execute("echo")
        self.assertEqual(2, mock_exec.call_count)




===========Repository Name===========
ironic-lib
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ironic-lib\ironic_lib\tests\test_disk_partitioner.py
===========File Type===========
.py
===========File Content===========
# Copyright 2014 Red Hat, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import eventlet
import mock
from testtools.matchers import HasLength

from ironic_lib import disk_partitioner
from ironic_lib import exception
from ironic_lib.tests import base
from ironic_lib import utils


class DiskPartitionerTestCase(base.IronicLibTestCase):

    def test_add_partition(self):
        dp = disk_partitioner.DiskPartitioner('/dev/fake')
        dp.add_partition(1024)
        dp.add_partition(512, fs_type='linux-swap')
        dp.add_partition(2048, boot_flag='boot')
        dp.add_partition(2048, boot_flag='bios_grub')
        expected = [(1, {'boot_flag': None,
                         'extra_flags': None,
                         'fs_type': '',
                         'type': 'primary',
                         'size': 1024}),
                    (2, {'boot_flag': None,
                         'extra_flags': None,
                         'fs_type': 'linux-swap',
                         'type': 'primary',
                         'size': 512}),
                    (3, {'boot_flag': 'boot',
                         'extra_flags': None,
                         'fs_type': '',
                         'type': 'primary',
                         'size': 2048}),
                    (4, {'boot_flag': 'bios_grub',
                         'extra_flags': None,
                         'fs_type': '',
                         'type': 'primary',
                         'size': 2048})]
        partitions = [(n, p) for n, p in dp.get_partitions()]
        self.assertThat(partitions, HasLength(4))
        self.assertEqual(expected, partitions)

    @mock.patch.object(disk_partitioner.DiskPartitioner, '_exec',
                       autospec=True)
    @mock.patch.object(utils, 'execute', autospec=True)
    def test_commit(self, mock_utils_exc, mock_disk_partitioner_exec):
        dp = disk_partitioner.DiskPartitioner('/dev/fake')
        fake_parts = [(1, {'boot_flag': None,
                           'extra_flags': None,
                           'fs_type': 'fake-fs-type',
                           'type': 'fake-type',
                           'size': 1}),
                      (2, {'boot_flag': 'boot',
                           'extra_flags': None,
                           'fs_type': 'fake-fs-type',
                           'type': 'fake-type',
                           'size': 1}),
                      (3, {'boot_flag': 'bios_grub',
                           'extra_flags': None,
                           'fs_type': 'fake-fs-type',
                           'type': 'fake-type',
                           'size': 1}),
                      (4, {'boot_flag': 'boot',
                           'extra_flags': ['prep', 'fake-flag'],
                           'fs_type': 'fake-fs-type',
                           'type': 'fake-type',
                           'size': 1})]
        with mock.patch.object(dp, 'get_partitions', autospec=True) as mock_gp:
            mock_gp.return_value = fake_parts
            mock_utils_exc.return_value = ('', '')
            dp.commit()

        mock_disk_partitioner_exec.assert_called_once_with(
            mock.ANY, 'mklabel', 'msdos',
            'mkpart', 'fake-type', 'fake-fs-type', '1', '2',
            'mkpart', 'fake-type', 'fake-fs-type', '2', '3',
            'set', '2', 'boot', 'on',
            'mkpart', 'fake-type', 'fake-fs-type', '3', '4',
            'set', '3', 'bios_grub', 'on',
            'mkpart', 'fake-type', 'fake-fs-type', '4', '5',
            'set', '4', 'boot', 'on', 'set', '4', 'prep', 'on',
            'set', '4', 'fake-flag', 'on')
        mock_utils_exc.assert_called_once_with(
            'fuser', '/dev/fake', run_as_root=True,
            check_exit_code=[0, 1])

    @mock.patch.object(eventlet.greenthread, 'sleep', lambda seconds: None)
    @mock.patch.object(disk_partitioner.DiskPartitioner, '_exec',
                       autospec=True)
    @mock.patch.object(utils, 'execute', autospec=True)
    def test_commit_with_device_is_busy_once(self, mock_utils_exc,
                                             mock_disk_partitioner_exec):
        dp = disk_partitioner.DiskPartitioner('/dev/fake')
        fake_parts = [(1, {'boot_flag': None,
                           'extra_flags': None,
                           'fs_type': 'fake-fs-type',
                           'type': 'fake-type',
                           'size': 1}),
                      (2, {'boot_flag': 'boot',
                           'extra_flags': None,
                           'fs_type': 'fake-fs-type',
                           'type': 'fake-type',
                           'size': 1})]
        # Test as if the 'psmisc' version of 'fuser' which has stderr output
        fuser_outputs = iter([(" 10000 10001", '/dev/fake:\n'), ('', '')])

        with mock.patch.object(dp, 'get_partitions', autospec=True) as mock_gp:
            mock_gp.return_value = fake_parts
            mock_utils_exc.side_effect = fuser_outputs
            dp.commit()

        mock_disk_partitioner_exec.assert_called_once_with(
            mock.ANY, 'mklabel', 'msdos',
            'mkpart', 'fake-type', 'fake-fs-type', '1', '2',
            'mkpart', 'fake-type', 'fake-fs-type', '2', '3',
            'set', '2', 'boot', 'on')
        mock_utils_exc.assert_called_with(
            'fuser', '/dev/fake', run_as_root=True,
            check_exit_code=[0, 1])
        self.assertEqual(2, mock_utils_exc.call_count)

    @mock.patch.object(eventlet.greenthread, 'sleep', lambda seconds: None)
    @mock.patch.object(disk_partitioner.DiskPartitioner, '_exec',
                       autospec=True)
    @mock.patch.object(utils, 'execute', autospec=True)
    def test_commit_with_device_is_always_busy(self, mock_utils_exc,
                                               mock_disk_partitioner_exec):
        dp = disk_partitioner.DiskPartitioner('/dev/fake')
        fake_parts = [(1, {'boot_flag': None,
                           'extra_flags': None,
                           'fs_type': 'fake-fs-type',
                           'type': 'fake-type',
                           'size': 1}),
                      (2, {'boot_flag': 'boot',
                           'extra_flags': None,
                           'fs_type': 'fake-fs-type',
                           'type': 'fake-type',
                           'size': 1})]

        with mock.patch.object(dp, 'get_partitions', autospec=True) as mock_gp:
            mock_gp.return_value = fake_parts
            # Test as if the 'busybox' version of 'fuser' which does not have
            # stderr output
            mock_utils_exc.return_value = ("10000 10001", '')
            self.assertRaises(exception.InstanceDeployFailure, dp.commit)

        mock_disk_partitioner_exec.assert_called_once_with(
            mock.ANY, 'mklabel', 'msdos',
            'mkpart', 'fake-type', 'fake-fs-type', '1', '2',
            'mkpart', 'fake-type', 'fake-fs-type', '2', '3',
            'set', '2', 'boot', 'on')
        mock_utils_exc.assert_called_with(
            'fuser', '/dev/fake', run_as_root=True,
            check_exit_code=[0, 1])
        self.assertEqual(20, mock_utils_exc.call_count)

    # Mock the eventlet.greenthread.sleep for the looping_call
    @mock.patch.object(eventlet.greenthread, 'sleep', lambda seconds: None)
    @mock.patch.object(disk_partitioner.DiskPartitioner, '_exec',
                       autospec=True)
    @mock.patch.object(utils, 'execute', autospec=True)
    def test_commit_with_device_disconnected(self, mock_utils_exc,
                                             mock_disk_partitioner_exec):
        dp = disk_partitioner.DiskPartitioner('/dev/fake')
        fake_parts = [(1, {'boot_flag': None,
                           'extra_flags': None,
                           'fs_type': 'fake-fs-type',
                           'type': 'fake-type',
                           'size': 1}),
                      (2, {'boot_flag': 'boot',
                           'extra_flags': None,
                           'fs_type': 'fake-fs-type',
                           'type': 'fake-type',
                           'size': 1})]

        with mock.patch.object(dp, 'get_partitions', autospec=True) as mock_gp:
            mock_gp.return_value = fake_parts
            mock_utils_exc.return_value = ('', "Specified filename /dev/fake"
                                               " does not exist.")
            self.assertRaises(exception.InstanceDeployFailure, dp.commit)

        mock_disk_partitioner_exec.assert_called_once_with(
            mock.ANY, 'mklabel', 'msdos',
            'mkpart', 'fake-type', 'fake-fs-type', '1', '2',
            'mkpart', 'fake-type', 'fake-fs-type', '2', '3',
            'set', '2', 'boot', 'on')
        mock_utils_exc.assert_called_with(
            'fuser', '/dev/fake', run_as_root=True,
            check_exit_code=[0, 1])
        self.assertEqual(20, mock_utils_exc.call_count)




===========Repository Name===========
ironic-lib
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ironic-lib\ironic_lib\tests\test_disk_utils.py
===========File Type===========
.py
===========File Content===========
# Copyright 2014 Red Hat, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import gzip
import os
import shutil
import stat
import tempfile

import mock
from oslo_concurrency import processutils
from oslo_config import cfg
from oslo_serialization import base64
from oslo_utils import imageutils
import requests

from ironic_lib import disk_partitioner
from ironic_lib import disk_utils
from ironic_lib import exception
from ironic_lib.tests import base
from ironic_lib import utils

CONF = cfg.CONF


@mock.patch.object(utils, 'execute', autospec=True)
class ListPartitionsTestCase(base.IronicLibTestCase):

    def test_correct(self, execute_mock):
        output = """
BYT;
/dev/sda:500107862016B:scsi:512:4096:msdos:ATA HGST HTS725050A7:;
1:1.00MiB:501MiB:500MiB:ext4::boot;
2:501MiB:476940MiB:476439MiB:::;
"""
        expected = [
            {'number': 1, 'start': 1, 'end': 501, 'size': 500,
             'filesystem': 'ext4', 'flags': 'boot'},
            {'number': 2, 'start': 501, 'end': 476940, 'size': 476439,
             'filesystem': '', 'flags': ''},
        ]
        execute_mock.return_value = (output, '')
        result = disk_utils.list_partitions('/dev/fake')
        self.assertEqual(expected, result)
        execute_mock.assert_called_once_with(
            'parted', '-s', '-m', '/dev/fake', 'unit', 'MiB', 'print',
            use_standard_locale=True, run_as_root=True)

    @mock.patch.object(disk_utils.LOG, 'warning', autospec=True)
    def test_incorrect(self, log_mock, execute_mock):
        output = """
BYT;
/dev/sda:500107862016B:scsi:512:4096:msdos:ATA HGST HTS725050A7:;
1:XX1076MiB:---:524MiB:ext4::boot;
"""
        execute_mock.return_value = (output, '')
        self.assertEqual([], disk_utils.list_partitions('/dev/fake'))
        self.assertEqual(1, log_mock.call_count)


@mock.patch.object(disk_partitioner.DiskPartitioner, 'commit', lambda _: None)
class WorkOnDiskTestCase(base.IronicLibTestCase):

    def setUp(self):
        super(WorkOnDiskTestCase, self).setUp()
        self.image_path = '/tmp/xyz/image'
        self.root_mb = 128
        self.swap_mb = 64
        self.ephemeral_mb = 0
        self.ephemeral_format = None
        self.configdrive_mb = 0
        self.node_uuid = "12345678-1234-1234-1234-1234567890abcxyz"
        self.dev = '/dev/fake'
        self.swap_part = '/dev/fake-part1'
        self.root_part = '/dev/fake-part2'

        self.mock_ibd_obj = mock.patch.object(
            disk_utils, 'is_block_device', autospec=True)
        self.mock_ibd = self.mock_ibd_obj.start()
        self.addCleanup(self.mock_ibd_obj.stop)
        self.mock_mp_obj = mock.patch.object(
            disk_utils, 'make_partitions', autospec=True)
        self.mock_mp = self.mock_mp_obj.start()
        self.addCleanup(self.mock_mp_obj.stop)
        self.mock_remlbl_obj = mock.patch.object(
            disk_utils, 'destroy_disk_metadata', autospec=True)
        self.mock_remlbl = self.mock_remlbl_obj.start()
        self.addCleanup(self.mock_remlbl_obj.stop)
        self.mock_mp.return_value = {'swap': self.swap_part,
                                     'root': self.root_part}

    def test_no_root_partition(self):
        self.mock_ibd.return_value = False
        self.assertRaises(exception.InstanceDeployFailure,
                          disk_utils.work_on_disk, self.dev, self.root_mb,
                          self.swap_mb, self.ephemeral_mb,
                          self.ephemeral_format, self.image_path,
                          self.node_uuid)
        self.mock_ibd.assert_called_once_with(self.root_part)
        self.mock_mp.assert_called_once_with(self.dev, self.root_mb,
                                             self.swap_mb, self.ephemeral_mb,
                                             self.configdrive_mb,
                                             self.node_uuid, commit=True,
                                             boot_option="netboot",
                                             boot_mode="bios",
                                             disk_label=None,
                                             cpu_arch="")

    def test_no_swap_partition(self):
        self.mock_ibd.side_effect = iter([True, False])
        calls = [mock.call(self.root_part),
                 mock.call(self.swap_part)]
        self.assertRaises(exception.InstanceDeployFailure,
                          disk_utils.work_on_disk, self.dev, self.root_mb,
                          self.swap_mb, self.ephemeral_mb,
                          self.ephemeral_format, self.image_path,
                          self.node_uuid)
        self.assertEqual(self.mock_ibd.call_args_list, calls)
        self.mock_mp.assert_called_once_with(self.dev, self.root_mb,
                                             self.swap_mb, self.ephemeral_mb,
                                             self.configdrive_mb,
                                             self.node_uuid, commit=True,
                                             boot_option="netboot",
                                             boot_mode="bios",
                                             disk_label=None,
                                             cpu_arch="")

    def test_no_ephemeral_partition(self):
        ephemeral_part = '/dev/fake-part1'
        swap_part = '/dev/fake-part2'
        root_part = '/dev/fake-part3'
        ephemeral_mb = 256
        ephemeral_format = 'exttest'

        self.mock_mp.return_value = {'ephemeral': ephemeral_part,
                                     'swap': swap_part,
                                     'root': root_part}
        self.mock_ibd.side_effect = iter([True, True, False])
        calls = [mock.call(root_part),
                 mock.call(swap_part),
                 mock.call(ephemeral_part)]
        self.assertRaises(exception.InstanceDeployFailure,
                          disk_utils.work_on_disk, self.dev, self.root_mb,
                          self.swap_mb, ephemeral_mb, ephemeral_format,
                          self.image_path, self.node_uuid)
        self.assertEqual(self.mock_ibd.call_args_list, calls)
        self.mock_mp.assert_called_once_with(self.dev, self.root_mb,
                                             self.swap_mb, ephemeral_mb,
                                             self.configdrive_mb,
                                             self.node_uuid, commit=True,
                                             boot_option="netboot",
                                             boot_mode="bios",
                                             disk_label=None,
                                             cpu_arch="")

    @mock.patch.object(utils, 'unlink_without_raise', autospec=True)
    @mock.patch.object(disk_utils, '_get_configdrive', autospec=True)
    def test_no_configdrive_partition(self, mock_configdrive, mock_unlink):
        mock_configdrive.return_value = (10, 'fake-path')
        swap_part = '/dev/fake-part1'
        configdrive_part = '/dev/fake-part2'
        root_part = '/dev/fake-part3'
        configdrive_url = 'http://1.2.3.4/cd'
        configdrive_mb = 10

        self.mock_mp.return_value = {'swap': swap_part,
                                     'configdrive': configdrive_part,
                                     'root': root_part}
        self.mock_ibd.side_effect = iter([True, True, False])
        calls = [mock.call(root_part),
                 mock.call(swap_part),
                 mock.call(configdrive_part)]
        self.assertRaises(exception.InstanceDeployFailure,
                          disk_utils.work_on_disk, self.dev, self.root_mb,
                          self.swap_mb, self.ephemeral_mb,
                          self.ephemeral_format, self.image_path,
                          self.node_uuid, preserve_ephemeral=False,
                          configdrive=configdrive_url,
                          boot_option="netboot")
        self.assertEqual(self.mock_ibd.call_args_list, calls)
        self.mock_mp.assert_called_once_with(self.dev, self.root_mb,
                                             self.swap_mb, self.ephemeral_mb,
                                             configdrive_mb, self.node_uuid,
                                             commit=True,
                                             boot_option="netboot",
                                             boot_mode="bios",
                                             disk_label=None,
                                             cpu_arch="")
        mock_unlink.assert_called_once_with('fake-path')

    @mock.patch.object(utils, 'mkfs', lambda fs, path, label=None: None)
    @mock.patch.object(disk_utils, 'block_uuid', lambda p: 'uuid')
    @mock.patch.object(disk_utils, 'populate_image', autospec=True)
    def test_without_image(self, mock_populate):
        ephemeral_part = '/dev/fake-part1'
        swap_part = '/dev/fake-part2'
        root_part = '/dev/fake-part3'
        ephemeral_mb = 256
        ephemeral_format = 'exttest'

        self.mock_mp.return_value = {'ephemeral': ephemeral_part,
                                     'swap': swap_part,
                                     'root': root_part}
        self.mock_ibd.return_value = True
        calls = [mock.call(root_part),
                 mock.call(swap_part),
                 mock.call(ephemeral_part)]
        res = disk_utils.work_on_disk(self.dev, self.root_mb,
                                      self.swap_mb, ephemeral_mb,
                                      ephemeral_format,
                                      None, self.node_uuid)
        self.assertEqual(self.mock_ibd.call_args_list, calls)
        self.mock_mp.assert_called_once_with(self.dev, self.root_mb,
                                             self.swap_mb, ephemeral_mb,
                                             self.configdrive_mb,
                                             self.node_uuid, commit=True,
                                             boot_option="netboot",
                                             boot_mode="bios",
                                             disk_label=None,
                                             cpu_arch="")
        self.assertEqual(root_part, res['partitions']['root'])
        self.assertEqual('uuid', res['root uuid'])
        self.assertFalse(mock_populate.called)

    @mock.patch.object(utils, 'mkfs', lambda fs, path, label=None: None)
    @mock.patch.object(disk_utils, 'block_uuid', lambda p: 'uuid')
    @mock.patch.object(disk_utils, 'populate_image', lambda image_path,
                       root_path, conv_flags=None: None)
    def test_gpt_disk_label(self):
        ephemeral_part = '/dev/fake-part1'
        swap_part = '/dev/fake-part2'
        root_part = '/dev/fake-part3'
        ephemeral_mb = 256
        ephemeral_format = 'exttest'

        self.mock_mp.return_value = {'ephemeral': ephemeral_part,
                                     'swap': swap_part,
                                     'root': root_part}
        self.mock_ibd.return_value = True
        calls = [mock.call(root_part),
                 mock.call(swap_part),
                 mock.call(ephemeral_part)]
        disk_utils.work_on_disk(self.dev, self.root_mb,
                                self.swap_mb, ephemeral_mb, ephemeral_format,
                                self.image_path, self.node_uuid,
                                disk_label='gpt', conv_flags=None)
        self.assertEqual(self.mock_ibd.call_args_list, calls)
        self.mock_mp.assert_called_once_with(self.dev, self.root_mb,
                                             self.swap_mb, ephemeral_mb,
                                             self.configdrive_mb,
                                             self.node_uuid, commit=True,
                                             boot_option="netboot",
                                             boot_mode="bios",
                                             disk_label='gpt',
                                             cpu_arch="")

    @mock.patch.object(disk_utils, 'block_uuid', autospec=True)
    @mock.patch.object(disk_utils, 'populate_image', autospec=True)
    @mock.patch.object(utils, 'mkfs', autospec=True)
    def test_uefi_localboot(self, mock_mkfs, mock_populate_image,
                            mock_block_uuid):
        """Test that we create a fat filesystem with UEFI localboot."""
        root_part = '/dev/fake-part1'
        efi_part = '/dev/fake-part2'
        self.mock_mp.return_value = {'root': root_part,
                                     'efi system partition': efi_part}
        self.mock_ibd.return_value = True
        mock_ibd_calls = [mock.call(root_part),
                          mock.call(efi_part)]

        disk_utils.work_on_disk(self.dev, self.root_mb,
                                self.swap_mb, self.ephemeral_mb,
                                self.ephemeral_format,
                                self.image_path, self.node_uuid,
                                boot_option="local", boot_mode="uefi")

        self.mock_mp.assert_called_once_with(self.dev, self.root_mb,
                                             self.swap_mb, self.ephemeral_mb,
                                             self.configdrive_mb,
                                             self.node_uuid, commit=True,
                                             boot_option="local",
                                             boot_mode="uefi",
                                             disk_label=None,
                                             cpu_arch="")
        self.assertEqual(self.mock_ibd.call_args_list, mock_ibd_calls)
        mock_mkfs.assert_called_once_with(fs='vfat', path=efi_part,
                                          label='efi-part')
        mock_populate_image.assert_called_once_with(self.image_path,
                                                    root_part, conv_flags=None)
        mock_block_uuid.assert_any_call(root_part)
        mock_block_uuid.assert_any_call(efi_part)

    @mock.patch.object(disk_utils, 'block_uuid', autospec=True)
    @mock.patch.object(disk_utils, 'populate_image', autospec=True)
    @mock.patch.object(utils, 'mkfs', autospec=True)
    def test_preserve_ephemeral(self, mock_mkfs, mock_populate_image,
                                mock_block_uuid):
        """Test that ephemeral partition doesn't get overwritten."""
        ephemeral_part = '/dev/fake-part1'
        root_part = '/dev/fake-part2'
        ephemeral_mb = 256
        ephemeral_format = 'exttest'

        self.mock_mp.return_value = {'ephemeral': ephemeral_part,
                                     'root': root_part}
        self.mock_ibd.return_value = True
        calls = [mock.call(root_part),
                 mock.call(ephemeral_part)]
        disk_utils.work_on_disk(self.dev, self.root_mb,
                                self.swap_mb, ephemeral_mb, ephemeral_format,
                                self.image_path, self.node_uuid,
                                preserve_ephemeral=True)
        self.assertEqual(self.mock_ibd.call_args_list, calls)
        self.mock_mp.assert_called_once_with(self.dev, self.root_mb,
                                             self.swap_mb, ephemeral_mb,
                                             self.configdrive_mb,
                                             self.node_uuid, commit=False,
                                             boot_option="netboot",
                                             boot_mode="bios",
                                             disk_label=None,
                                             cpu_arch="")
        self.assertFalse(mock_mkfs.called)

    @mock.patch.object(disk_utils, 'block_uuid', autospec=True)
    @mock.patch.object(disk_utils, 'populate_image', autospec=True)
    @mock.patch.object(utils, 'mkfs', autospec=True)
    def test_ppc64le_prep_part(self, mock_mkfs, mock_populate_image,
                               mock_block_uuid):
        """Test that PReP partition uuid is returned."""
        prep_part = '/dev/fake-part1'
        root_part = '/dev/fake-part2'

        self.mock_mp.return_value = {'PReP Boot partition': prep_part,
                                     'root': root_part}
        self.mock_ibd.return_vaue = True
        calls = [mock.call(root_part),
                 mock.call(prep_part)]
        disk_utils.work_on_disk(self.dev, self.root_mb,
                                self.swap_mb, self.ephemeral_mb,
                                self.ephemeral_format, self.image_path,
                                self.node_uuid, boot_option="local",
                                cpu_arch='ppc64le')
        self.assertEqual(self.mock_ibd.call_args_list, calls)
        self.mock_mp.assert_called_once_with(self.dev, self.root_mb,
                                             self.swap_mb, self.ephemeral_mb,
                                             self.configdrive_mb,
                                             self.node_uuid, commit=True,
                                             boot_option="local",
                                             boot_mode="bios",
                                             disk_label=None,
                                             cpu_arch="ppc64le")
        self.assertFalse(mock_mkfs.called)

    @mock.patch.object(disk_utils, 'block_uuid', autospec=True)
    @mock.patch.object(disk_utils, 'populate_image', autospec=True)
    @mock.patch.object(utils, 'mkfs', autospec=True)
    def test_convert_to_sparse(self, mock_mkfs, mock_populate_image,
                               mock_block_uuid):
        ephemeral_part = '/dev/fake-part1'
        swap_part = '/dev/fake-part2'
        root_part = '/dev/fake-part3'
        ephemeral_mb = 256
        ephemeral_format = 'exttest'

        self.mock_mp.return_value = {'ephemeral': ephemeral_part,
                                     'swap': swap_part,
                                     'root': root_part}
        self.mock_ibd.return_value = True
        disk_utils.work_on_disk(self.dev, self.root_mb,
                                self.swap_mb, ephemeral_mb, ephemeral_format,
                                self.image_path, self.node_uuid,
                                disk_label='gpt', conv_flags='sparse')

        mock_populate_image.assert_called_once_with(self.image_path,
                                                    root_part,
                                                    conv_flags='sparse')


@mock.patch.object(utils, 'execute', autospec=True)
class MakePartitionsTestCase(base.IronicLibTestCase):

    def setUp(self):
        super(MakePartitionsTestCase, self).setUp()
        self.dev = 'fake-dev'
        self.root_mb = 1024
        self.swap_mb = 512
        self.ephemeral_mb = 0
        self.configdrive_mb = 0
        self.node_uuid = "12345678-1234-1234-1234-1234567890abcxyz"
        self.efi_size = CONF.disk_utils.efi_system_partition_size
        self.bios_size = CONF.disk_utils.bios_boot_partition_size

    def _get_parted_cmd(self, dev, label=None):
        if label is None:
            label = 'msdos'

        return ['parted', '-a', 'optimal', '-s', dev,
                '--', 'unit', 'MiB', 'mklabel', label]

    def _test_make_partitions(self, mock_exc, boot_option, boot_mode='bios',
                              disk_label=None, cpu_arch=""):
        mock_exc.return_value = ('', '')
        disk_utils.make_partitions(self.dev, self.root_mb, self.swap_mb,
                                   self.ephemeral_mb, self.configdrive_mb,
                                   self.node_uuid, boot_option=boot_option,
                                   boot_mode=boot_mode, disk_label=disk_label,
                                   cpu_arch=cpu_arch)

        if boot_option == "local" and boot_mode == "uefi":
            add_efi_sz = lambda x: str(x + self.efi_size)
            expected_mkpart = ['mkpart', 'primary', 'fat32', '1',
                               add_efi_sz(1),
                               'set', '1', 'boot', 'on',
                               'mkpart', 'primary', 'linux-swap',
                               add_efi_sz(1), add_efi_sz(513), 'mkpart',
                               'primary', '', add_efi_sz(513),
                               add_efi_sz(1537)]
        else:
            if boot_option == "local":
                if disk_label == "gpt":
                    if cpu_arch.startswith('ppc64'):
                        expected_mkpart = ['mkpart', 'primary', '', '1', '9',
                                           'set', '1', 'prep', 'on',
                                           'mkpart', 'primary', 'linux-swap',
                                           '9', '521', 'mkpart', 'primary',
                                           '', '521', '1545']
                    else:
                        add_bios_sz = lambda x: str(x + self.bios_size)
                        expected_mkpart = ['mkpart', 'primary', '', '1',
                                           add_bios_sz(1),
                                           'set', '1', 'bios_grub', 'on',
                                           'mkpart', 'primary', 'linux-swap',
                                           add_bios_sz(1), add_bios_sz(513),
                                           'mkpart', 'primary', '',
                                           add_bios_sz(513), add_bios_sz(1537)]
                elif cpu_arch.startswith('ppc64'):
                    expected_mkpart = ['mkpart', 'primary', '', '1', '9',
                                       'set', '1', 'boot', 'on',
                                       'set', '1', 'prep', 'on',
                                       'mkpart', 'primary', 'linux-swap',
                                       '9', '521', 'mkpart', 'primary',
                                       '', '521', '1545']
                else:
                    expected_mkpart = ['mkpart', 'primary', 'linux-swap', '1',
                                       '513', 'mkpart', 'primary', '', '513',
                                       '1537', 'set', '2', 'boot', 'on']
            else:
                expected_mkpart = ['mkpart', 'primary', 'linux-swap', '1',
                                   '513', 'mkpart', 'primary', '', '513',
                                   '1537']
        self.dev = 'fake-dev'
        parted_cmd = (self._get_parted_cmd(self.dev, disk_label) +
                      expected_mkpart)
        parted_call = mock.call(*parted_cmd, use_standard_locale=True,
                                run_as_root=True, check_exit_code=[0])
        fuser_cmd = ['fuser', 'fake-dev']
        fuser_call = mock.call(*fuser_cmd, run_as_root=True,
                               check_exit_code=[0, 1])
        mock_exc.assert_has_calls([parted_call, fuser_call])

    def test_make_partitions(self, mock_exc):
        self._test_make_partitions(mock_exc, boot_option="netboot")

    def test_make_partitions_local_boot(self, mock_exc):
        self._test_make_partitions(mock_exc, boot_option="local")

    def test_make_partitions_local_boot_uefi(self, mock_exc):
        self._test_make_partitions(mock_exc, boot_option="local",
                                   boot_mode="uefi", disk_label="gpt")

    def test_make_partitions_local_boot_gpt_bios(self, mock_exc):
        self._test_make_partitions(mock_exc, boot_option="local",
                                   disk_label="gpt")

    def test_make_partitions_disk_label_gpt(self, mock_exc):
        self._test_make_partitions(mock_exc, boot_option="netboot",
                                   disk_label="gpt")

    def test_make_partitions_mbr_with_prep(self, mock_exc):
        self._test_make_partitions(mock_exc, boot_option="local",
                                   disk_label="msdos", cpu_arch="ppc64le")

    def test_make_partitions_gpt_with_prep(self, mock_exc):
        self._test_make_partitions(mock_exc, boot_option="local",
                                   disk_label="gpt", cpu_arch="ppc64le")

    def test_make_partitions_with_ephemeral(self, mock_exc):
        self.ephemeral_mb = 2048
        expected_mkpart = ['mkpart', 'primary', '', '1', '2049',
                           'mkpart', 'primary', 'linux-swap', '2049', '2561',
                           'mkpart', 'primary', '', '2561', '3585']
        self.dev = 'fake-dev'
        cmd = self._get_parted_cmd(self.dev) + expected_mkpart
        mock_exc.return_value = ('', '')
        disk_utils.make_partitions(self.dev, self.root_mb, self.swap_mb,
                                   self.ephemeral_mb, self.configdrive_mb,
                                   self.node_uuid)

        parted_call = mock.call(*cmd, use_standard_locale=True,
                                run_as_root=True, check_exit_code=[0])
        mock_exc.assert_has_calls([parted_call])

    def test_make_partitions_with_iscsi_device(self, mock_exc):
        self.ephemeral_mb = 2048
        expected_mkpart = ['mkpart', 'primary', '', '1', '2049',
                           'mkpart', 'primary', 'linux-swap', '2049', '2561',
                           'mkpart', 'primary', '', '2561', '3585']
        self.dev = '/dev/iqn.2008-10.org.openstack:%s.fake-9' % self.node_uuid
        ep = '/dev/iqn.2008-10.org.openstack:%s.fake-9-part1' % self.node_uuid
        swap = ('/dev/iqn.2008-10.org.openstack:%s.fake-9-part2'
                % self.node_uuid)
        root = ('/dev/iqn.2008-10.org.openstack:%s.fake-9-part3'
                % self.node_uuid)
        expected_result = {'ephemeral': ep,
                           'swap': swap,
                           'root': root}
        cmd = self._get_parted_cmd(self.dev) + expected_mkpart
        mock_exc.return_value = ('', '')
        result = disk_utils.make_partitions(
            self.dev, self.root_mb, self.swap_mb, self.ephemeral_mb,
            self.configdrive_mb, self.node_uuid)

        parted_call = mock.call(*cmd, use_standard_locale=True,
                                run_as_root=True, check_exit_code=[0])
        mock_exc.assert_has_calls([parted_call])
        self.assertEqual(expected_result, result)

    def test_make_partitions_with_nvme_device(self, mock_exc):
        self.ephemeral_mb = 2048
        expected_mkpart = ['mkpart', 'primary', '', '1', '2049',
                           'mkpart', 'primary', 'linux-swap', '2049', '2561',
                           'mkpart', 'primary', '', '2561', '3585']
        self.dev = '/dev/nvmefake-9'
        ep = '/dev/nvmefake-9p1'
        swap = '/dev/nvmefake-9p2'
        root = '/dev/nvmefake-9p3'
        expected_result = {'ephemeral': ep,
                           'swap': swap,
                           'root': root}
        cmd = self._get_parted_cmd(self.dev) + expected_mkpart
        mock_exc.return_value = ('', '')
        result = disk_utils.make_partitions(
            self.dev, self.root_mb, self.swap_mb, self.ephemeral_mb,
            self.configdrive_mb, self.node_uuid)

        parted_call = mock.call(*cmd, use_standard_locale=True,
                                run_as_root=True, check_exit_code=[0])
        mock_exc.assert_has_calls([parted_call])
        self.assertEqual(expected_result, result)

    def test_make_partitions_with_local_device(self, mock_exc):
        self.ephemeral_mb = 2048
        expected_mkpart = ['mkpart', 'primary', '', '1', '2049',
                           'mkpart', 'primary', 'linux-swap', '2049', '2561',
                           'mkpart', 'primary', '', '2561', '3585']
        self.dev = 'fake-dev'
        expected_result = {'ephemeral': 'fake-dev1',
                           'swap': 'fake-dev2',
                           'root': 'fake-dev3'}
        cmd = self._get_parted_cmd(self.dev) + expected_mkpart
        mock_exc.return_value = ('', '')
        result = disk_utils.make_partitions(
            self.dev, self.root_mb, self.swap_mb, self.ephemeral_mb,
            self.configdrive_mb, self.node_uuid)

        parted_call = mock.call(*cmd, use_standard_locale=True,
                                run_as_root=True, check_exit_code=[0])
        mock_exc.assert_has_calls([parted_call])
        self.assertEqual(expected_result, result)


@mock.patch.object(utils, 'execute', autospec=True, return_value=('', ''))
class DestroyMetaDataTestCase(base.IronicLibTestCase):

    def setUp(self):
        super(DestroyMetaDataTestCase, self).setUp()
        self.dev = 'fake-dev'
        self.node_uuid = "12345678-1234-1234-1234-1234567890abcxyz"

    def test_destroy_disk_metadata(self, mock_exec):
        expected_calls = [mock.call('wipefs', '--force', '--all', 'fake-dev',
                                    run_as_root=True,
                                    use_standard_locale=True),
                          mock.call('sgdisk', '-Z', 'fake-dev',
                                    run_as_root=True,
                                    use_standard_locale=True),
                          mock.call('fuser', self.dev,
                                    check_exit_code=[0, 1],
                                    run_as_root=True)]
        disk_utils.destroy_disk_metadata(self.dev, self.node_uuid)
        mock_exec.assert_has_calls(expected_calls)

    def test_destroy_disk_metadata_wipefs_fail(self, mock_exec):
        mock_exec.side_effect = processutils.ProcessExecutionError

        expected_call = [mock.call('wipefs', '--force', '--all', 'fake-dev',
                                   run_as_root=True,
                                   use_standard_locale=True)]
        self.assertRaises(processutils.ProcessExecutionError,
                          disk_utils.destroy_disk_metadata,
                          self.dev,
                          self.node_uuid)
        mock_exec.assert_has_calls(expected_call)

    def test_destroy_disk_metadata_sgdisk_fail(self, mock_exec):
        expected_calls = [mock.call('wipefs', '--force', '--all', 'fake-dev',
                                    run_as_root=True,
                                    use_standard_locale=True),
                          mock.call('sgdisk', '-Z', 'fake-dev',
                                    run_as_root=True,
                                    use_standard_locale=True)]
        mock_exec.side_effect = [(None, None),
                                 processutils.ProcessExecutionError()]
        self.assertRaises(processutils.ProcessExecutionError,
                          disk_utils.destroy_disk_metadata,
                          self.dev,
                          self.node_uuid)
        mock_exec.assert_has_calls(expected_calls)

    def test_destroy_disk_metadata_wipefs_not_support_force(self, mock_exec):
        mock_exec.side_effect = iter(
            [processutils.ProcessExecutionError(description='--force'),
             (None, None),
             (None, None),
             ('', '')])

        expected_call = [mock.call('wipefs', '--force', '--all', 'fake-dev',
                                   run_as_root=True,
                                   use_standard_locale=True),
                         mock.call('wipefs', '--all', 'fake-dev',
                                   run_as_root=True,
                                   use_standard_locale=True)]
        disk_utils.destroy_disk_metadata(self.dev, self.node_uuid)
        mock_exec.assert_has_calls(expected_call)


@mock.patch.object(utils, 'execute', autospec=True)
class GetDeviceBlockSizeTestCase(base.IronicLibTestCase):

    def setUp(self):
        super(GetDeviceBlockSizeTestCase, self).setUp()
        self.dev = 'fake-dev'
        self.node_uuid = "12345678-1234-1234-1234-1234567890abcxyz"

    def test_get_dev_block_size(self, mock_exec):
        mock_exec.return_value = ("64", "")
        expected_call = [mock.call('blockdev', '--getsz', self.dev,
                                   run_as_root=True, check_exit_code=[0])]
        disk_utils.get_dev_block_size(self.dev)
        mock_exec.assert_has_calls(expected_call)


@mock.patch.object(disk_utils, 'dd', autospec=True)
@mock.patch.object(disk_utils, 'qemu_img_info', autospec=True)
@mock.patch.object(disk_utils, 'convert_image', autospec=True)
class PopulateImageTestCase(base.IronicLibTestCase):

    def test_populate_raw_image(self, mock_cg, mock_qinfo, mock_dd):
        type(mock_qinfo.return_value).file_format = mock.PropertyMock(
            return_value='raw')
        disk_utils.populate_image('src', 'dst')
        mock_dd.assert_called_once_with('src', 'dst', conv_flags=None)
        self.assertFalse(mock_cg.called)

    def test_populate_raw_image_with_convert(self, mock_cg, mock_qinfo,
                                             mock_dd):
        type(mock_qinfo.return_value).file_format = mock.PropertyMock(
            return_value='raw')
        disk_utils.populate_image('src', 'dst', conv_flags='sparse')
        mock_dd.assert_called_once_with('src', 'dst', conv_flags='sparse')
        self.assertFalse(mock_cg.called)

    def test_populate_qcow2_image(self, mock_cg, mock_qinfo, mock_dd):
        type(mock_qinfo.return_value).file_format = mock.PropertyMock(
            return_value='qcow2')
        disk_utils.populate_image('src', 'dst')
        mock_cg.assert_called_once_with('src', 'dst', 'raw', True)
        self.assertFalse(mock_dd.called)


@mock.patch.object(utils, 'wait_for_disk_to_become_available', lambda *_: None)
@mock.patch.object(disk_utils, 'is_block_device', lambda d: True)
@mock.patch.object(disk_utils, 'block_uuid', lambda p: 'uuid')
@mock.patch.object(disk_utils, 'dd', lambda *_: None)
@mock.patch.object(disk_utils, 'convert_image', lambda *_: None)
@mock.patch.object(utils, 'mkfs', lambda fs, path, label=None: None)
# NOTE(dtantsur): destroy_disk_metadata resets file size, disabling it
@mock.patch.object(disk_utils, 'destroy_disk_metadata', lambda *_: None)
class RealFilePartitioningTestCase(base.IronicLibTestCase):
    """This test applies some real-world partitioning scenario to a file.

    This test covers the whole partitioning, mocking everything not possible
    on a file. That helps us assure, that we do all partitioning math properly
    and also conducts integration testing of DiskPartitioner.
    """

    # Allow calls to utils.execute() and related functions
    block_execute = False

    def setUp(self):
        super(RealFilePartitioningTestCase, self).setUp()
        # NOTE(dtantsur): no parted utility on gate-ironic-python26
        try:
            utils.execute('parted', '--version')
        except OSError as exc:
            self.skipTest('parted utility was not found: %s' % exc)
        self.file = tempfile.NamedTemporaryFile(delete=False)
        # NOTE(ifarkas): the file needs to be closed, so fuser won't report
        #                any usage
        self.file.close()
        # NOTE(dtantsur): 20 MiB file with zeros
        utils.execute('dd', 'if=/dev/zero', 'of=%s' % self.file.name,
                      'bs=1', 'count=0', 'seek=20MiB')

    @staticmethod
    def _run_without_root(func, *args, **kwargs):
        """Make sure root is not required when using utils.execute."""
        real_execute = utils.execute

        def fake_execute(*cmd, **kwargs):
            kwargs['run_as_root'] = False
            return real_execute(*cmd, **kwargs)

        with mock.patch.object(utils, 'execute', fake_execute):
            return func(*args, **kwargs)

    def test_different_sizes(self):
        # NOTE(dtantsur): Keep this list in order with expected partitioning
        fields = ['ephemeral_mb', 'swap_mb', 'root_mb']
        variants = ((0, 0, 12), (4, 2, 8), (0, 4, 10), (5, 0, 10))
        for variant in variants:
            kwargs = dict(zip(fields, variant))
            self._run_without_root(disk_utils.work_on_disk,
                                   self.file.name, ephemeral_format='ext4',
                                   node_uuid='', image_path='path', **kwargs)
            part_table = self._run_without_root(
                disk_utils.list_partitions, self.file.name)
            for part, expected_size in zip(part_table, filter(None, variant)):
                self.assertEqual(expected_size, part['size'],
                                 "comparison failed for %s" % list(variant))

    def test_whole_disk(self):
        # 6 MiB ephemeral + 3 MiB swap + 9 MiB root + 1 MiB for MBR
        # + 1 MiB MAGIC == 20 MiB whole disk
        # TODO(dtantsur): figure out why we need 'magic' 1 more MiB
        # and why the is different on Ubuntu and Fedora (see below)
        self._run_without_root(disk_utils.work_on_disk, self.file.name,
                               root_mb=9, ephemeral_mb=6, swap_mb=3,
                               ephemeral_format='ext4', node_uuid='',
                               image_path='path')
        part_table = self._run_without_root(
            disk_utils.list_partitions, self.file.name)
        sizes = [part['size'] for part in part_table]
        # NOTE(dtantsur): parted in Ubuntu 12.04 will occupy the last MiB,
        # parted in Fedora 20 won't - thus two possible variants for last part
        self.assertEqual([6, 3], sizes[:2],
                         "unexpected partitioning %s" % part_table)
        self.assertIn(sizes[2], (9, 10))


@mock.patch.object(shutil, 'copyfileobj', autospec=True)
@mock.patch.object(requests, 'get', autospec=True)
class GetConfigdriveTestCase(base.IronicLibTestCase):

    @mock.patch.object(gzip, 'GzipFile', autospec=True)
    def test_get_configdrive(self, mock_gzip, mock_requests, mock_copy):
        mock_requests.return_value = mock.MagicMock(content='Zm9vYmFy')
        tempdir = tempfile.mkdtemp()
        (size, path) = disk_utils._get_configdrive('http://1.2.3.4/cd',
                                                   'fake-node-uuid',
                                                   tempdir=tempdir)
        self.assertTrue(path.startswith(tempdir))
        mock_requests.assert_called_once_with('http://1.2.3.4/cd')
        mock_gzip.assert_called_once_with('configdrive', 'rb',
                                          fileobj=mock.ANY)
        mock_copy.assert_called_once_with(mock.ANY, mock.ANY)

    @mock.patch.object(gzip, 'GzipFile', autospec=True)
    def test_get_configdrive_base64_string(self, mock_gzip, mock_requests,
                                           mock_copy):
        disk_utils._get_configdrive('Zm9vYmFy', 'fake-node-uuid')
        self.assertFalse(mock_requests.called)
        mock_gzip.assert_called_once_with('configdrive', 'rb',
                                          fileobj=mock.ANY)
        mock_copy.assert_called_once_with(mock.ANY, mock.ANY)

    def test_get_configdrive_bad_url(self, mock_requests, mock_copy):
        mock_requests.side_effect = requests.exceptions.RequestException
        self.assertRaises(exception.InstanceDeployFailure,
                          disk_utils._get_configdrive,
                          'http://1.2.3.4/cd', 'fake-node-uuid')
        self.assertFalse(mock_copy.called)

    @mock.patch.object(base64, 'decode_as_bytes', autospec=True)
    def test_get_configdrive_base64_error(self, mock_b64, mock_requests,
                                          mock_copy):
        mock_b64.side_effect = TypeError
        self.assertRaises(exception.InstanceDeployFailure,
                          disk_utils._get_configdrive,
                          'malformed', 'fake-node-uuid')
        mock_b64.assert_called_once_with('malformed')
        self.assertFalse(mock_copy.called)

    @mock.patch.object(gzip, 'GzipFile', autospec=True)
    def test_get_configdrive_gzip_error(self, mock_gzip, mock_requests,
                                        mock_copy):
        mock_requests.return_value = mock.MagicMock(content='Zm9vYmFy')
        mock_copy.side_effect = IOError
        self.assertRaises(exception.InstanceDeployFailure,
                          disk_utils._get_configdrive,
                          'http://1.2.3.4/cd', 'fake-node-uuid')
        mock_requests.assert_called_once_with('http://1.2.3.4/cd')
        mock_gzip.assert_called_once_with('configdrive', 'rb',
                                          fileobj=mock.ANY)
        mock_copy.assert_called_once_with(mock.ANY, mock.ANY)


@mock.patch('time.sleep', lambda sec: None)
class OtherFunctionTestCase(base.IronicLibTestCase):

    @mock.patch.object(os, 'stat', autospec=True)
    @mock.patch.object(stat, 'S_ISBLK', autospec=True)
    def test_is_block_device_works(self, mock_is_blk, mock_os):
        device = '/dev/disk/by-path/ip-1.2.3.4:5678-iscsi-iqn.fake-lun-9'
        mock_is_blk.return_value = True
        mock_os().st_mode = 10000
        self.assertTrue(disk_utils.is_block_device(device))
        mock_is_blk.assert_called_once_with(mock_os().st_mode)

    @mock.patch.object(os, 'stat', autospec=True)
    def test_is_block_device_raises(self, mock_os):
        device = '/dev/disk/by-path/ip-1.2.3.4:5678-iscsi-iqn.fake-lun-9'
        mock_os.side_effect = OSError
        self.assertRaises(exception.InstanceDeployFailure,
                          disk_utils.is_block_device, device)
        mock_os.assert_has_calls([mock.call(device)] * 3)

    @mock.patch.object(imageutils, 'QemuImgInfo', autospec=True)
    @mock.patch.object(os.path, 'exists', return_value=False, autospec=True)
    def test_qemu_img_info_path_doesnt_exist(self, path_exists_mock,
                                             qemu_img_info_mock):
        disk_utils.qemu_img_info('noimg')
        path_exists_mock.assert_called_once_with('noimg')
        qemu_img_info_mock.assert_called_once_with()

    @mock.patch.object(utils, 'execute', return_value=('out', 'err'),
                       autospec=True)
    @mock.patch.object(imageutils, 'QemuImgInfo', autospec=True)
    @mock.patch.object(os.path, 'exists', return_value=True, autospec=True)
    def test_qemu_img_info_path_exists(self, path_exists_mock,
                                       qemu_img_info_mock, execute_mock):
        disk_utils.qemu_img_info('img')
        path_exists_mock.assert_called_once_with('img')
        execute_mock.assert_called_once_with('env', 'LC_ALL=C', 'LANG=C',
                                             'qemu-img', 'info', 'img',
                                             prlimit=mock.ANY)
        qemu_img_info_mock.assert_called_once_with('out')

    @mock.patch.object(utils, 'execute', autospec=True)
    def test_convert_image(self, execute_mock):
        disk_utils.convert_image('source', 'dest', 'out_format')
        execute_mock.assert_called_once_with('qemu-img', 'convert', '-O',
                                             'out_format', 'source', 'dest',
                                             run_as_root=False,
                                             prlimit=mock.ANY)

    @mock.patch.object(os.path, 'getsize', autospec=True)
    @mock.patch.object(disk_utils, 'qemu_img_info', autospec=True)
    def test_get_image_mb(self, mock_qinfo, mock_getsize):
        mb = 1024 * 1024

        mock_getsize.return_value = 0
        type(mock_qinfo.return_value).virtual_size = mock.PropertyMock(
            return_value=0)
        self.assertEqual(0, disk_utils.get_image_mb('x', False))
        self.assertEqual(0, disk_utils.get_image_mb('x', True))
        mock_getsize.return_value = 1
        type(mock_qinfo.return_value).virtual_size = mock.PropertyMock(
            return_value=1)
        self.assertEqual(1, disk_utils.get_image_mb('x', False))
        self.assertEqual(1, disk_utils.get_image_mb('x', True))
        mock_getsize.return_value = mb
        type(mock_qinfo.return_value).virtual_size = mock.PropertyMock(
            return_value=mb)
        self.assertEqual(1, disk_utils.get_image_mb('x', False))
        self.assertEqual(1, disk_utils.get_image_mb('x', True))
        mock_getsize.return_value = mb + 1
        type(mock_qinfo.return_value).virtual_size = mock.PropertyMock(
            return_value=mb + 1)
        self.assertEqual(2, disk_utils.get_image_mb('x', False))
        self.assertEqual(2, disk_utils.get_image_mb('x', True))

    def _test_count_mbr_partitions(self, output, mock_execute):
        mock_execute.return_value = (output, '')
        out = disk_utils.count_mbr_partitions('/dev/fake')
        mock_execute.assert_called_once_with('partprobe', '-d', '-s',
                                             '/dev/fake', run_as_root=True,
                                             use_standard_locale=True)
        return out

    @mock.patch.object(utils, 'execute', autospec=True)
    def test_count_mbr_partitions(self, mock_execute):
        output = "/dev/fake: msdos partitions 1 2 3 <5 6>"
        pp, lp = self._test_count_mbr_partitions(output, mock_execute)
        self.assertEqual(3, pp)
        self.assertEqual(2, lp)

    @mock.patch.object(utils, 'execute', autospec=True)
    def test_count_mbr_partitions_no_logical_partitions(self, mock_execute):
        output = "/dev/fake: msdos partitions 1 2"
        pp, lp = self._test_count_mbr_partitions(output, mock_execute)
        self.assertEqual(2, pp)
        self.assertEqual(0, lp)

    @mock.patch.object(utils, 'execute', autospec=True)
    def test_count_mbr_partitions_wrong_partition_table(self, mock_execute):
        output = "/dev/fake: gpt partitions 1 2 3 4 5 6"
        mock_execute.return_value = (output, '')
        self.assertRaises(ValueError, disk_utils.count_mbr_partitions,
                          '/dev/fake')
        mock_execute.assert_called_once_with('partprobe', '-d', '-s',
                                             '/dev/fake', run_as_root=True,
                                             use_standard_locale=True)

    @mock.patch.object(utils, 'execute', autospec=True)
    def test_block_uuid_fallback_to_uuid(self, mock_execute):
        mock_execute.side_effect = [('', ''),
                                    ('value', '')]
        self.assertEqual('value',
                         disk_utils.block_uuid('/dev/fake'))
        execute_calls = [
            mock.call('blkid', '-s', 'UUID', '-o', 'value',
                      '/dev/fake', check_exit_code=[0],
                      run_as_root=True),
            mock.call('blkid', '-s', 'PARTUUID', '-o', 'value',
                      '/dev/fake', check_exit_code=[0],
                      run_as_root=True)
        ]
        mock_execute.assert_has_calls(execute_calls)


@mock.patch.object(utils, 'execute', autospec=True)
class WholeDiskPartitionTestCases(base.IronicLibTestCase):

    def setUp(self):
        super(WholeDiskPartitionTestCases, self).setUp()
        self.dev = "/dev/fake"
        self.config_part_label = "config-2"
        self.node_uuid = "12345678-1234-1234-1234-1234567890abcxyz"

    def test_get_partition_present(self, mock_execute):
        lsblk_output = 'NAME="fake12" LABEL="config-2"\n'
        part_result = '/dev/fake12'
        mock_execute.side_effect = [(None, ''), (lsblk_output, '')]
        result = disk_utils._get_labelled_partition(self.dev,
                                                    self.config_part_label,
                                                    self.node_uuid)
        self.assertEqual(part_result, result)
        execute_calls = [
            mock.call('partprobe', self.dev, run_as_root=True, attempts=10),
            mock.call('lsblk', '-Po', 'name,label', self.dev,
                      check_exit_code=[0, 1],
                      use_standard_locale=True, run_as_root=True)
        ]
        mock_execute.assert_has_calls(execute_calls)

    def test_get_partition_present_uppercase(self, mock_execute):
        lsblk_output = 'NAME="fake12" LABEL="CONFIG-2"\n'
        part_result = '/dev/fake12'
        mock_execute.side_effect = [(None, ''), (lsblk_output, '')]
        result = disk_utils._get_labelled_partition(self.dev,
                                                    self.config_part_label,
                                                    self.node_uuid)
        self.assertEqual(part_result, result)
        execute_calls = [
            mock.call('partprobe', self.dev, run_as_root=True, attempts=10),
            mock.call('lsblk', '-Po', 'name,label', self.dev,
                      check_exit_code=[0, 1],
                      use_standard_locale=True, run_as_root=True)
        ]
        mock_execute.assert_has_calls(execute_calls)

    def test_get_partition_absent(self, mock_execute):
        mock_execute.side_effect = [(None, ''),
                                    (None, '')]
        result = disk_utils._get_labelled_partition(self.dev,
                                                    self.config_part_label,
                                                    self.node_uuid)
        self.assertIsNone(result)
        execute_calls = [
            mock.call('partprobe', self.dev, run_as_root=True, attempts=10),
            mock.call('lsblk', '-Po', 'name,label', self.dev,
                      check_exit_code=[0, 1],
                      use_standard_locale=True, run_as_root=True)
        ]
        mock_execute.assert_has_calls(execute_calls)

    def test_get_partition_DeployFail_exc(self, mock_execute):
        label = 'config-2'
        lsblk_output = ('NAME="fake12" LABEL="%s"\n'
                        'NAME="fake13" LABEL="%s"\n' %
                        (label, label))
        mock_execute.side_effect = [(None, ''), (lsblk_output, '')]
        self.assertRaisesRegex(exception.InstanceDeployFailure,
                               'fake .*fake12 .*fake13',
                               disk_utils._get_labelled_partition, self.dev,
                               self.config_part_label, self.node_uuid)
        execute_calls = [
            mock.call('partprobe', self.dev, run_as_root=True, attempts=10),
            mock.call('lsblk', '-Po', 'name,label', self.dev,
                      check_exit_code=[0, 1],
                      use_standard_locale=True, run_as_root=True)
        ]
        mock_execute.assert_has_calls(execute_calls)

    @mock.patch.object(disk_utils.LOG, 'error', autospec=True)
    def test_get_partition_exc(self, mock_log, mock_execute):
        mock_execute.side_effect = processutils.ProcessExecutionError
        self.assertRaisesRegex(exception.InstanceDeployFailure,
                               'Failed to retrieve partition labels',
                               disk_utils._get_labelled_partition, self.dev,
                               self.config_part_label, self.node_uuid)
        mock_execute.assert_called_once_with(
            'partprobe', self.dev, run_as_root=True, attempts=10)
        self.assertEqual(1, mock_log.call_count)

    def _test_is_disk_larger_than_max_size(self, mock_execute, blk_out):
        mock_execute.return_value = ('%s\n' % blk_out, '')
        result = disk_utils._is_disk_larger_than_max_size(self.dev,
                                                          self.node_uuid)
        mock_execute.assert_called_once_with('blockdev', '--getsize64',
                                             '/dev/fake', run_as_root=True,
                                             use_standard_locale=True)
        return result

    def test_is_disk_larger_than_max_size_false(self, mock_execute):
        blkid_out = "53687091200"
        ret = self._test_is_disk_larger_than_max_size(mock_execute,
                                                      blk_out=blkid_out)
        self.assertFalse(ret)

    def test_is_disk_larger_than_max_size_true(self, mock_execute):
        blkid_out = "4398046511104"
        ret = self._test_is_disk_larger_than_max_size(mock_execute,
                                                      blk_out=blkid_out)
        self.assertTrue(ret)

    @mock.patch.object(disk_utils.LOG, 'error', autospec=True)
    def test_is_disk_larger_than_max_size_exc(self, mock_log, mock_execute):
        mock_execute.side_effect = processutils.ProcessExecutionError
        self.assertRaisesRegex(exception.InstanceDeployFailure,
                               'Failed to get size of disk',
                               disk_utils._is_disk_larger_than_max_size,
                               self.dev, self.node_uuid)
        mock_execute.assert_called_once_with('blockdev', '--getsize64',
                                             '/dev/fake', run_as_root=True,
                                             use_standard_locale=True)
        self.assertEqual(1, mock_log.call_count)

    def test__is_disk_gpt_partitioned_true(self, mock_execute):
        blkid_output = 'gpt\n'
        mock_execute.return_value = (blkid_output, '')
        result = disk_utils._is_disk_gpt_partitioned('/dev/fake',
                                                     self.node_uuid)
        self.assertTrue(result)
        mock_execute.assert_called_once_with('blkid', '-p', '-o', 'value',
                                             '-s', 'PTTYPE', '/dev/fake',
                                             use_standard_locale=True,
                                             run_as_root=True)

    def test_is_disk_gpt_partitioned_false(self, mock_execute):
        blkid_output = 'dos\n'
        mock_execute.return_value = (blkid_output, '')
        result = disk_utils._is_disk_gpt_partitioned('/dev/fake',
                                                     self.node_uuid)
        self.assertFalse(result)
        mock_execute.assert_called_once_with('blkid', '-p', '-o', 'value',
                                             '-s', 'PTTYPE', '/dev/fake',
                                             use_standard_locale=True,
                                             run_as_root=True)

    @mock.patch.object(disk_utils.LOG, 'error', autospec=True)
    def test_is_disk_gpt_partitioned_exc(self, mock_log, mock_execute):
        mock_execute.side_effect = processutils.ProcessExecutionError
        self.assertRaisesRegex(exception.InstanceDeployFailure,
                               'Failed to retrieve partition table type',
                               disk_utils._is_disk_gpt_partitioned,
                               self.dev, self.node_uuid)
        mock_execute.assert_called_once_with('blkid', '-p', '-o', 'value',
                                             '-s', 'PTTYPE', '/dev/fake',
                                             use_standard_locale=True,
                                             run_as_root=True)
        self.assertEqual(1, mock_log.call_count)

    def test_fix_gpt_structs_fix_required(self, mock_execute):
        sgdisk_v_output = """
Problem: The secondary header's self-pointer indicates that it doesn't reside
at the end of the disk. If you've added a disk to a RAID array, use the 'e'
option on the experts' menu to adjust the secondary header's and partition
table's locations.

Identified 1 problems!
"""
        mock_execute.return_value = (sgdisk_v_output, '')
        execute_calls = [
            mock.call('sgdisk', '-v', '/dev/fake', run_as_root=True),
            mock.call('sgdisk', '-e', '/dev/fake', run_as_root=True)
        ]
        disk_utils._fix_gpt_structs('/dev/fake', self.node_uuid)
        mock_execute.assert_has_calls(execute_calls)

    def test_fix_gpt_structs_fix_not_required(self, mock_execute):
        mock_execute.return_value = ('', '')

        disk_utils._fix_gpt_structs('/dev/fake', self.node_uuid)
        mock_execute.assert_called_once_with('sgdisk', '-v', '/dev/fake',
                                             run_as_root=True)

    @mock.patch.object(disk_utils.LOG, 'error', autospec=True)
    def test_fix_gpt_structs_exc(self, mock_log, mock_execute):
        mock_execute.side_effect = processutils.ProcessExecutionError
        self.assertRaisesRegex(exception.InstanceDeployFailure,
                               'Failed to fix GPT data structures on disk',
                               disk_utils._fix_gpt_structs,
                               self.dev, self.node_uuid)
        mock_execute.assert_called_once_with('sgdisk', '-v', '/dev/fake',
                                             run_as_root=True)
        self.assertEqual(1, mock_log.call_count)


class WholeDiskConfigDriveTestCases(base.IronicLibTestCase):

    def setUp(self):
        super(WholeDiskConfigDriveTestCases, self).setUp()
        self.dev = "/dev/fake"
        self.config_part_label = "config-2"
        self.node_uuid = "12345678-1234-1234-1234-1234567890abcxyz"

    @mock.patch.object(utils, 'execute', autospec=True)
    @mock.patch.object(utils, 'unlink_without_raise',
                       autospec=True)
    @mock.patch.object(disk_utils, 'dd',
                       autospec=True)
    @mock.patch.object(disk_utils, 'fix_gpt_partition',
                       autospec=True)
    @mock.patch.object(disk_utils, '_fix_gpt_structs',
                       autospec=True)
    @mock.patch.object(disk_utils, '_is_disk_gpt_partitioned',
                       autospec=True)
    @mock.patch.object(disk_utils, 'list_partitions',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_labelled_partition',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_configdrive',
                       autospec=True)
    def test_create_partition_exists(self, mock_get_configdrive,
                                     mock_get_labelled_partition,
                                     mock_list_partitions, mock_is_disk_gpt,
                                     mock_fix_gpt, mock_fix_gpt_partition,
                                     mock_dd, mock_unlink, mock_execute):
        config_url = 'http://1.2.3.4/cd'
        configdrive_part = '/dev/fake-part1'
        configdrive_file = '/tmp/xyz'
        configdrive_mb = 10

        mock_get_labelled_partition.return_value = configdrive_part
        mock_get_configdrive.return_value = (configdrive_mb, configdrive_file)
        disk_utils.create_config_drive_partition(self.node_uuid, self.dev,
                                                 config_url)
        mock_fix_gpt_partition.assert_called_with(self.dev, self.node_uuid)
        mock_get_configdrive.assert_called_with(config_url, self.node_uuid)
        mock_get_labelled_partition.assert_called_with(self.dev,
                                                       self.config_part_label,
                                                       self.node_uuid)
        self.assertFalse(mock_list_partitions.called)
        self.assertFalse(mock_execute.called)
        self.assertFalse(mock_is_disk_gpt.called)
        self.assertFalse(mock_fix_gpt.called)
        mock_dd.assert_called_with(configdrive_file, configdrive_part)
        mock_unlink.assert_called_with(configdrive_file)

    @mock.patch.object(utils, 'execute', autospec=True)
    @mock.patch.object(utils, 'unlink_without_raise',
                       autospec=True)
    @mock.patch.object(disk_utils, 'dd',
                       autospec=True)
    @mock.patch.object(disk_utils, 'fix_gpt_partition',
                       autospec=True)
    @mock.patch.object(disk_utils, '_fix_gpt_structs',
                       autospec=True)
    @mock.patch.object(disk_utils, '_is_disk_gpt_partitioned',
                       autospec=True)
    @mock.patch.object(disk_utils, 'list_partitions',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_labelled_partition',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_configdrive',
                       autospec=True)
    def test_create_partition_gpt(self, mock_get_configdrive,
                                  mock_get_labelled_partition,
                                  mock_list_partitions, mock_is_disk_gpt,
                                  mock_fix_gpt, mock_fix_gpt_partition,
                                  mock_dd, mock_unlink, mock_execute):
        config_url = 'http://1.2.3.4/cd'
        configdrive_file = '/tmp/xyz'
        configdrive_mb = 10

        initial_partitions = [{'end': 49152, 'number': 1, 'start': 1,
                               'flags': 'boot', 'filesystem': 'ext4',
                               'size': 49151},
                              {'end': 51099, 'number': 3, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046},
                              {'end': 51099, 'number': 5, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046}]
        updated_partitions = [{'end': 49152, 'number': 1, 'start': 1,
                               'flags': 'boot', 'filesystem': 'ext4',
                               'size': 49151},
                              {'end': 51099, 'number': 3, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046},
                              {'end': 51099, 'number': 4, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046},
                              {'end': 51099, 'number': 5, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046}]

        mock_get_configdrive.return_value = (configdrive_mb, configdrive_file)
        mock_get_labelled_partition.return_value = None

        mock_is_disk_gpt.return_value = True
        mock_list_partitions.side_effect = [initial_partitions,
                                            updated_partitions]
        expected_part = '/dev/fake4'
        disk_utils.create_config_drive_partition(self.node_uuid, self.dev,
                                                 config_url)
        mock_execute.assert_has_calls([
            mock.call('sgdisk', '-n', '0:-64MB:0', self.dev,
                      run_as_root=True),
            mock.call('udevadm', 'settle'),
            mock.call('test', '-e', expected_part, attempts=15,
                      check_exit_code=[0], delay_on_retry=True)
        ])

        self.assertEqual(2, mock_list_partitions.call_count)
        mock_is_disk_gpt.assert_called_with(self.dev, self.node_uuid)
        mock_fix_gpt_partition.assert_called_with(self.dev, self.node_uuid)
        self.assertFalse(mock_fix_gpt.called)
        mock_fix_gpt_partition.assert_called_with(self.dev, self.node_uuid)
        mock_dd.assert_called_with(configdrive_file, expected_part)
        mock_unlink.assert_called_with(configdrive_file)

    @mock.patch.object(disk_utils, 'count_mbr_partitions', autospec=True)
    @mock.patch.object(utils, 'execute', autospec=True)
    @mock.patch.object(disk_utils.LOG, 'warning', autospec=True)
    @mock.patch.object(utils, 'unlink_without_raise',
                       autospec=True)
    @mock.patch.object(disk_utils, 'dd',
                       autospec=True)
    @mock.patch.object(disk_utils, '_is_disk_larger_than_max_size',
                       autospec=True)
    @mock.patch.object(disk_utils, 'fix_gpt_partition',
                       autospec=True)
    @mock.patch.object(disk_utils, '_fix_gpt_structs',
                       autospec=True)
    @mock.patch.object(disk_utils, '_is_disk_gpt_partitioned',
                       autospec=True)
    @mock.patch.object(disk_utils, 'list_partitions',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_labelled_partition',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_configdrive',
                       autospec=True)
    def _test_create_partition_mbr(self, mock_get_configdrive,
                                   mock_get_labelled_partition,
                                   mock_list_partitions,
                                   mock_is_disk_gpt, mock_fix_gpt,
                                   mock_fix_gpt_partition,
                                   mock_disk_exceeds, mock_dd,
                                   mock_unlink, mock_log, mock_execute,
                                   mock_count, disk_size_exceeds_max=False,
                                   is_iscsi_device=False,
                                   is_nvme_device=False):
        config_url = 'http://1.2.3.4/cd'
        configdrive_file = '/tmp/xyz'
        configdrive_mb = 10
        mock_disk_exceeds.return_value = disk_size_exceeds_max

        initial_partitions = [{'end': 49152, 'number': 1, 'start': 1,
                               'flags': 'boot', 'filesystem': 'ext4',
                               'size': 49151},
                              {'end': 51099, 'number': 3, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046},
                              {'end': 51099, 'number': 5, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046}]
        updated_partitions = [{'end': 49152, 'number': 1, 'start': 1,
                               'flags': 'boot', 'filesystem': 'ext4',
                               'size': 49151},
                              {'end': 51099, 'number': 3, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046},
                              {'end': 51099, 'number': 4, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046},
                              {'end': 51099, 'number': 5, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046}]
        mock_list_partitions.side_effect = [initial_partitions,
                                            updated_partitions]
        # 2 primary partitions, 0 logical partitions
        mock_count.return_value = (2, 0)
        mock_get_configdrive.return_value = (configdrive_mb, configdrive_file)
        mock_get_labelled_partition.return_value = None
        mock_is_disk_gpt.return_value = False

        self.node_uuid = "12345678-1234-1234-1234-1234567890abcxyz"
        if is_iscsi_device:
            self.dev = ('/dev/iqn.2008-10.org.openstack:%s.fake' %
                        self.node_uuid)
            expected_part = '%s-part4' % self.dev
        elif is_nvme_device:
            self.dev = '/dev/nvmefake'
            expected_part = '%sp4' % self.dev
        else:
            expected_part = '/dev/fake4'

        disk_utils.create_config_drive_partition(self.node_uuid, self.dev,
                                                 config_url)
        mock_get_configdrive.assert_called_with(config_url, self.node_uuid)
        if disk_size_exceeds_max:
            self.assertEqual(1, mock_log.call_count)
            parted_call = mock.call('parted', '-a', 'optimal', '-s',
                                    '--', self.dev, 'mkpart',
                                    'primary', 'fat32', 2097087,
                                    2097151, run_as_root=True)
        else:
            self.assertEqual(0, mock_log.call_count)
            parted_call = mock.call('parted', '-a', 'optimal', '-s',
                                    '--', self.dev, 'mkpart',
                                    'primary', 'fat32', '-64MiB',
                                    '-0', run_as_root=True)
        mock_execute.assert_has_calls([
            parted_call,
            mock.call('udevadm', 'settle'),
            mock.call('test', '-e', expected_part, attempts=15,
                      check_exit_code=[0], delay_on_retry=True)
        ])
        self.assertEqual(2, mock_list_partitions.call_count)
        mock_is_disk_gpt.assert_called_with(self.dev, self.node_uuid)
        mock_fix_gpt_partition.assert_called_with(self.dev, self.node_uuid)
        mock_disk_exceeds.assert_called_with(self.dev, self.node_uuid)
        mock_dd.assert_called_with(configdrive_file, expected_part)
        mock_unlink.assert_called_with(configdrive_file)
        self.assertFalse(mock_fix_gpt.called)
        self.assertFalse(mock_fix_gpt.called)
        mock_count.assert_called_with(self.dev)

    def test__create_partition_mbr_disk_under_2TB(self):
        self._test_create_partition_mbr(disk_size_exceeds_max=False,
                                        is_iscsi_device=True,
                                        is_nvme_device=False)

    def test__create_partition_mbr_disk_under_2TB_nvme(self):
        self._test_create_partition_mbr(disk_size_exceeds_max=False,
                                        is_iscsi_device=False,
                                        is_nvme_device=True)

    def test__create_partition_mbr_disk_exceeds_2TB(self):
        self._test_create_partition_mbr(disk_size_exceeds_max=True,
                                        is_iscsi_device=False,
                                        is_nvme_device=False)

    def test__create_partition_mbr_disk_exceeds_2TB_nvme(self):
        self._test_create_partition_mbr(disk_size_exceeds_max=True,
                                        is_iscsi_device=False,
                                        is_nvme_device=True)

    @mock.patch.object(disk_utils, 'count_mbr_partitions', autospec=True)
    @mock.patch.object(utils, 'execute', autospec=True)
    @mock.patch.object(utils, 'unlink_without_raise',
                       autospec=True)
    @mock.patch.object(disk_utils, 'dd',
                       autospec=True)
    @mock.patch.object(disk_utils, '_is_disk_larger_than_max_size',
                       autospec=True)
    @mock.patch.object(disk_utils, 'fix_gpt_partition',
                       autospec=True)
    @mock.patch.object(disk_utils, '_fix_gpt_structs',
                       autospec=True)
    @mock.patch.object(disk_utils, '_is_disk_gpt_partitioned',
                       autospec=True)
    @mock.patch.object(disk_utils, 'list_partitions',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_labelled_partition',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_configdrive',
                       autospec=True)
    def test_create_partition_part_create_fail(self, mock_get_configdrive,
                                               mock_get_labelled_partition,
                                               mock_list_partitions,
                                               mock_is_disk_gpt, mock_fix_gpt,
                                               mock_fix_gpt_partition,
                                               mock_disk_exceeds, mock_dd,
                                               mock_unlink, mock_execute,
                                               mock_count):
        config_url = 'http://1.2.3.4/cd'
        configdrive_file = '/tmp/xyz'
        configdrive_mb = 10

        initial_partitions = [{'end': 49152, 'number': 1, 'start': 1,
                               'flags': 'boot', 'filesystem': 'ext4',
                               'size': 49151},
                              {'end': 51099, 'number': 3, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046},
                              {'end': 51099, 'number': 5, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046}]
        updated_partitions = [{'end': 49152, 'number': 1, 'start': 1,
                               'flags': 'boot', 'filesystem': 'ext4',
                               'size': 49151},
                              {'end': 51099, 'number': 3, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046},
                              {'end': 51099, 'number': 5, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046}]
        mock_get_configdrive.return_value = (configdrive_mb, configdrive_file)
        mock_get_labelled_partition.return_value = None
        mock_is_disk_gpt.return_value = False
        mock_disk_exceeds.return_value = False
        mock_list_partitions.side_effect = [initial_partitions,
                                            initial_partitions,
                                            updated_partitions]
        # 2 primary partitions, 0 logical partitions
        mock_count.return_value = (2, 0)

        self.assertRaisesRegex(exception.InstanceDeployFailure,
                               'Disk partitioning failed on device',
                               disk_utils.create_config_drive_partition,
                               self.node_uuid, self.dev, config_url)

        mock_get_configdrive.assert_called_with(config_url, self.node_uuid)
        mock_execute.assert_called_with('parted', '-a', 'optimal', '-s', '--',
                                        self.dev, 'mkpart', 'primary',
                                        'fat32', '-64MiB', '-0',
                                        run_as_root=True)
        self.assertEqual(2, mock_list_partitions.call_count)
        mock_fix_gpt_partition.assert_called_with(self.dev, self.node_uuid)
        mock_is_disk_gpt.assert_called_with(self.dev, self.node_uuid)
        mock_disk_exceeds.assert_called_with(self.dev, self.node_uuid)
        self.assertFalse(mock_fix_gpt.called)
        self.assertFalse(mock_dd.called)
        mock_unlink.assert_called_with(configdrive_file)
        mock_count.assert_called_once_with(self.dev)

    @mock.patch.object(disk_utils, 'count_mbr_partitions', autospec=True)
    @mock.patch.object(utils, 'execute', autospec=True)
    @mock.patch.object(utils, 'unlink_without_raise',
                       autospec=True)
    @mock.patch.object(disk_utils, 'dd',
                       autospec=True)
    @mock.patch.object(disk_utils, '_is_disk_larger_than_max_size',
                       autospec=True)
    @mock.patch.object(disk_utils, 'fix_gpt_partition',
                       autospec=True)
    @mock.patch.object(disk_utils, '_fix_gpt_structs',
                       autospec=True)
    @mock.patch.object(disk_utils, '_is_disk_gpt_partitioned',
                       autospec=True)
    @mock.patch.object(disk_utils, 'list_partitions',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_labelled_partition',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_configdrive',
                       autospec=True)
    def test_create_partition_part_create_exc(self, mock_get_configdrive,
                                              mock_get_labelled_partition,
                                              mock_list_partitions,
                                              mock_is_disk_gpt, mock_fix_gpt,
                                              mock_fix_gpt_partition,
                                              mock_disk_exceeds, mock_dd,
                                              mock_unlink, mock_execute,
                                              mock_count):
        config_url = 'http://1.2.3.4/cd'
        configdrive_file = '/tmp/xyz'
        configdrive_mb = 10

        initial_partitions = [{'end': 49152, 'number': 1, 'start': 1,
                               'flags': 'boot', 'filesystem': 'ext4',
                               'size': 49151},
                              {'end': 51099, 'number': 3, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046},
                              {'end': 51099, 'number': 5, 'start': 49153,
                               'flags': '', 'filesystem': '', 'size': 2046}]
        mock_get_configdrive.return_value = (configdrive_mb, configdrive_file)
        mock_get_labelled_partition.return_value = None
        mock_is_disk_gpt.return_value = False
        mock_disk_exceeds.return_value = False
        mock_list_partitions.side_effect = [initial_partitions,
                                            initial_partitions]
        # 2 primary partitions, 0 logical partitions
        mock_count.return_value = (2, 0)

        mock_execute.side_effect = processutils.ProcessExecutionError

        self.assertRaisesRegex(exception.InstanceDeployFailure,
                               'Failed to create config drive on disk',
                               disk_utils.create_config_drive_partition,
                               self.node_uuid, self.dev, config_url)

        mock_get_configdrive.assert_called_with(config_url, self.node_uuid)
        mock_execute.assert_called_with('parted', '-a', 'optimal', '-s', '--',
                                        self.dev, 'mkpart', 'primary',
                                        'fat32', '-64MiB', '-0',
                                        run_as_root=True)
        self.assertEqual(1, mock_list_partitions.call_count)
        mock_fix_gpt_partition.assert_called_with(self.dev, self.node_uuid)
        mock_is_disk_gpt.assert_called_with(self.dev, self.node_uuid)
        mock_disk_exceeds.assert_called_with(self.dev, self.node_uuid)
        self.assertFalse(mock_fix_gpt.called)
        self.assertFalse(mock_dd.called)
        mock_unlink.assert_called_with(configdrive_file)
        mock_count.assert_called_once_with(self.dev)

    @mock.patch.object(disk_utils, 'count_mbr_partitions', autospec=True)
    @mock.patch.object(utils, 'unlink_without_raise',
                       autospec=True)
    @mock.patch.object(disk_utils, 'dd',
                       autospec=True)
    @mock.patch.object(disk_utils, 'fix_gpt_partition',
                       autospec=True)
    @mock.patch.object(disk_utils, '_fix_gpt_structs',
                       autospec=True)
    @mock.patch.object(disk_utils, '_is_disk_gpt_partitioned',
                       autospec=True)
    @mock.patch.object(disk_utils, 'list_partitions',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_labelled_partition',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_configdrive',
                       autospec=True)
    def test_create_partition_num_parts_exceed(self, mock_get_configdrive,
                                               mock_get_labelled_partition,
                                               mock_list_partitions,
                                               mock_is_disk_gpt, mock_fix_gpt,
                                               mock_fix_gpt_partition,
                                               mock_dd, mock_unlink,
                                               mock_count):
        config_url = 'http://1.2.3.4/cd'
        configdrive_file = '/tmp/xyz'
        configdrive_mb = 10

        partitions = [{'end': 49152, 'number': 1, 'start': 1,
                       'flags': 'boot', 'filesystem': 'ext4',
                       'size': 49151},
                      {'end': 51099, 'number': 2, 'start': 49153,
                       'flags': '', 'filesystem': '', 'size': 2046},
                      {'end': 51099, 'number': 3, 'start': 49153,
                       'flags': '', 'filesystem': '', 'size': 2046},
                      {'end': 51099, 'number': 4, 'start': 49153,
                       'flags': '', 'filesystem': '', 'size': 2046}]
        mock_get_configdrive.return_value = (configdrive_mb, configdrive_file)
        mock_get_labelled_partition.return_value = None
        mock_is_disk_gpt.return_value = False
        mock_list_partitions.side_effect = [partitions, partitions]
        # 4 primary partitions, 0 logical partitions
        mock_count.return_value = (4, 0)

        self.assertRaisesRegex(exception.InstanceDeployFailure,
                               'Config drive cannot be created for node',
                               disk_utils.create_config_drive_partition,
                               self.node_uuid, self.dev, config_url)

        mock_get_configdrive.assert_called_with(config_url, self.node_uuid)
        self.assertEqual(1, mock_list_partitions.call_count)
        mock_fix_gpt_partition.assert_called_with(self.dev, self.node_uuid)
        mock_is_disk_gpt.assert_called_with(self.dev, self.node_uuid)
        self.assertFalse(mock_fix_gpt.called)
        self.assertFalse(mock_dd.called)
        mock_unlink.assert_called_with(configdrive_file)
        mock_count.assert_called_once_with(self.dev)

    @mock.patch.object(utils, 'execute', autospec=True)
    @mock.patch.object(utils, 'unlink_without_raise',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_labelled_partition',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_configdrive',
                       autospec=True)
    def test_create_partition_conf_drive_sz_exceed(self, mock_get_configdrive,
                                                   mock_get_labelled_partition,
                                                   mock_unlink, mock_execute):
        config_url = 'http://1.2.3.4/cd'
        configdrive_file = '/tmp/xyz'
        configdrive_mb = 65

        mock_get_configdrive.return_value = (configdrive_mb, configdrive_file)
        mock_get_labelled_partition.return_value = None

        self.assertRaisesRegex(exception.InstanceDeployFailure,
                               'Config drive size exceeds maximum limit',
                               disk_utils.create_config_drive_partition,
                               self.node_uuid, self.dev, config_url)

        mock_get_configdrive.assert_called_with(config_url, self.node_uuid)
        mock_unlink.assert_called_with(configdrive_file)

    @mock.patch.object(disk_utils, 'count_mbr_partitions', autospec=True)
    @mock.patch.object(utils, 'execute', autospec=True)
    @mock.patch.object(utils, 'unlink_without_raise',
                       autospec=True)
    @mock.patch.object(disk_utils, 'fix_gpt_partition',
                       autospec=True)
    @mock.patch.object(disk_utils, '_is_disk_gpt_partitioned',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_labelled_partition',
                       autospec=True)
    @mock.patch.object(disk_utils, '_get_configdrive',
                       autospec=True)
    def test_create_partition_conf_drive_error_counting(
            self, mock_get_configdrive, mock_get_labelled_partition,
            mock_is_disk_gpt, mock_fix_gpt_partition,
            mock_unlink, mock_execute, mock_count):
        config_url = 'http://1.2.3.4/cd'
        configdrive_file = '/tmp/xyz'
        configdrive_mb = 10

        mock_get_configdrive.return_value = (configdrive_mb, configdrive_file)
        mock_get_labelled_partition.return_value = None
        mock_is_disk_gpt.return_value = False
        mock_count.side_effect = ValueError('Booooom')

        self.assertRaisesRegex(exception.InstanceDeployFailure,
                               'Failed to check the number of primary ',
                               disk_utils.create_config_drive_partition,
                               self.node_uuid, self.dev, config_url)

        mock_get_configdrive.assert_called_with(config_url, self.node_uuid)
        mock_unlink.assert_called_with(configdrive_file)
        mock_fix_gpt_partition.assert_called_with(self.dev, self.node_uuid)
        mock_is_disk_gpt.assert_called_with(self.dev, self.node_uuid)
        mock_count.assert_called_once_with(self.dev)




===========Repository Name===========
ironic-lib
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ironic-lib\ironic_lib\tests\test_metrics.py
===========File Type===========
.py
===========File Content===========
# Copyright 2016 Rackspace Hosting
# All Rights Reserved
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import types

import mock
from oslo_utils import reflection

from ironic_lib import metrics as metricslib
from ironic_lib import metrics_utils
from ironic_lib.tests import base


METRICS = metrics_utils.get_metrics_logger(prefix='foo', backend='noop')


@METRICS.timer('testing1')
def timer_check(run, timer=None):
    pass


@METRICS.counter('testing2')
def counter_check(run, counter=None):
    pass


@METRICS.gauge('testing2')
def gauge_check(run, gauge=None):
    pass


class MockedMetricLogger(metricslib.MetricLogger):
    _gauge = mock.Mock(spec_set=types.FunctionType)
    _counter = mock.Mock(spec_set=types.FunctionType)
    _timer = mock.Mock(spec_set=types.FunctionType)


class TestMetricReflection(base.IronicLibTestCase):
    def test_timer_reflection(self):
        # Ensure our decorator is done correctly (six.wraps) and we can get the
        # arguments of our decorated function.
        expected = ['run', 'timer']
        signature = reflection.get_signature(timer_check)
        parameters = list(signature.parameters)
        self.assertEqual(expected, parameters)

    def test_counter_reflection(self):
        # Ensure our decorator is done correctly (six.wraps) and we can get the
        # arguments of our decorated function.
        expected = ['run', 'counter']
        signature = reflection.get_signature(counter_check)
        parameters = list(signature.parameters)
        self.assertEqual(expected, parameters)

    def test_gauge_reflection(self):
        # Ensure our decorator is done correctly (six.wraps) and we can get the
        # arguments of our decorated function.
        expected = ['run', 'gauge']
        signature = reflection.get_signature(gauge_check)
        parameters = list(signature.parameters)
        self.assertEqual(expected, parameters)


class TestMetricLogger(base.IronicLibTestCase):
    def setUp(self):
        super(TestMetricLogger, self).setUp()
        self.ml = MockedMetricLogger('prefix', '.')
        self.ml_no_prefix = MockedMetricLogger('', '.')
        self.ml_other_delim = MockedMetricLogger('prefix', '*')
        self.ml_default = MockedMetricLogger()

    def test_init(self):
        self.assertEqual(self.ml._prefix, 'prefix')
        self.assertEqual(self.ml._delimiter, '.')

        self.assertEqual(self.ml_no_prefix._prefix, '')
        self.assertEqual(self.ml_other_delim._delimiter, '*')
        self.assertEqual(self.ml_default._prefix, '')

    def test_get_metric_name(self):
        self.assertEqual(
            self.ml.get_metric_name('metric'),
            'prefix.metric')

        self.assertEqual(
            self.ml_no_prefix.get_metric_name('metric'),
            'metric')

        self.assertEqual(
            self.ml_other_delim.get_metric_name('metric'),
            'prefix*metric')

    def test_send_gauge(self):
        self.ml.send_gauge('prefix.metric', 10)
        self.ml._gauge.assert_called_once_with('prefix.metric', 10)

    def test_send_counter(self):
        self.ml.send_counter('prefix.metric', 10)
        self.ml._counter.assert_called_once_with(
            'prefix.metric', 10,
            sample_rate=None)
        self.ml._counter.reset_mock()

        self.ml.send_counter('prefix.metric', 10, sample_rate=1.0)
        self.ml._counter.assert_called_once_with(
            'prefix.metric', 10,
            sample_rate=1.0)
        self.ml._counter.reset_mock()

        self.ml.send_counter('prefix.metric', 10, sample_rate=0.0)
        self.assertFalse(self.ml._counter.called)

    def test_send_timer(self):
        self.ml.send_timer('prefix.metric', 10)
        self.ml._timer.assert_called_once_with('prefix.metric', 10)

    @mock.patch('ironic_lib.metrics._time', autospec=True)
    @mock.patch('ironic_lib.metrics.MetricLogger.send_timer', autospec=True)
    def test_decorator_timer(self, mock_timer, mock_time):
        mock_time.side_effect = [1, 43]

        @self.ml.timer('foo.bar.baz')
        def func(x):
            return x * x

        func(10)

        mock_timer.assert_called_once_with(self.ml, 'prefix.foo.bar.baz',
                                           42 * 1000)

    @mock.patch('ironic_lib.metrics.MetricLogger.send_counter', autospec=True)
    def test_decorator_counter(self, mock_counter):

        @self.ml.counter('foo.bar.baz')
        def func(x):
            return x * x

        func(10)

        mock_counter.assert_called_once_with(self.ml, 'prefix.foo.bar.baz', 1,
                                             sample_rate=None)

    @mock.patch('ironic_lib.metrics.MetricLogger.send_counter', autospec=True)
    def test_decorator_counter_sample_rate(self, mock_counter):

        @self.ml.counter('foo.bar.baz', sample_rate=0.5)
        def func(x):
            return x * x

        func(10)

        mock_counter.assert_called_once_with(self.ml, 'prefix.foo.bar.baz', 1,
                                             sample_rate=0.5)

    @mock.patch('ironic_lib.metrics.MetricLogger.send_gauge', autospec=True)
    def test_decorator_gauge(self, mock_gauge):
        @self.ml.gauge('foo.bar.baz')
        def func(x):
            return x

        func(10)

        mock_gauge.assert_called_once_with(self.ml, 'prefix.foo.bar.baz', 10)

    @mock.patch('ironic_lib.metrics._time', autospec=True)
    @mock.patch('ironic_lib.metrics.MetricLogger.send_timer', autospec=True)
    def test_context_mgr_timer(self, mock_timer, mock_time):
        mock_time.side_effect = [1, 43]

        with self.ml.timer('foo.bar.baz'):
            pass

        mock_timer.assert_called_once_with(self.ml, 'prefix.foo.bar.baz',
                                           42 * 1000)

    @mock.patch('ironic_lib.metrics.MetricLogger.send_counter', autospec=True)
    def test_context_mgr_counter(self, mock_counter):

        with self.ml.counter('foo.bar.baz'):
            pass

        mock_counter.assert_called_once_with(self.ml, 'prefix.foo.bar.baz', 1,
                                             sample_rate=None)

    @mock.patch('ironic_lib.metrics.MetricLogger.send_counter', autospec=True)
    def test_context_mgr_counter_sample_rate(self, mock_counter):

        with self.ml.counter('foo.bar.baz', sample_rate=0.5):
            pass

        mock_counter.assert_called_once_with(self.ml, 'prefix.foo.bar.baz', 1,
                                             sample_rate=0.5)




===========Repository Name===========
ironic-lib
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ironic-lib\ironic_lib\tests\test_metrics_statsd.py
===========File Type===========
.py
===========File Content===========
# Copyright 2016 Rackspace Hosting
# All Rights Reserved
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import socket

import mock

from ironic_lib import metrics_statsd
from ironic_lib.tests import base


def connect(family=None, type=None, proto=None):
    """Dummy function to provide signature for autospec"""
    pass


class TestStatsdMetricLogger(base.IronicLibTestCase):
    def setUp(self):
        super(TestStatsdMetricLogger, self).setUp()
        self.ml = metrics_statsd.StatsdMetricLogger('prefix', '.', 'test-host',
                                                    4321)

    def test_init(self):
        self.assertEqual(self.ml._host, 'test-host')
        self.assertEqual(self.ml._port, 4321)
        self.assertEqual(self.ml._target, ('test-host', 4321))

    @mock.patch('ironic_lib.metrics_statsd.StatsdMetricLogger._send',
                autospec=True)
    def test_gauge(self, mock_send):
        self.ml._gauge('metric', 10)
        mock_send.assert_called_once_with(self.ml, 'metric', 10, 'g')

    @mock.patch('ironic_lib.metrics_statsd.StatsdMetricLogger._send',
                autospec=True)
    def test_counter(self, mock_send):
        self.ml._counter('metric', 10)
        mock_send.assert_called_once_with(self.ml, 'metric', 10, 'c',
                                          sample_rate=None)
        mock_send.reset_mock()

        self.ml._counter('metric', 10, sample_rate=1.0)
        mock_send.assert_called_once_with(self.ml, 'metric', 10, 'c',
                                          sample_rate=1.0)

    @mock.patch('ironic_lib.metrics_statsd.StatsdMetricLogger._send',
                autospec=True)
    def test_timer(self, mock_send):
        self.ml._timer('metric', 10)
        mock_send.assert_called_once_with(self.ml, 'metric', 10, 'ms')

    @mock.patch('socket.socket', autospec=connect)
    def test_open_socket(self, mock_socket_constructor):
        self.ml._open_socket()
        mock_socket_constructor.assert_called_once_with(
            socket.AF_INET,
            socket.SOCK_DGRAM)

    @mock.patch('socket.socket', autospec=connect)
    def test_send(self, mock_socket_constructor):
        mock_socket = mock.Mock()
        mock_socket_constructor.return_value = mock_socket

        self.ml._send('part1.part2', 2, 'type')
        mock_socket.sendto.assert_called_once_with(
            'part1.part2:2|type',
            ('test-host', 4321))
        mock_socket.close.assert_called_once_with()
        mock_socket.reset_mock()

        self.ml._send('part1.part2', 3.14159, 'type')
        mock_socket.sendto.assert_called_once_with(
            'part1.part2:3.14159|type',
            ('test-host', 4321))
        mock_socket.close.assert_called_once_with()
        mock_socket.reset_mock()

        self.ml._send('part1.part2', 5, 'type')
        mock_socket.sendto.assert_called_once_with(
            'part1.part2:5|type',
            ('test-host', 4321))
        mock_socket.close.assert_called_once_with()
        mock_socket.reset_mock()

        self.ml._send('part1.part2', 5, 'type', sample_rate=0.5)
        mock_socket.sendto.assert_called_once_with(
            'part1.part2:5|type@0.5',
            ('test-host', 4321))
        mock_socket.close.assert_called_once_with()




===========Repository Name===========
ironic-lib
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ironic-lib\ironic_lib\tests\test_metrics_utils.py
===========File Type===========
.py
===========File Content===========
# Copyright 2016 Rackspace Hosting
# All Rights Reserved
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from oslo_config import cfg

from ironic_lib import exception
from ironic_lib import metrics as metricslib
from ironic_lib import metrics_statsd
from ironic_lib import metrics_utils
from ironic_lib.tests import base

CONF = cfg.CONF


class TestGetLogger(base.IronicLibTestCase):
    def setUp(self):
        super(TestGetLogger, self).setUp()

    def test_default_backend(self):
        metrics = metrics_utils.get_metrics_logger('foo')
        self.assertIsInstance(metrics, metricslib.NoopMetricLogger)

    def test_statsd_backend(self):
        CONF.set_override('backend', 'statsd', group='metrics')

        metrics = metrics_utils.get_metrics_logger('foo')
        self.assertIsInstance(metrics, metrics_statsd.StatsdMetricLogger)
        CONF.clear_override('backend', group='metrics')

    def test_nonexisting_backend(self):
        self.assertRaises(exception.InvalidMetricConfig,
                          metrics_utils.get_metrics_logger, 'foo', 'test')

    def test_numeric_prefix(self):
        self.assertRaises(exception.InvalidMetricConfig,
                          metrics_utils.get_metrics_logger, 1)

    def test_numeric_list_prefix(self):
        self.assertRaises(exception.InvalidMetricConfig,
                          metrics_utils.get_metrics_logger, (1, 2))

    def test_default_prefix(self):
        metrics = metrics_utils.get_metrics_logger()
        self.assertIsInstance(metrics, metricslib.NoopMetricLogger)
        self.assertEqual(metrics.get_metric_name("bar"), "bar")

    def test_prepend_host_backend(self):
        CONF.set_override('prepend_host', True, group='metrics')
        CONF.set_override('prepend_host_reverse', False, group='metrics')

        metrics = metrics_utils.get_metrics_logger(prefix='foo',
                                                   host="host.example.com")
        self.assertIsInstance(metrics, metricslib.NoopMetricLogger)
        self.assertEqual(metrics.get_metric_name("bar"),
                         "host.example.com.foo.bar")

        CONF.clear_override('prepend_host', group='metrics')
        CONF.clear_override('prepend_host_reverse', group='metrics')

    def test_prepend_global_prefix_host_backend(self):
        CONF.set_override('prepend_host', True, group='metrics')
        CONF.set_override('prepend_host_reverse', False, group='metrics')
        CONF.set_override('global_prefix', 'global_pre', group='metrics')

        metrics = metrics_utils.get_metrics_logger(prefix='foo',
                                                   host="host.example.com")
        self.assertIsInstance(metrics, metricslib.NoopMetricLogger)
        self.assertEqual(metrics.get_metric_name("bar"),
                         "global_pre.host.example.com.foo.bar")

        CONF.clear_override('prepend_host', group='metrics')
        CONF.clear_override('prepend_host_reverse', group='metrics')
        CONF.clear_override('global_prefix', group='metrics')

    def test_prepend_other_delim(self):
        metrics = metrics_utils.get_metrics_logger('foo', delimiter='*')
        self.assertIsInstance(metrics, metricslib.NoopMetricLogger)
        self.assertEqual(metrics.get_metric_name("bar"),
                         "foo*bar")

    def test_prepend_host_reverse_backend(self):
        CONF.set_override('prepend_host', True, group='metrics')
        CONF.set_override('prepend_host_reverse', True, group='metrics')

        metrics = metrics_utils.get_metrics_logger('foo',
                                                   host="host.example.com")
        self.assertIsInstance(metrics, metricslib.NoopMetricLogger)
        self.assertEqual(metrics.get_metric_name("bar"),
                         "com.example.host.foo.bar")

        CONF.clear_override('prepend_host', group='metrics')
        CONF.clear_override('prepend_host_reverse', group='metrics')




===========Repository Name===========
ironic-lib
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ironic-lib\ironic_lib\tests\test_utils.py
===========File Type===========
.py
===========File Content===========
# Copyright 2011 Justin Santa Barbara
# Copyright 2012 Hewlett-Packard Development Company, L.P.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import copy
import errno
import os
import os.path

import mock
from oslo_concurrency import processutils
from oslo_config import cfg

from ironic_lib import exception
from ironic_lib.tests import base
from ironic_lib import utils

CONF = cfg.CONF


class BareMetalUtilsTestCase(base.IronicLibTestCase):

    def test_unlink(self):
        with mock.patch.object(os, "unlink", autospec=True) as unlink_mock:
            unlink_mock.return_value = None
            utils.unlink_without_raise("/fake/path")
            unlink_mock.assert_called_once_with("/fake/path")

    def test_unlink_ENOENT(self):
        with mock.patch.object(os, "unlink", autospec=True) as unlink_mock:
            unlink_mock.side_effect = OSError(errno.ENOENT)
            utils.unlink_without_raise("/fake/path")
            unlink_mock.assert_called_once_with("/fake/path")


class ExecuteTestCase(base.IronicLibTestCase):
    # Allow calls to utils.execute() and related functions
    block_execute = False

    @mock.patch.object(processutils, 'execute', autospec=True)
    @mock.patch.object(os.environ, 'copy', return_value={}, autospec=True)
    def test_execute_use_standard_locale_no_env_variables(self, env_mock,
                                                          execute_mock):
        utils.execute('foo', use_standard_locale=True)
        execute_mock.assert_called_once_with('foo',
                                             env_variables={'LC_ALL': 'C'})

    @mock.patch.object(processutils, 'execute', autospec=True)
    def test_execute_use_standard_locale_with_env_variables(self,
                                                            execute_mock):
        utils.execute('foo', use_standard_locale=True,
                      env_variables={'foo': 'bar'})
        execute_mock.assert_called_once_with('foo',
                                             env_variables={'LC_ALL': 'C',
                                                            'foo': 'bar'})

    @mock.patch.object(processutils, 'execute', autospec=True)
    def test_execute_not_use_standard_locale(self, execute_mock):
        utils.execute('foo', use_standard_locale=False,
                      env_variables={'foo': 'bar'})
        execute_mock.assert_called_once_with('foo',
                                             env_variables={'foo': 'bar'})

    def test_execute_without_root_helper(self):
        CONF.set_override('root_helper', None, group='ironic_lib')
        with mock.patch.object(
                processutils, 'execute', autospec=True) as execute_mock:
            utils.execute('foo', run_as_root=False)
            execute_mock.assert_called_once_with('foo', run_as_root=False)

    def test_execute_without_root_helper_run_as_root(self):
        CONF.set_override('root_helper', None, group='ironic_lib')
        with mock.patch.object(
                processutils, 'execute', autospec=True) as execute_mock:
            utils.execute('foo', run_as_root=True)
            execute_mock.assert_called_once_with('foo', run_as_root=False)

    def test_execute_with_root_helper(self):
        with mock.patch.object(
                processutils, 'execute', autospec=True) as execute_mock:
            utils.execute('foo', run_as_root=False)
            execute_mock.assert_called_once_with('foo', run_as_root=False)

    def test_execute_with_root_helper_run_as_root(self):
        with mock.patch.object(
                processutils, 'execute', autospec=True) as execute_mock:
            utils.execute('foo', run_as_root=True)
            execute_mock.assert_called_once_with(
                'foo', run_as_root=True,
                root_helper=CONF.ironic_lib.root_helper)

    @mock.patch.object(utils, 'LOG', autospec=True)
    def _test_execute_with_log_stdout(self, log_mock, log_stdout=None):
        with mock.patch.object(
                processutils, 'execute', autospec=True) as execute_mock:
            execute_mock.return_value = ('stdout', 'stderr')
            if log_stdout is not None:
                utils.execute('foo', log_stdout=log_stdout)
            else:
                utils.execute('foo')
            execute_mock.assert_called_once_with('foo')
            name, args, kwargs = log_mock.debug.mock_calls[1]
            if log_stdout is False:
                self.assertEqual(2, log_mock.debug.call_count)
                self.assertNotIn('stdout', args[0])
            else:
                self.assertEqual(3, log_mock.debug.call_count)
                self.assertIn('stdout', args[0])

    def test_execute_with_log_stdout_default(self):
        self._test_execute_with_log_stdout()

    def test_execute_with_log_stdout_true(self):
        self._test_execute_with_log_stdout(log_stdout=True)

    def test_execute_with_log_stdout_false(self):
        self._test_execute_with_log_stdout(log_stdout=False)


class MkfsTestCase(base.IronicLibTestCase):

    @mock.patch.object(utils, 'execute', autospec=True)
    def test_mkfs(self, execute_mock):
        utils.mkfs('ext4', '/my/block/dev')
        utils.mkfs('msdos', '/my/msdos/block/dev')
        utils.mkfs('swap', '/my/swap/block/dev')

        expected = [mock.call('mkfs', '-t', 'ext4', '-F', '/my/block/dev',
                              run_as_root=True,
                              use_standard_locale=True),
                    mock.call('mkfs', '-t', 'msdos', '/my/msdos/block/dev',
                              run_as_root=True,
                              use_standard_locale=True),
                    mock.call('mkswap', '/my/swap/block/dev',
                              run_as_root=True,
                              use_standard_locale=True)]
        self.assertEqual(expected, execute_mock.call_args_list)

    @mock.patch.object(utils, 'execute', autospec=True)
    def test_mkfs_with_label(self, execute_mock):
        utils.mkfs('ext4', '/my/block/dev', 'ext4-vol')
        utils.mkfs('msdos', '/my/msdos/block/dev', 'msdos-vol')
        utils.mkfs('swap', '/my/swap/block/dev', 'swap-vol')

        expected = [mock.call('mkfs', '-t', 'ext4', '-F', '-L', 'ext4-vol',
                              '/my/block/dev', run_as_root=True,
                              use_standard_locale=True),
                    mock.call('mkfs', '-t', 'msdos', '-n', 'msdos-vol',
                              '/my/msdos/block/dev', run_as_root=True,
                              use_standard_locale=True),
                    mock.call('mkswap', '-L', 'swap-vol',
                              '/my/swap/block/dev', run_as_root=True,
                              use_standard_locale=True)]
        self.assertEqual(expected, execute_mock.call_args_list)

    @mock.patch.object(utils, 'execute', autospec=True,
                       side_effect=processutils.ProcessExecutionError(
                           stderr=os.strerror(errno.ENOENT)))
    def test_mkfs_with_unsupported_fs(self, execute_mock):
        self.assertRaises(exception.FileSystemNotSupported,
                          utils.mkfs, 'foo', '/my/block/dev')

    @mock.patch.object(utils, 'execute', autospec=True,
                       side_effect=processutils.ProcessExecutionError(
                           stderr='fake'))
    def test_mkfs_with_unexpected_error(self, execute_mock):
        self.assertRaises(processutils.ProcessExecutionError, utils.mkfs,
                          'ext4', '/my/block/dev', 'ext4-vol')


class IsHttpUrlTestCase(base.IronicLibTestCase):

    def test_is_http_url(self):
        self.assertTrue(utils.is_http_url('http://127.0.0.1'))
        self.assertTrue(utils.is_http_url('https://127.0.0.1'))
        self.assertTrue(utils.is_http_url('HTTP://127.1.2.3'))
        self.assertTrue(utils.is_http_url('HTTPS://127.3.2.1'))
        self.assertFalse(utils.is_http_url('Zm9vYmFy'))
        self.assertFalse(utils.is_http_url('11111111'))


class ParseRootDeviceTestCase(base.IronicLibTestCase):

    def test_parse_root_device_hints_without_operators(self):
        root_device = {
            'wwn': '123456', 'model': 'FOO model', 'size': 12345,
            'serial': 'foo-serial', 'vendor': 'foo VENDOR with space',
            'name': '/dev/sda', 'wwn_with_extension': '123456111',
            'wwn_vendor_extension': '111', 'rotational': True,
            'hctl': '1:0:0:0', 'by_path': '/dev/disk/by-path/1:0:0:0'}
        result = utils.parse_root_device_hints(root_device)
        expected = {
            'wwn': 's== 123456', 'model': 's== foo%20model',
            'size': '== 12345', 'serial': 's== foo-serial',
            'vendor': 's== foo%20vendor%20with%20space',
            'name': 's== /dev/sda', 'wwn_with_extension': 's== 123456111',
            'wwn_vendor_extension': 's== 111', 'rotational': True,
            'hctl': 's== 1%3A0%3A0%3A0',
            'by_path': 's== /dev/disk/by-path/1%3A0%3A0%3A0'}
        self.assertEqual(expected, result)

    def test_parse_root_device_hints_with_operators(self):
        root_device = {
            'wwn': 's== 123456', 'model': 's== foo MODEL', 'size': '>= 12345',
            'serial': 's!= foo-serial', 'vendor': 's== foo VENDOR with space',
            'name': '<or> /dev/sda <or> /dev/sdb',
            'wwn_with_extension': 's!= 123456111',
            'wwn_vendor_extension': 's== 111', 'rotational': True,
            'hctl': 's== 1:0:0:0', 'by_path': 's== /dev/disk/by-path/1:0:0:0'}

        # Validate strings being normalized
        expected = copy.deepcopy(root_device)
        expected['model'] = 's== foo%20model'
        expected['vendor'] = 's== foo%20vendor%20with%20space'
        expected['hctl'] = 's== 1%3A0%3A0%3A0'
        expected['by_path'] = 's== /dev/disk/by-path/1%3A0%3A0%3A0'

        result = utils.parse_root_device_hints(root_device)
        # The hints already contain the operators, make sure we keep it
        self.assertEqual(expected, result)

    def test_parse_root_device_hints_no_hints(self):
        result = utils.parse_root_device_hints({})
        self.assertIsNone(result)

    def test_parse_root_device_hints_convert_size(self):
        for size in (12345, '12345'):
            result = utils.parse_root_device_hints({'size': size})
            self.assertEqual({'size': '== 12345'}, result)

    def test_parse_root_device_hints_invalid_size(self):
        for value in ('not-int', -123, 0):
            self.assertRaises(ValueError, utils.parse_root_device_hints,
                              {'size': value})

    def test_parse_root_device_hints_int_or(self):
        expr = '<or> 123 <or> 456 <or> 789'
        result = utils.parse_root_device_hints({'size': expr})
        self.assertEqual({'size': expr}, result)

    def test_parse_root_device_hints_int_or_invalid(self):
        expr = '<or> 123 <or> non-int <or> 789'
        self.assertRaises(ValueError, utils.parse_root_device_hints,
                          {'size': expr})

    def test_parse_root_device_hints_string_or_space(self):
        expr = '<or> foo <or> foo bar <or> bar'
        expected = '<or> foo <or> foo%20bar <or> bar'
        result = utils.parse_root_device_hints({'model': expr})
        self.assertEqual({'model': expected}, result)

    def _parse_root_device_hints_convert_rotational(self, values,
                                                    expected_value):
        for value in values:
            result = utils.parse_root_device_hints({'rotational': value})
            self.assertEqual({'rotational': expected_value}, result)

    def test_parse_root_device_hints_convert_rotational(self):
        self._parse_root_device_hints_convert_rotational(
            (True, 'true', 'on', 'y', 'yes'), True)

        self._parse_root_device_hints_convert_rotational(
            (False, 'false', 'off', 'n', 'no'), False)

    def test_parse_root_device_hints_invalid_rotational(self):
        self.assertRaises(ValueError, utils.parse_root_device_hints,
                          {'rotational': 'not-bool'})

    def test_parse_root_device_hints_invalid_wwn(self):
        self.assertRaises(ValueError, utils.parse_root_device_hints,
                          {'wwn': 123})

    def test_parse_root_device_hints_invalid_wwn_with_extension(self):
        self.assertRaises(ValueError, utils.parse_root_device_hints,
                          {'wwn_with_extension': 123})

    def test_parse_root_device_hints_invalid_wwn_vendor_extension(self):
        self.assertRaises(ValueError, utils.parse_root_device_hints,
                          {'wwn_vendor_extension': 123})

    def test_parse_root_device_hints_invalid_model(self):
        self.assertRaises(ValueError, utils.parse_root_device_hints,
                          {'model': 123})

    def test_parse_root_device_hints_invalid_serial(self):
        self.assertRaises(ValueError, utils.parse_root_device_hints,
                          {'serial': 123})

    def test_parse_root_device_hints_invalid_vendor(self):
        self.assertRaises(ValueError, utils.parse_root_device_hints,
                          {'vendor': 123})

    def test_parse_root_device_hints_invalid_name(self):
        self.assertRaises(ValueError, utils.parse_root_device_hints,
                          {'name': 123})

    def test_parse_root_device_hints_invalid_hctl(self):
        self.assertRaises(ValueError, utils.parse_root_device_hints,
                          {'hctl': 123})

    def test_parse_root_device_hints_invalid_by_path(self):
        self.assertRaises(ValueError, utils.parse_root_device_hints,
                          {'by_path': 123})

    def test_parse_root_device_hints_non_existent_hint(self):
        self.assertRaises(ValueError, utils.parse_root_device_hints,
                          {'non-existent': 'foo'})

    def test_extract_hint_operator_and_values_single_value(self):
        expected = {'op': '>=', 'values': ['123']}
        self.assertEqual(
            expected, utils._extract_hint_operator_and_values(
                '>= 123', 'size'))

    def test_extract_hint_operator_and_values_multiple_values(self):
        expected = {'op': '<or>', 'values': ['123', '456', '789']}
        expr = '<or> 123 <or> 456 <or> 789'
        self.assertEqual(
            expected, utils._extract_hint_operator_and_values(expr, 'size'))

    def test_extract_hint_operator_and_values_multiple_values_space(self):
        expected = {'op': '<or>', 'values': ['foo', 'foo bar', 'bar']}
        expr = '<or> foo <or> foo bar <or> bar'
        self.assertEqual(
            expected, utils._extract_hint_operator_and_values(expr, 'model'))

    def test_extract_hint_operator_and_values_no_operator(self):
        expected = {'op': '', 'values': ['123']}
        self.assertEqual(
            expected, utils._extract_hint_operator_and_values('123', 'size'))

    def test_extract_hint_operator_and_values_empty_value(self):
        self.assertRaises(
            ValueError, utils._extract_hint_operator_and_values, '', 'size')

    def test_extract_hint_operator_and_values_integer(self):
        expected = {'op': '', 'values': ['123']}
        self.assertEqual(
            expected, utils._extract_hint_operator_and_values(123, 'size'))

    def test__append_operator_to_hints(self):
        root_device = {'serial': 'foo', 'size': 12345,
                       'model': 'foo model', 'rotational': True}
        expected = {'serial': 's== foo', 'size': '== 12345',
                    'model': 's== foo model', 'rotational': True}

        result = utils._append_operator_to_hints(root_device)
        self.assertEqual(expected, result)

    def test_normalize_hint_expression_or(self):
        expr = '<or> foo <or> foo bar <or> bar'
        expected = '<or> foo <or> foo%20bar <or> bar'
        result = utils._normalize_hint_expression(expr, 'model')
        self.assertEqual(expected, result)

    def test_normalize_hint_expression_in(self):
        expr = '<in> foo <in> foo bar <in> bar'
        expected = '<in> foo <in> foo%20bar <in> bar'
        result = utils._normalize_hint_expression(expr, 'model')
        self.assertEqual(expected, result)

    def test_normalize_hint_expression_op_space(self):
        expr = 's== test string with space'
        expected = 's== test%20string%20with%20space'
        result = utils._normalize_hint_expression(expr, 'model')
        self.assertEqual(expected, result)

    def test_normalize_hint_expression_op_no_space(self):
        expr = 's!= SpongeBob'
        expected = 's!= spongebob'
        result = utils._normalize_hint_expression(expr, 'model')
        self.assertEqual(expected, result)

    def test_normalize_hint_expression_no_op_space(self):
        expr = 'no operators'
        expected = 'no%20operators'
        result = utils._normalize_hint_expression(expr, 'model')
        self.assertEqual(expected, result)

    def test_normalize_hint_expression_no_op_no_space(self):
        expr = 'NoSpace'
        expected = 'nospace'
        result = utils._normalize_hint_expression(expr, 'model')
        self.assertEqual(expected, result)

    def test_normalize_hint_expression_empty_value(self):
        self.assertRaises(
            ValueError, utils._normalize_hint_expression, '', 'size')


class MatchRootDeviceTestCase(base.IronicLibTestCase):

    def setUp(self):
        super(MatchRootDeviceTestCase, self).setUp()
        self.devices = [
            {'name': '/dev/sda', 'size': 64424509440, 'model': 'ok model',
             'serial': 'fakeserial'},
            {'name': '/dev/sdb', 'size': 128849018880, 'model': 'big model',
             'serial': 'veryfakeserial', 'rotational': 'yes'},
            {'name': '/dev/sdc', 'size': 10737418240, 'model': 'small model',
             'serial': 'veryveryfakeserial', 'rotational': False},
        ]

    def test_match_root_device_hints_one_hint(self):
        root_device_hints = {'size': '>= 70'}
        dev = utils.match_root_device_hints(self.devices, root_device_hints)
        self.assertEqual('/dev/sdb', dev['name'])

    def test_match_root_device_hints_rotational(self):
        root_device_hints = {'rotational': False}
        dev = utils.match_root_device_hints(self.devices, root_device_hints)
        self.assertEqual('/dev/sdc', dev['name'])

    def test_match_root_device_hints_rotational_convert_devices_bool(self):
        root_device_hints = {'size': '>=100', 'rotational': True}
        dev = utils.match_root_device_hints(self.devices, root_device_hints)
        self.assertEqual('/dev/sdb', dev['name'])

    def test_match_root_device_hints_multiple_hints(self):
        root_device_hints = {'size': '>= 50', 'model': 's==big model',
                             'serial': 's==veryfakeserial'}
        dev = utils.match_root_device_hints(self.devices, root_device_hints)
        self.assertEqual('/dev/sdb', dev['name'])

    def test_match_root_device_hints_multiple_hints2(self):
        root_device_hints = {
            'size': '<= 20',
            'model': '<or> model 5 <or> foomodel <or> small model <or>',
            'serial': 's== veryveryfakeserial'}
        dev = utils.match_root_device_hints(self.devices, root_device_hints)
        self.assertEqual('/dev/sdc', dev['name'])

    def test_match_root_device_hints_multiple_hints3(self):
        root_device_hints = {'rotational': False, 'model': '<in> small'}
        dev = utils.match_root_device_hints(self.devices, root_device_hints)
        self.assertEqual('/dev/sdc', dev['name'])

    def test_match_root_device_hints_no_operators(self):
        root_device_hints = {'size': '120', 'model': 'big model',
                             'serial': 'veryfakeserial'}
        dev = utils.match_root_device_hints(self.devices, root_device_hints)
        self.assertEqual('/dev/sdb', dev['name'])

    def test_match_root_device_hints_no_device_found(self):
        root_device_hints = {'size': '>=50', 'model': 's==foo'}
        dev = utils.match_root_device_hints(self.devices, root_device_hints)
        self.assertIsNone(dev)

    @mock.patch.object(utils.LOG, 'warning', autospec=True)
    def test_match_root_device_hints_empty_device_attribute(self, mock_warn):
        empty_dev = [{'name': '/dev/sda', 'model': ' '}]
        dev = utils.match_root_device_hints(empty_dev, {'model': 'foo'})
        self.assertIsNone(dev)
        self.assertTrue(mock_warn.called)


class WaitForDisk(base.IronicLibTestCase):

    def setUp(self):
        super(WaitForDisk, self).setUp()
        CONF.set_override('check_device_interval', .01,
                          group='disk_partitioner')
        CONF.set_override('check_device_max_retries', 2,
                          group='disk_partitioner')

    @mock.patch.object(utils, 'execute', autospec=True)
    def test_wait_for_disk_to_become_available(self, mock_exc):
        mock_exc.return_value = ('', '')
        utils.wait_for_disk_to_become_available('fake-dev')
        fuser_cmd = ['fuser', 'fake-dev']
        fuser_call = mock.call(*fuser_cmd, run_as_root=True,
                               check_exit_code=[0, 1])
        self.assertEqual(1, mock_exc.call_count)
        mock_exc.assert_has_calls([fuser_call])

    @mock.patch.object(utils, 'execute', autospec=True,
                       side_effect=processutils.ProcessExecutionError(
                           stderr='fake'))
    def test_wait_for_disk_to_become_available_no_fuser(self, mock_exc):
        self.assertRaises(exception.IronicException,
                          utils.wait_for_disk_to_become_available,
                          'fake-dev')
        fuser_cmd = ['fuser', 'fake-dev']
        fuser_call = mock.call(*fuser_cmd, run_as_root=True,
                               check_exit_code=[0, 1])
        self.assertEqual(2, mock_exc.call_count)
        mock_exc.assert_has_calls([fuser_call, fuser_call])

    @mock.patch.object(utils, 'execute', autospec=True)
    def test_wait_for_disk_to_become_available_device_in_use_psmisc(
            self, mock_exc):
        # Test that the device is not available. This version has the 'psmisc'
        # version of 'fuser' values for stdout and stderr.
        # NOTE(TheJulia): Looks like fuser returns the actual list of pids
        # in the stdout output, where as all other text is returned in
        # stderr.
        # The 'psmisc' version has a leading space character in stdout. The
        # filename is output to stderr
        mock_exc.side_effect = [(' 1234   ', 'fake-dev: '),
                                (' 15503  3919 15510 15511', 'fake-dev:')]
        expected_error = ('Processes with the following PIDs are '
                          'holding device fake-dev: 15503, 3919, 15510, '
                          '15511. Timed out waiting for completion.')
        self.assertRaisesRegex(
            exception.IronicException,
            expected_error,
            utils.wait_for_disk_to_become_available,
            'fake-dev')
        fuser_cmd = ['fuser', 'fake-dev']
        fuser_call = mock.call(*fuser_cmd, run_as_root=True,
                               check_exit_code=[0, 1])
        self.assertEqual(2, mock_exc.call_count)
        mock_exc.assert_has_calls([fuser_call, fuser_call])

    @mock.patch.object(utils, 'execute', autospec=True)
    def test_wait_for_disk_to_become_available_device_in_use_busybox(
            self, mock_exc):
        # Test that the device is not available. This version has the 'busybox'
        # version of 'fuser' values for stdout and stderr.
        # NOTE(TheJulia): Looks like fuser returns the actual list of pids
        # in the stdout output, where as all other text is returned in
        # stderr.
        # The 'busybox' version does not have a leading space character in
        # stdout. Also nothing is output to stderr.
        mock_exc.side_effect = [('1234', ''),
                                ('15503  3919 15510 15511', '')]
        expected_error = ('Processes with the following PIDs are '
                          'holding device fake-dev: 15503, 3919, 15510, '
                          '15511. Timed out waiting for completion.')
        self.assertRaisesRegex(
            exception.IronicException,
            expected_error,
            utils.wait_for_disk_to_become_available,
            'fake-dev')
        fuser_cmd = ['fuser', 'fake-dev']
        fuser_call = mock.call(*fuser_cmd, run_as_root=True,
                               check_exit_code=[0, 1])
        self.assertEqual(2, mock_exc.call_count)
        mock_exc.assert_has_calls([fuser_call, fuser_call])

    @mock.patch.object(utils, 'execute', autospec=True)
    def test_wait_for_disk_to_become_available_no_device(self, mock_exc):
        # NOTE(TheJulia): Looks like fuser returns the actual list of pids
        # in the stdout output, where as all other text is returned in
        # stderr.

        mock_exc.return_value = ('', 'Specified filename /dev/fake '
                                     'does not exist.')
        expected_error = ('Fuser exited with "Specified filename '
                          '/dev/fake does not exist." while checking '
                          'locks for device fake-dev. Timed out waiting '
                          'for completion.')
        self.assertRaisesRegex(
            exception.IronicException,
            expected_error,
            utils.wait_for_disk_to_become_available,
            'fake-dev')
        fuser_cmd = ['fuser', 'fake-dev']
        fuser_call = mock.call(*fuser_cmd, run_as_root=True,
                               check_exit_code=[0, 1])
        self.assertEqual(2, mock_exc.call_count)
        mock_exc.assert_has_calls([fuser_call, fuser_call])

    @mock.patch.object(utils, 'execute', autospec=True)
    def test_wait_for_disk_to_become_available_dev_becomes_avail_psmisc(
            self, mock_exc):
        # Test that initially device is not available but then becomes
        # available. This version has the 'psmisc' version of 'fuser' values
        # for stdout and stderr.
        # The 'psmisc' version has a leading space character in stdout. The
        # filename is output to stderr
        mock_exc.side_effect = [(' 1234   ', 'fake-dev: '),
                                ('', '')]
        utils.wait_for_disk_to_become_available('fake-dev')
        fuser_cmd = ['fuser', 'fake-dev']
        fuser_call = mock.call(*fuser_cmd, run_as_root=True,
                               check_exit_code=[0, 1])
        self.assertEqual(2, mock_exc.call_count)
        mock_exc.assert_has_calls([fuser_call, fuser_call])

    @mock.patch.object(utils, 'execute', autospec=True)
    def test_wait_for_disk_to_become_available_dev_becomes_avail_busybox(
            self, mock_exc):
        # Test that initially device is not available but then becomes
        # available. This version has the 'busybox' version of 'fuser' values
        # for stdout and stderr.
        # The 'busybox' version does not have a leading space character in
        # stdout. Also nothing is output to stderr.
        mock_exc.side_effect = [('1234 5895', ''),
                                ('', '')]
        utils.wait_for_disk_to_become_available('fake-dev')
        fuser_cmd = ['fuser', 'fake-dev']
        fuser_call = mock.call(*fuser_cmd, run_as_root=True,
                               check_exit_code=[0, 1])
        self.assertEqual(2, mock_exc.call_count)
        mock_exc.assert_has_calls([fuser_call, fuser_call])




===========Repository Name===========
ironic-lib
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\ironic-lib\ironic_lib\tests\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\tox.ini
===========File Type===========
.ini
===========File Content===========
[tox]
minversion = 2.0
envlist = py35,py27,pep8
skipsdist = True

[testenv]
usedevelop = True
install_command = pip install {opts} {packages}
setenv =
   VIRTUAL_ENV={envdir}
   PYTHONWARNINGS=default::DeprecationWarning
deps = -c{env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt}
       -r{toxinidir}/requirements.txt
       -r{toxinidir}/test-requirements.txt
whitelist_externals = bash
                      rm
commands =
  {toxinidir}/tools/ostestr_compat_shim.sh {posargs}
passenv = http_proxy HTTP_PROXY https_proxy HTTPS_PROXY no_proxy NO_PROXY

[testenv:pep8]
basepython = python3
commands = flake8
           {toxinidir}/tools/coding-checks.sh --pylint '{posargs}'
           doc8 doc/source devstack releasenotes/source vagrant rally-jobs
           neutron-db-manage --subproject=networking-ovn check_migration

[testenv:venv]
basepython = python3
commands = {posargs}

[testenv:functional]
setenv =
  {[testenv]setenv}
  OS_TEST_PATH=./networking_ovn/tests/functional
  OS_TEST_TIMEOUT=240
deps = {[testenv]deps}
       -r{toxinidir}/networking_ovn/tests/functional/requirements.txt

[testenv:functional-py35]
basepython = python3.5
setenv =
  {[testenv]setenv}
  OS_TEST_PATH=./networking_ovn/tests/functional
  OS_TEST_TIMEOUT=240
deps = {[testenv]deps}

[testenv:dsvm]
# Fake job to define environment variables shared between dsvm jobs
setenv = OS_TEST_TIMEOUT=240
         OS_LOG_PATH={env:OS_LOG_PATH:/opt/stack/logs}
commands = false

[testenv:dsvm-functional]
setenv = {[testenv:functional]setenv}
         {[testenv:dsvm]setenv}
deps = {[testenv:functional]deps}
commands =
  {toxinidir}/tools/ostestr_compat_shim.sh {posargs}

[testenv:dsvm-functional-py35]
basepython = python3.5
setenv = {[testenv:functional]setenv}
         {[testenv:dsvm]setenv}
deps = {[testenv:functional]deps}
commands =
  {toxinidir}/tools/ostestr_compat_shim.sh {posargs}

[testenv:cover]
basepython = python3
setenv =
  {[testenv]setenv}
  PYTHON=coverage run --source networking_ovn --parallel-mode
commands =
  stestr run --no-subunit-trace {posargs}
  coverage combine
  coverage report --fail-under=70 --skip-covered
  coverage html -d cover
  coverage xml -o cover/coverage.xml

[testenv:docs]
basepython = python3
commands =
  rm -rf doc/build
  doc8 doc/source devstack releasenotes/source vagrant rally-jobs
  sphinx-build -W -b html doc/source doc/build/html

[testenv:debug]
commands = oslo_debug_helper -t networking_ovn/tests {posargs}

[testenv:genconfig]
commands =
    mkdir -p etc/neutron/plugins/ml2
    oslo-config-generator --config-file etc/oslo-config-generator/ml2_conf.ini
    oslo-config-generator --config-file etc/oslo-config-generator/networking_ovn_metadata_agent.ini
whitelist_externals = mkdir

[testenv:releasenotes]
basepython = python3
commands = sphinx-build -a -E -W -d releasenotes/build/doctrees -b html releasenotes/source releasenotes/build/html

[doc8]
# File extensions to check
extensions = .rst

[flake8]
# E123, E125 skipped as they are invalid PEP-8.
# TODO(dougwig) -- uncomment this to test for remaining linkages
# N530 direct neutron imports not allowed

show-source = True
ignore = E123,E125,N530
exclude=.venv,.git,.tox,dist,doc,*lib/python*,*egg,build,.tmp
import-order-style = pep8

[hacking]
import_exceptions = networking_ovn
local-check-factory = neutron_lib.hacking.checks.factory

[testenv:lower-constraints]
basepython = python3
deps =
  -c{toxinidir}/lower-constraints.txt
  -r{toxinidir}/test-requirements.txt
  -r{toxinidir}/requirements.txt




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\base.py
===========File Type===========
.py
===========File Content===========
# -*- coding: utf-8 -*-

# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import os

from oslo_utils import fileutils
from oslotest import base

from networking_ovn.common import config


def setup_test_logging(config_opts, log_dir, log_file_path_template):
    # Have each test log into its own log file
    config_opts.set_override('debug', True)
    fileutils.ensure_tree(log_dir, mode=0o755)
    log_file = sanitize_log_path(
        os.path.join(log_dir, log_file_path_template))
    config_opts.set_override('log_file', log_file)
    config.setup_logging()


def sanitize_log_path(path):
    # Sanitize the string so that its log path is shell friendly
    replace_map = {' ': '-', '(': '_', ')': '_'}
    for s, r in replace_map.items():
        path = path.replace(s, r)
    return path


class TestCase(base.BaseTestCase):

    """Test case base class for all unit tests."""




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\contrib\gate_hook.sh
===========File Type===========
.sh
===========File Content===========
#!/usr/bin/env bash

set -ex

VENV=${1:-"dsvm-functional"}

GATE_DEST=$BASE/new
NEUTRON_PATH=$GATE_DEST/neutron
DEVSTACK_PATH=$GATE_DEST/devstack
NETWORKING_OVN_PATH=$GATE_DEST/networking-ovn
GATE_STACK_USER=stack

case $VENV in
"dsvm-functional"|"dsvm-functional-py35")
    # The logic to set YUM or DNF as the package manager lives in stackrc,
    # let's source it so it gets applied
    source $DEVSTACK_PATH/stackrc
    source $DEVSTACK_PATH/functions
    source $NEUTRON_PATH/devstack/lib/ovs

    # NOTE(numans) Functional tests after upgrade to xenial in
    # the CI are breaking because of missing six package.
    # Installing the package for now as a workaround
    # https://bugs.launchpad.net/networking-ovn/+bug/1648670
    sudo pip install six
    # Install SSL dependencies here for now as a workaround
    # https://bugs.launchpad.net/networking-ovn/+bug/1696713
    if is_fedora ; then
        install_package openssl-devel
    elif is_ubuntu ; then
        install_package libssl-dev
    fi
    # In order to run functional tests, we want to compile OVS
    # from sources and installed. We don't need to start ovs services.
    remove_ovs_packages
    # compile_ovs expects "DEST" to be defined
    DEST=$GATE_DEST
    compile_ovs True /usr/local /var

    # Make the workspace owned by GATE_STACK_USER
    sudo chown -R $GATE_STACK_USER:$GATE_STACK_USER $BASE

    source $NETWORKING_OVN_PATH/tools/configure_for_func_testing.sh

    configure_host_for_func_testing
    ;;

*)
    echo "Unrecognized environment $VENV".
    exit 1
esac




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\contrib\post_test_hook.sh
===========File Type===========
.sh
===========File Content===========
#!/usr/bin/env bash

set -xe

NETWORKING_OVN_DIR="$BASE/new/networking-ovn"
SCRIPTS_DIR="/usr/os-testr-env/bin/"
GATE_STACK_USER=stack

venv=${1:-"dsvm-functional"}

function generate_testr_results {
    # Give job user rights to access tox logs
    sudo -H -u $owner chmod o+rw .
    sudo -H -u $owner chmod o+rw -R .stestr
    if [ -f ".stestr/0" ] ; then
        .tox/$venv/bin/subunit-1to2 < .stestr/0 > ./stestr.subunit
        $SCRIPTS_DIR/subunit2html ./stestr.subunit testr_results.html
        gzip -9 ./stestr.subunit
        gzip -9 ./testr_results.html
        sudo mv ./*.gz /opt/stack/logs/
    fi
}

function generate_log_index {
    local xtrace
    xtrace=$(set +o | grep xtrace)
    set +o xtrace

    # honor job flavors like -python35
    case $venv in
    *"dsvm-functional"*)
        venv="dsvm-functional"
        ;;
    *)
        echo "Unrecognized environment $venv".
        exit 1
    esac

    virtualenv /tmp/os-log-merger
    /tmp/os-log-merger/bin/pip install -U os-log-merger==1.1.0
    files=$(find /opt/stack/logs/$venv-logs -name '*.txt' -o -name '*.log')
    # -a3 to truncate common path prefix
    # || true to avoid the whole run failure because of os-log-merger crashes and such
    # TODO(ihrachys) remove || true when we have more trust in os-log-merger
    contents=$(/tmp/os-log-merger/bin/os-log-merger -a3 $files || true)
    echo "$contents" | sudo tee /opt/stack/logs/$venv-index.txt > /dev/null

    $xtrace
}

if [[ "$venv" == dsvm-functional* ]]
then
    owner=$GATE_STACK_USER
    sudo_env=

    # Set owner permissions according to job's requirements.
    cd $NETWORKING_OVN_DIR
    sudo chown -R $owner:$owner $NETWORKING_OVN_DIR

    # Run tests
    echo "Running networking-ovn $venv test suite"
    set +e
    sudo -H -u $owner $sudo_env tox -e $venv
    testr_exit_code=$?
    set -e

    # Collect and parse results
    generate_testr_results
    generate_log_index
    exit $testr_exit_code
fi




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\contrib\README
===========File Type===========

===========File Content===========
The files in this directory are intended for use by the
networking-ovn infra jobs that run the various functional test
suites in the gate.




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\base.py
===========File Type===========
.py
===========File Content===========
# Copyright 2016 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os
import time

import fixtures
import mock
from neutron.conf.plugins.ml2 import config
from neutron.plugins.ml2.drivers import type_geneve  # noqa
from neutron.tests.unit.plugins.ml2 import test_plugin
from neutron_lib.plugins import constants
from neutron_lib.plugins import directory
from oslo_config import cfg
from oslo_log import log
from oslo_utils import uuidutils
from ovsdbapp.backend.ovs_idl import command
from ovsdbapp.backend.ovs_idl import connection

# Load all the models to register them into SQLAlchemy metadata before using
# the SqlFixture
from networking_ovn.db import models  # noqa
from networking_ovn.ovsdb import impl_idl_ovn
from networking_ovn.ovsdb import ovsdb_monitor
from networking_ovn.tests import base
from networking_ovn.tests.functional.resources import process

LOG = log.getLogger(__name__)

# This is the directory from which infra fetches log files for functional tests
DEFAULT_LOG_DIR = os.path.join(os.environ.get('OS_LOG_PATH', '/tmp'),
                               'dsvm-functional-logs')


class AddFakeChassisCommand(command.BaseCommand):
    """Add a fake chassis in OVN SB DB for functional test."""

    def __init__(self, api, name, ip, **columns):
        super(AddFakeChassisCommand, self).__init__(api)
        self.name = name
        self.ip = ip
        self.columns = columns

    def run_idl(self, txn):
        encap_row = txn.insert(self.api._tables['Encap'])
        encap_row.type = 'geneve'
        encap_row.ip = self.ip
        self.columns.update({'encaps': [encap_row.uuid]})

        row = txn.insert(self.api._tables['Chassis'])
        row.name = self.name
        for col, val in self.columns.items():
            setattr(row, col, val)


class ConnectionFixture(fixtures.Fixture):
    def __init__(self, idl=None, constr=None, schema=None, timeout=60):
        self.idl = idl or ovsdb_monitor.BaseOvnIdl.from_server(
            constr, schema)
        self.connection = connection.Connection(
            idl=self.idl, timeout=timeout)

    def _setUp(self):
        self.addCleanup(self.stop)
        self.connection.start()

    def stop(self):
        self.connection.stop()


class TestOVNFunctionalBase(test_plugin.Ml2PluginV2TestCase):

    # Please see networking_ovn/tests/contrib/gate_hook.sh.
    # It installs openvswitch in the '/usr/local' path and the ovn-nb schema
    # file will be present in this path.
    OVS_INSTALL_SHARE_PATH = '/usr/local/share/openvswitch'
    _mechanism_drivers = ['logger', 'ovn']
    _extension_drivers = ['port_security']
    l3_plugin = 'networking_ovn.l3.l3_ovn.OVNL3RouterPlugin'

    def setUp(self, ovn_worker=False):
        config.cfg.CONF.set_override('extension_drivers',
                                     self._extension_drivers,
                                     group='ml2')
        config.cfg.CONF.set_override('tenant_network_types',
                                     ['geneve'],
                                     group='ml2')
        config.cfg.CONF.set_override('vni_ranges',
                                     ['1:65536'],
                                     group='ml2_type_geneve')
        config.cfg.CONF.set_override('dns_servers',
                                     ['10.10.10.10'],
                                     group='ovn')

        super(TestOVNFunctionalBase, self).setUp()
        base.setup_test_logging(
            cfg.CONF, DEFAULT_LOG_DIR, "%s.txt" % self.id())

        mm = directory.get_plugin().mechanism_manager
        self.mech_driver = mm.mech_drivers['ovn'].obj
        self.l3_plugin = directory.get_plugin(constants.L3)
        self.ovsdb_server_mgr = None
        self.ovn_northd_mgr = None
        self.ovn_worker = ovn_worker
        self._start_ovsdb_server_and_idls()
        self._start_ovn_northd()

    def get_additional_service_plugins(self):
        p = super(TestOVNFunctionalBase, self).get_additional_service_plugins()
        p.update({'revision_plugin_name': 'revisions'})
        return p

    @property
    def _ovsdb_protocol(self):
        return self.get_ovsdb_server_protocol()

    def get_ovsdb_server_protocol(self):
        return 'unix'

    def _start_ovn_northd(self):
        if not self.ovsdb_server_mgr:
            return
        ovn_nb_db = self.ovsdb_server_mgr.get_ovsdb_connection_path('nb')
        ovn_sb_db = self.ovsdb_server_mgr.get_ovsdb_connection_path('sb')
        self.ovn_northd_mgr = self.useFixture(
            process.OvnNorthd(self.temp_dir,
                              ovn_nb_db, ovn_sb_db,
                              protocol=self._ovsdb_protocol))

    def _start_ovsdb_server_and_idls(self):
        self.temp_dir = self.useFixture(fixtures.TempDir()).path
        # Start 2 ovsdb-servers one each for OVN NB DB and OVN SB DB
        # ovsdb-server with OVN SB DB can be used to test the chassis up/down
        # events.
        mgr = self.ovsdb_server_mgr = self.useFixture(
            process.OvsdbServer(self.temp_dir, self.OVS_INSTALL_SHARE_PATH,
                                ovn_nb_db=True, ovn_sb_db=True,
                                protocol=self._ovsdb_protocol))
        set_cfg = cfg.CONF.set_override
        set_cfg('ovn_nb_connection',
                self.ovsdb_server_mgr.get_ovsdb_connection_path(), 'ovn')
        set_cfg('ovn_sb_connection',
                self.ovsdb_server_mgr.get_ovsdb_connection_path(
                    db_type='sb'), 'ovn')
        set_cfg('ovn_nb_private_key', self.ovsdb_server_mgr.private_key, 'ovn')
        set_cfg('ovn_nb_certificate', self.ovsdb_server_mgr.certificate, 'ovn')
        set_cfg('ovn_nb_ca_cert', self.ovsdb_server_mgr.ca_cert, 'ovn')
        set_cfg('ovn_sb_private_key', self.ovsdb_server_mgr.private_key, 'ovn')
        set_cfg('ovn_sb_certificate', self.ovsdb_server_mgr.certificate, 'ovn')
        set_cfg('ovn_sb_ca_cert', self.ovsdb_server_mgr.ca_cert, 'ovn')

        num_attempts = 0
        # 5 seconds should be more than enough for the transaction to complete
        # for the test cases.
        # This also fixes the bug #1607639.
        cfg.CONF.set_override(
            'ovsdb_connection_timeout', 5,
            'ovn')

        # Created monitor IDL connection to the OVN NB DB.
        # This monitor IDL connection can be used to
        #   - Verify that the ML2 OVN driver has written to the OVN NB DB
        #     as expected.
        #   - Create and delete resources in OVN NB DB outside of the
        #     ML2 OVN driver scope to test scenarios like ovn_nb_sync.
        while num_attempts < 3:
            try:
                con = self.useFixture(ConnectionFixture(
                    constr=mgr.get_ovsdb_connection_path(),
                    schema='OVN_Northbound')).connection
                self.nb_api = impl_idl_ovn.OvsdbNbOvnIdl(con)
                break
            except Exception:
                LOG.exception("Error connecting to the OVN_Northbound DB")
                num_attempts += 1
                time.sleep(1)

        num_attempts = 0

        # Create monitor IDL connection to the OVN SB DB.
        # This monitor IDL connection can be used to
        #  - Create chassis rows
        #  - Update chassis columns etc.
        while num_attempts < 3:
            try:
                con = self.useFixture(ConnectionFixture(
                    constr=mgr.get_ovsdb_connection_path('sb'),
                    schema='OVN_Southbound')).connection
                self.sb_api = impl_idl_ovn.OvsdbSbOvnIdl(con)
                break
            except Exception:
                LOG.exception("Error connecting to the OVN_Southbound DB")
                num_attempts += 1
                time.sleep(1)

        class TriggerCls(mock.MagicMock):
            def trigger(self):
                pass

        trigger_cls = TriggerCls()
        if self.ovn_worker:
            trigger_cls.trigger.__self__.__class__ = ovsdb_monitor.OvnWorker
            cfg.CONF.set_override('neutron_sync_mode', 'off', 'ovn')

        self.addCleanup(self.stop)

        # mech_driver.post_fork_initialize creates the IDL connections
        self.mech_driver.post_fork_initialize(
            mock.ANY, mock.ANY, trigger_cls.trigger)

    def stop(self):
        if self.ovn_worker:
            self.mech_driver.nb_synchronizer.stop()
            self.mech_driver.sb_synchronizer.stop()
        self.mech_driver._nb_ovn.ovsdb_connection.stop()
        self.mech_driver._sb_ovn.ovsdb_connection.stop()

    def restart(self):
        self.stop()
        # The OVN sync test starts its own synchronizers...
        self.l3_plugin._nb_ovn_idl.ovsdb_connection.stop()
        self.l3_plugin._sb_ovn_idl.ovsdb_connection.stop()
        # Stop our monitor connections
        self.nb_api.ovsdb_connection.stop()
        self.sb_api.ovsdb_connection.stop()

        if self.ovsdb_server_mgr:
            self.ovsdb_server_mgr.stop()
        if self.ovn_northd_mgr:
            self.ovn_northd_mgr.stop()

        self.mech_driver._nb_ovn = None
        self.mech_driver._sb_ovn = None
        self.l3_plugin._nb_ovn_idl = None
        self.l3_plugin._sb_ovn_idl = None
        self.nb_api.ovsdb_connection = None
        self.sb_api.ovsdb_connection = None

        self._start_ovsdb_server_and_idls()
        self._start_ovn_northd()

    def add_fake_chassis(self, host, physical_nets=None, external_ids=None):
        physical_nets = physical_nets or []
        external_ids = external_ids or {}

        bridge_mapping = ",".join(["%s:br-provider%s" % (phys_net, i)
                                  for i, phys_net in enumerate(physical_nets)])
        name = uuidutils.generate_uuid()
        external_ids['ovn-bridge-mappings'] = bridge_mapping
        self.sb_api.chassis_add(
            name, ['geneve'], '172.24.4.10', external_ids=external_ids,
            hostname=host).execute(check_error=True)
        return name




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\requirements.txt
===========File Type===========
.txt
===========File Content===========
# Additional requirements for functional tests

# The order of packages is significant, because pip processes them in the order
# of appearance. Changing the order has an impact on the overall integration
# process, which may cause wedges in the gate later.

octavia>=3.0.0.0b3
psutil>=1.1.1,<2.0.0
psycopg2
PyMySQL>=0.6.2  # MIT License




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\test_impl_idl.py
===========File Type===========
.py
===========File Content===========
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import uuid

from ovsdbapp import event as ovsdb_event
from ovsdbapp.tests.functional import base
from ovsdbapp.tests.functional.schema.ovn_southbound import test_impl_idl as \
    test_sb
from ovsdbapp.tests import utils

from networking_ovn.ovsdb import impl_idl_ovn as impl


class WaitForPortBindingEvent(test_sb.WaitForPortBindingEvent):
    def run(self, event, row, old):
        self.row = row
        super(WaitForPortBindingEvent, self).run(event, row, old)


class TestSbApi(base.FunctionalTestCase):
    schemas = ['OVN_Southbound', 'OVN_Northbound']

    def setUp(self):
        super(TestSbApi, self).setUp()
        self.data = {
            'chassis': [
                {'external_ids': {'ovn-bridge-mappings':
                                  'public:br-ex,private:br-0'}},
                {'external_ids': {'ovn-bridge-mappings':
                                  'public:br-ex,public2:br-ex'}},
                {'external_ids': {'ovn-bridge-mappings':
                                  'public:br-ex'}},
            ]
        }
        self.api = impl.OvsdbSbOvnIdl(self.connection['OVN_Southbound'])
        self.nbapi = impl.OvsdbNbOvnIdl(self.connection['OVN_Northbound'])
        self.load_test_data()
        self.handler = ovsdb_event.RowEventHandler()
        self.api.idl.notify = self.handler.notify

    def load_test_data(self):
        with self.api.transaction(check_error=True) as txn:
            for i, chassis in enumerate(self.data['chassis']):
                chassis['name'] = utils.get_rand_device_name('chassis')
                chassis['hostname'] = '%s.localdomain.com' % chassis['name']
                txn.add(self.api.chassis_add(
                    chassis['name'], ['geneve'], '192.0.2.%d' % (i + 1,),
                    hostname=chassis['hostname'],
                    external_ids=chassis['external_ids']))

    def test_get_chassis_hostname_and_physnets(self):
        mapping = self.api.get_chassis_hostname_and_physnets()
        self.assertTrue(len(self.data['chassis']) <= len(mapping))
        self.assertTrue(set(mapping.keys()) >=
                        {c['hostname'] for c in self.data['chassis']})

    def test_get_all_chassis(self):
        chassis_list = set(self.api.get_all_chassis())
        our_chassis = {c['name'] for c in self.data['chassis']}
        self.assertTrue(our_chassis <= chassis_list)

    def test_get_chassis_data_for_ml2_bind_port(self):
        host = self.data['chassis'][0]['hostname']
        dp, iface, phys = self.api.get_chassis_data_for_ml2_bind_port(host)
        self.assertEqual(dp, '')
        self.assertEqual(iface, '')
        self.assertItemsEqual(phys, ['private', 'public'])

    def test_chassis_exists(self):
        self.assertTrue(self.api.chassis_exists(
            self.data['chassis'][0]['hostname']))
        self.assertFalse(self.api.chassis_exists("nochassishere"))

    def test_get_chassis_and_physnets(self):
        mapping = self.api.get_chassis_and_physnets()
        self.assertTrue(len(self.data['chassis']) <= len(mapping))
        self.assertTrue(set(mapping.keys()) >=
                        {c['name'] for c in self.data['chassis']})

    def _add_switch_port(self, chassis_name, type='localport'):
        sname, pname = (utils.get_rand_device_name(prefix=p)
                        for p in ('switch', 'port'))
        chassis = self.api.lookup('Chassis', chassis_name)
        row_event = WaitForPortBindingEvent(pname)
        self.handler.watch_event(row_event)
        with self.nbapi.transaction(check_error=True) as txn:
            switch = txn.add(self.nbapi.ls_add(sname))
            port = txn.add(self.nbapi.lsp_add(sname, pname, type=type))
        row_event.wait()
        return chassis, switch.result, port.result, row_event.row

    def test_get_metadata_port_network(self):
        chassis, switch, port, binding = self._add_switch_port(
            self.data['chassis'][0]['name'])
        result = self.api.get_metadata_port_network(str(binding.datapath.uuid))
        self.assertEqual(binding, result)
        self.assertEqual(binding.datapath.external_ids['logical-switch'],
                         str(switch.uuid))

    def test_get_metadata_port_network_missing(self):
        val = str(uuid.uuid4())
        self.assertIsNone(self.api.get_metadata_port_network(val))

    def test_set_get_chassis_metadata_networks(self):
        name = self.data['chassis'][0]['name']
        nets = [str(uuid.uuid4()) for _ in range(3)]
        self.api.set_chassis_metadata_networks(name, nets).execute(
            check_error=True)
        self.assertEqual(nets, self.api.get_chassis_metadata_networks(name))

    def test_get_network_port_bindings_by_ip(self):
        chassis, switch, port, binding = self._add_switch_port(
            self.data['chassis'][0]['name'])
        mac = 'de:ad:be:ef:4d:ad'
        ipaddr = '192.0.2.1'
        self.nbapi.lsp_set_addresses(
            port.name, ['%s %s' % (mac, ipaddr)]).execute(check_error=True)
        self.api.lsp_bind(port.name, chassis.name).execute(check_error=True)
        result = self.api.get_network_port_bindings_by_ip(
            str(binding.datapath.uuid), ipaddr)
        self.assertIn(binding, result)

    def test_get_ports_on_chassis(self):
        chassis, switch, port, binding = self._add_switch_port(
            self.data['chassis'][0]['name'])
        self.api.lsp_bind(port.name, chassis.name).execute(check_error=True)
        self.assertEqual([binding],
                         self.api.get_ports_on_chassis(chassis.name))

    def test_get_logical_port_chassis_and_datapath(self):
        chassis, switch, port, binding = self._add_switch_port(
            self.data['chassis'][0]['name'])
        self.api.lsp_bind(port.name, chassis.name).execute(check_error=True)
        self.assertEqual(
            (chassis.name, str(binding.datapath.uuid)),
            self.api.get_logical_port_chassis_and_datapath(port.name))




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\test_maintenance.py
===========File Type===========
.py
===========File Content===========
# Copyright 2018 Red Hat, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mock

from futurist import periodics
from neutron.tests.unit.api import test_extensions
from neutron.tests.unit.extensions import test_extraroute
from neutron.tests.unit.extensions import test_securitygroup

from networking_ovn.common import config as ovn_config
from networking_ovn.common import constants as ovn_const
from networking_ovn.common import maintenance
from networking_ovn.common import utils
from networking_ovn.db import revision as db_rev
from networking_ovn.tests.functional import base
from neutron_lib.api.definitions import external_net as extnet_apidef
from neutron_lib import constants as n_const
from neutron_lib import context as n_context


class _TestMaintenanceHelper(base.TestOVNFunctionalBase):
    """A helper class to keep the code more organized."""

    def setUp(self):
        super(_TestMaintenanceHelper, self).setUp()
        self._ovn_client = self.mech_driver._ovn_client
        self._l3_ovn_client = self.l3_plugin._ovn_client
        ext_mgr = test_extraroute.ExtraRouteTestExtensionManager()
        self.ext_api = test_extensions.setup_extensions_middleware(ext_mgr)
        sg_mgr = test_securitygroup.SecurityGroupTestExtensionManager()
        self._sg_api = test_extensions.setup_extensions_middleware(sg_mgr)
        self.maint = maintenance.DBInconsistenciesPeriodics(self._ovn_client)
        self.context = n_context.get_admin_context()

    def _api_for_resource(self, resource):
        if resource.startswith('security-group'):
            return self._sg_api
        else:
            return super(_TestMaintenanceHelper, self)._api_for_resource(
                resource)

    def _find_network_row_by_name(self, name):
        for row in self.nb_api._tables['Logical_Switch'].rows.values():
            if (row.external_ids.get(
                    ovn_const.OVN_NETWORK_NAME_EXT_ID_KEY) == name):
                return row

    def _create_network(self, name, external=False):
        data = {'network': {'name': name, 'tenant_id': self._tenant_id,
                            extnet_apidef.EXTERNAL: external}}
        req = self.new_create_request('networks', data, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['network']

    def _update_network_name(self, net_id, new_name):
        data = {'network': {'name': new_name}}
        req = self.new_update_request('networks', data, net_id, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['network']

    def _create_port(self, name, net_id, security_groups=None,
                     device_owner=None):
        data = {'port': {'name': name,
                         'tenant_id': self._tenant_id,
                         'network_id': net_id}}

        if security_groups is not None:
            data['port']['security_groups'] = security_groups

        if device_owner is not None:
            data['port']['device_owner'] = device_owner

        req = self.new_create_request('ports', data, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['port']

    def _update_port_name(self, port_id, new_name):
        data = {'port': {'name': new_name}}
        req = self.new_update_request('ports', data, port_id, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['port']

    def _find_port_row_by_name(self, name):
        for row in self.nb_api._tables['Logical_Switch_Port'].rows.values():
            if (row.external_ids.get(
                    ovn_const.OVN_PORT_NAME_EXT_ID_KEY) == name):
                return row

    def _set_global_dhcp_opts(self, ip_version, opts):
        opt_string = ','.join(['{0}:{1}'.format(key, value)
                               for key, value
                               in opts.items()])
        if ip_version == 6:
            ovn_config.cfg.CONF.set_override('ovn_dhcp6_global_options',
                                             opt_string,
                                             group='ovn')
        if ip_version == 4:
            ovn_config.cfg.CONF.set_override('ovn_dhcp4_global_options',
                                             opt_string,
                                             group='ovn')

    def _unset_global_dhcp_opts(self, ip_version):
        if ip_version == 6:
            ovn_config.cfg.CONF.clear_override('ovn_dhcp6_global_options',
                                               group='ovn')
        if ip_version == 4:
            ovn_config.cfg.CONF.clear_override('ovn_dhcp4_global_options',
                                               group='ovn')

    def _create_subnet(self, name, net_id, ip_version=4):
        data = {'subnet': {'name': name,
                           'tenant_id': self._tenant_id,
                           'network_id': net_id,
                           'ip_version': ip_version,
                           'enable_dhcp': True}}
        if ip_version == 4:
            data['subnet']['cidr'] = '10.0.0.0/24'
        else:
            data['subnet']['cidr'] = 'eef0::/64'

        req = self.new_create_request('subnets', data, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['subnet']

    def _update_subnet_enable_dhcp(self, subnet_id, value):
        data = {'subnet': {'enable_dhcp': value}}
        req = self.new_update_request('subnets', data, subnet_id, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['subnet']

    def _find_subnet_row_by_id(self, subnet_id):
        for row in self.nb_api._tables['DHCP_Options'].rows.values():
            if (row.external_ids.get('subnet_id') == subnet_id and
               not row.external_ids.get('port_id')):
                return row

    def _create_router(self, name, external_gateway_info=None):
        data = {'router': {'name': name, 'tenant_id': self._tenant_id}}
        if external_gateway_info is not None:
            data['router']['external_gateway_info'] = external_gateway_info
        req = self.new_create_request('routers', data, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['router']

    def _update_router_name(self, net_id, new_name):
        data = {'router': {'name': new_name}}
        req = self.new_update_request('routers', data, net_id, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['router']

    def _find_router_row_by_name(self, name):
        for row in self.nb_api._tables['Logical_Router'].rows.values():
            if (row.external_ids.get(
                    ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY) == name):
                return row

    def _create_security_group(self):
        data = {'security_group': {'name': 'sgtest',
                                   'tenant_id': self._tenant_id,
                                   'description': 'SpongeBob Rocks!'}}
        req = self.new_create_request('security-groups', data, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['security_group']

    def _find_security_group_row_by_id(self, sg_id):
        if self.nb_api.is_port_groups_supported():
            for row in self.nb_api._tables['Port_Group'].rows.values():
                if row.name == utils.ovn_port_group_name(sg_id):
                    return row
        else:
            for row in self.nb_api._tables['Address_Set'].rows.values():
                if (row.external_ids.get(
                        ovn_const.OVN_SG_EXT_ID_KEY) == sg_id):
                    return row

    def _create_security_group_rule(self, sg_id):
        data = {'security_group_rule': {'security_group_id': sg_id,
                                        'direction': 'ingress',
                                        'protocol': n_const.PROTO_NAME_TCP,
                                        'ethertype': n_const.IPv4,
                                        'port_range_min': 22,
                                        'port_range_max': 22,
                                        'tenant_id': self._tenant_id}}
        req = self.new_create_request('security-group-rules', data, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['security_group_rule']

    def _find_security_group_rule_row_by_id(self, sgr_id):
        for row in self.nb_api._tables['ACL'].rows.values():
            if (row.external_ids.get(
                    ovn_const.OVN_SG_RULE_EXT_ID_KEY) == sgr_id):
                return row

    def _process_router_interface(self, action, router_id, subnet_id):
        req = self.new_action_request(
            'routers', {'subnet_id': subnet_id}, router_id,
            '%s_router_interface' % action)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)

    def _add_router_interface(self, router_id, subnet_id):
        return self._process_router_interface('add', router_id, subnet_id)

    def _remove_router_interface(self, router_id, subnet_id):
        return self._process_router_interface('remove', router_id, subnet_id)

    def _find_router_port_row_by_port_id(self, port_id):
        for row in self.nb_api._tables['Logical_Router_Port'].rows.values():
            if row.name == utils.ovn_lrouter_port_name(port_id):
                return row


class TestMaintenance(_TestMaintenanceHelper):

    def test_network(self):
        net_name = 'networktest'
        with mock.patch.object(self._ovn_client, 'create_network'):
            neutron_obj = self._create_network(net_name)

        # Assert the network doesn't exist in OVN
        self.assertIsNone(self._find_network_row_by_name(net_name))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the network was now created
        ovn_obj = self._find_network_row_by_name(net_name)
        self.assertIsNotNone(ovn_obj)
        self.assertEqual(
            neutron_obj['revision_number'],
            int(ovn_obj.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]))

        # > Update

        new_obj_name = 'networktest_updated'
        with mock.patch.object(self._ovn_client, 'update_network'):
            new_neutron_obj = self._update_network_name(neutron_obj['id'],
                                                        new_obj_name)

        # Assert the revision numbers are out-of-sync
        ovn_obj = self._find_network_row_by_name(net_name)
        self.assertNotEqual(
            new_neutron_obj['revision_number'],
            int(ovn_obj.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the old name doesn't exist anymore in the OVNDB
        self.assertIsNone(self._find_network_row_by_name(net_name))

        # Assert the network is now in sync
        ovn_obj = self._find_network_row_by_name(new_obj_name)
        self.assertEqual(
            new_neutron_obj['revision_number'],
            int(ovn_obj.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]))

        # > Delete

        with mock.patch.object(self._ovn_client, 'delete_network'):
            self._delete('networks', new_neutron_obj['id'])

        # Assert the network still exists in OVNDB
        self.assertIsNotNone(self._find_network_row_by_name(new_obj_name))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the network is now deleted from OVNDB
        self.assertIsNone(self._find_network_row_by_name(new_obj_name))

        # Assert the revision number no longer exists
        self.assertIsNone(db_rev.get_revision_row(new_neutron_obj['id']))

    def test_port(self):
        obj_name = 'porttest'
        neutron_net = self._create_network('network1')

        with mock.patch.object(self._ovn_client, 'create_port'):
            neutron_obj = self._create_port(obj_name, neutron_net['id'])

        # Assert the port doesn't exist in OVN
        self.assertIsNone(self._find_port_row_by_name(obj_name))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the port was now created
        ovn_obj = self._find_port_row_by_name(obj_name)
        self.assertIsNotNone(ovn_obj)
        self.assertEqual(
            neutron_obj['revision_number'],
            int(ovn_obj.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]))

        # > Update

        new_obj_name = 'porttest_updated'
        with mock.patch.object(self._ovn_client, 'update_port'):
            new_neutron_obj = self._update_port_name(neutron_obj['id'],
                                                     new_obj_name)

        # Assert the revision numbers are out-of-sync
        ovn_obj = self._find_port_row_by_name(obj_name)
        self.assertNotEqual(
            new_neutron_obj['revision_number'],
            int(ovn_obj.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the old name doesn't exist anymore in the OVNDB
        self.assertIsNone(self._find_port_row_by_name(obj_name))

        # Assert the port is now in sync. Note that for ports we are
        # fetching it again from the Neutron database prior to comparison
        # because of the monitor code that can update the ports again upon
        # changes to it.
        ovn_obj = self._find_port_row_by_name(new_obj_name)
        new_neutron_obj = self._ovn_client._plugin.get_port(
            self.context, neutron_obj['id'])
        self.assertEqual(
            new_neutron_obj['revision_number'],
            int(ovn_obj.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]))

        # > Delete

        with mock.patch.object(self._ovn_client, 'delete_port'):
            self._delete('ports', new_neutron_obj['id'])

        # Assert the port still exists in OVNDB
        self.assertIsNotNone(self._find_port_row_by_name(new_obj_name))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the port is now deleted from OVNDB
        self.assertIsNone(self._find_port_row_by_name(new_obj_name))

        # Assert the revision number no longer exists
        self.assertIsNone(db_rev.get_revision_row(neutron_obj['id']))

    def test_subnet_global_dhcp4_opts(self):
        obj_name = 'globaltestsubnet'
        options = {'ntp_server': '1.2.3.4'}
        neutron_net = self._create_network('network1')

        # Create a subnet without global options
        neutron_sub = self._create_subnet(obj_name, neutron_net['id'])

        # Assert that the option is not set
        ovn_obj = self._find_subnet_row_by_id(neutron_sub['id'])
        self.assertIsNone(ovn_obj.options.get('ntp_server', None))

        # Set some global DHCP Options
        self._set_global_dhcp_opts(ip_version=4, opts=options)

        # Run the maintenance task to add the new options
        self.assertRaises(periodics.NeverAgain,
                          self.maint.check_global_dhcp_opts)

        # Assert that the option was added
        ovn_obj = self._find_subnet_row_by_id(neutron_sub['id'])
        self.assertEqual(
            ovn_obj.options.get('ntp_server', None),
            '1.2.3.4')

        # Change the global option
        new_options = {'ntp_server': '4.3.2.1'}
        self._set_global_dhcp_opts(ip_version=4, opts=new_options)

        # Run the maintenance task to update the options
        self.assertRaises(periodics.NeverAgain,
                          self.maint.check_global_dhcp_opts)

        # Assert that the option was changed
        ovn_obj = self._find_subnet_row_by_id(neutron_sub['id'])
        self.assertEqual(
            ovn_obj.options.get('ntp_server', None),
            '4.3.2.1')

        # Change the global option to null
        new_options = {'ntp_server': ''}
        self._set_global_dhcp_opts(ip_version=4, opts=new_options)

        # Run the maintenance task to update the options
        self.assertRaises(periodics.NeverAgain,
                          self.maint.check_global_dhcp_opts)

        # Assert that the option was removed
        ovn_obj = self._find_subnet_row_by_id(neutron_sub['id'])
        self.assertIsNone(ovn_obj.options.get('ntp_server', None))

    def test_subnet_global_dhcp6_opts(self):
        obj_name = 'globaltestsubnet'
        options = {'ntp_server': '1.2.3.4'}
        neutron_net = self._create_network('network1')

        # Create a subnet without global options
        neutron_sub = self._create_subnet(obj_name, neutron_net['id'], 6)

        # Assert that the option is not set
        ovn_obj = self._find_subnet_row_by_id(neutron_sub['id'])
        self.assertIsNone(ovn_obj.options.get('ntp_server', None))

        # Set some global DHCP Options
        self._set_global_dhcp_opts(ip_version=6, opts=options)

        # Run the maintenance task to add the new options
        self.assertRaises(periodics.NeverAgain,
                          self.maint.check_global_dhcp_opts)

        # Assert that the option was added
        ovn_obj = self._find_subnet_row_by_id(neutron_sub['id'])
        self.assertEqual(
            ovn_obj.options.get('ntp_server', None),
            '1.2.3.4')

        # Change the global option
        new_options = {'ntp_server': '4.3.2.1'}
        self._set_global_dhcp_opts(ip_version=6, opts=new_options)

        # Run the maintenance task to update the options
        self.assertRaises(periodics.NeverAgain,
                          self.maint.check_global_dhcp_opts)

        # Assert that the option was changed
        ovn_obj = self._find_subnet_row_by_id(neutron_sub['id'])
        self.assertEqual(
            ovn_obj.options.get('ntp_server', None),
            '4.3.2.1')

        # Change the global option to null
        new_options = {'ntp_server': ''}
        self._set_global_dhcp_opts(ip_version=6, opts=new_options)

        # Run the maintenance task to update the options
        self.assertRaises(periodics.NeverAgain,
                          self.maint.check_global_dhcp_opts)

        # Assert that the option was removed
        ovn_obj = self._find_subnet_row_by_id(neutron_sub['id'])
        self.assertIsNone(ovn_obj.options.get('ntp_server', None))

    def test_subnet(self):
        obj_name = 'subnettest'
        neutron_net = self._create_network('network1')

        with mock.patch.object(self._ovn_client, 'create_subnet'):
            neutron_obj = self._create_subnet(obj_name, neutron_net['id'])

        # Assert the subnet doesn't exist in OVN
        self.assertIsNone(self._find_subnet_row_by_id(neutron_obj['id']))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the subnet was now created
        ovn_obj = self._find_subnet_row_by_id(neutron_obj['id'])
        self.assertIsNotNone(ovn_obj)
        self.assertEqual(
            neutron_obj['revision_number'],
            int(ovn_obj.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]))

        # > Update

        with mock.patch.object(self._ovn_client, 'update_subnet'):
            neutron_obj = self._update_subnet_enable_dhcp(
                neutron_obj['id'], False)

        # Assert the revision numbers are out-of-sync
        ovn_obj = self._find_subnet_row_by_id(neutron_obj['id'])
        self.assertNotEqual(
            neutron_obj['revision_number'],
            int(ovn_obj.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the old name doesn't exist anymore in the OVNDB. When
        # the subnet's enable_dhcp's is set to False, OVN will remove the
        # DHCP_Options entry related to that subnet.
        self.assertIsNone(self._find_subnet_row_by_id(neutron_obj['id']))

        # Re-enable the DHCP for the subnet and check if the maintenance
        # thread will re-create it in OVN
        with mock.patch.object(self._ovn_client, 'update_subnet'):
            neutron_obj = self._update_subnet_enable_dhcp(
                neutron_obj['id'], True)

        # Assert the DHCP_Options still doesn't exist in OVNDB
        self.assertIsNone(self._find_subnet_row_by_id(neutron_obj['id']))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the subnet is now in sync
        ovn_obj = self._find_subnet_row_by_id(neutron_obj['id'])
        self.assertEqual(
            neutron_obj['revision_number'],
            int(ovn_obj.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]))

        # > Delete

        with mock.patch.object(self._ovn_client, 'delete_subnet'):
            self._delete('subnets', neutron_obj['id'])

        # Assert the subnet still exists in OVNDB
        self.assertIsNotNone(self._find_subnet_row_by_id(neutron_obj['id']))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the subnet is now deleted from OVNDB
        self.assertIsNone(self._find_subnet_row_by_id(neutron_obj['id']))

        # Assert the revision number no longer exists
        self.assertIsNone(db_rev.get_revision_row(neutron_obj['id']))

    def test_router(self):
        obj_name = 'routertest'

        with mock.patch.object(self._l3_ovn_client, 'create_router'):
            neutron_obj = self._create_router(obj_name)

        # Assert the router doesn't exist in OVN
        self.assertIsNone(self._find_router_row_by_name(obj_name))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the router was now created
        ovn_obj = self._find_router_row_by_name(obj_name)
        self.assertIsNotNone(ovn_obj)
        self.assertEqual(
            neutron_obj['revision_number'],
            int(ovn_obj.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]))

        # > Update

        new_obj_name = 'routertest_updated'
        with mock.patch.object(self._l3_ovn_client, 'update_router'):
            new_neutron_obj = self._update_router_name(neutron_obj['id'],
                                                       new_obj_name)

        # Assert the revision numbers are out-of-sync
        ovn_obj = self._find_router_row_by_name(obj_name)
        self.assertNotEqual(
            new_neutron_obj['revision_number'],
            int(ovn_obj.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the old name doesn't exist anymore in the OVNDB
        self.assertIsNone(self._find_router_row_by_name(obj_name))

        # Assert the router is now in sync
        ovn_obj = self._find_router_row_by_name(new_obj_name)
        self.assertEqual(
            new_neutron_obj['revision_number'],
            int(ovn_obj.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]))

        # > Delete

        with mock.patch.object(self._l3_ovn_client, 'delete_router'):
            self._delete('routers', new_neutron_obj['id'])

        # Assert the router still exists in OVNDB
        self.assertIsNotNone(self._find_router_row_by_name(new_obj_name))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the router is now deleted from OVNDB
        self.assertIsNone(self._find_router_row_by_name(new_obj_name))

        # Assert the revision number no longer exists
        self.assertIsNone(db_rev.get_revision_row(new_neutron_obj['id']))

    def test_security_group(self):
        with mock.patch.object(self._ovn_client, 'create_security_group'):
            neutron_obj = self._create_security_group()

        # Assert the sg doesn't exist in OVN
        self.assertIsNone(
            self._find_security_group_row_by_id(neutron_obj['id']))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the sg was now created. We don't save the revision number
        # in the Security Group because OVN doesn't support updating it,
        # all we care about is whether it exists or not.
        self.assertIsNotNone(
            self._find_security_group_row_by_id(neutron_obj['id']))

        # > Delete

        with mock.patch.object(self._ovn_client, 'delete_security_group'):
            self._delete('security-groups', neutron_obj['id'])

        # Assert the sg still exists in OVNDB
        self.assertIsNotNone(
            self._find_security_group_row_by_id(neutron_obj['id']))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the sg is now deleted from OVNDB
        self.assertIsNone(
            self._find_security_group_row_by_id(neutron_obj['id']))

        # Assert the revision number no longer exists
        self.assertIsNone(db_rev.get_revision_row(neutron_obj['id']))

    def test_security_group_rule(self):
        neutron_sg = self._create_security_group()
        neutron_net = self._create_network('network1')
        self._create_port('portsgtest', neutron_net['id'],
                          security_groups=[neutron_sg['id']])

        with mock.patch.object(self._ovn_client, 'create_security_group_rule'):
            neutron_obj = self._create_security_group_rule(neutron_sg['id'])

        # Assert the sg rule doesn't exist in OVN
        self.assertIsNone(
            self._find_security_group_rule_row_by_id(neutron_obj['id']))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the sg rule was now created. We don't save the revision number
        # in the Security Group because OVN doesn't support updating it,
        # all we care about is whether it exists or not.
        self.assertIsNotNone(
            self._find_security_group_rule_row_by_id(neutron_obj['id']))

        # > Delete

        # FIXME(lucasagomes): Maintenance thread fixing deleted
        # security group rules is currently broken due to:
        # https://bugs.launchpad.net/networking-ovn/+bug/1756123

    def test_router_port(self):
        neutron_net = self._create_network('networktest', external=True)
        neutron_subnet = self._create_subnet('subnettest', neutron_net['id'])
        neutron_router = self._create_router('routertest')

        with mock.patch.object(self._l3_ovn_client, 'create_router_port'):
            with mock.patch('networking_ovn.db.revision.bump_revision'):
                neutron_obj = self._add_router_interface(neutron_router['id'],
                                                         neutron_subnet['id'])

        # Assert the router port doesn't exist in OVN
        self.assertIsNone(
            self._find_router_port_row_by_port_id(neutron_obj['port_id']))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the router port was now created
        self.assertIsNotNone(
            self._find_router_port_row_by_port_id(neutron_obj['port_id']))

        # > Delete

        with mock.patch.object(self._l3_ovn_client, 'delete_router_port'):
            self._remove_router_interface(neutron_router['id'],
                                          neutron_subnet['id'])

        # Assert the router port still exists in OVNDB
        self.assertIsNotNone(
            self._find_router_port_row_by_port_id(neutron_obj['port_id']))

        # Call the maintenance thread to fix the problem
        self.maint.check_for_inconsistencies()

        # Assert the router port is now deleted from OVNDB
        self.assertIsNone(
            self._find_router_port_row_by_port_id(neutron_obj['port_id']))

        # Assert the revision number no longer exists
        self.assertIsNone(db_rev.get_revision_row(neutron_obj['port_id']))




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\test_mech_driver.py
===========File Type===========
.py
===========File Content===========
# Copyright 2016 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from networking_ovn.common import utils
from networking_ovn.tests.functional import base
from oslo_config import cfg
from oslo_utils import uuidutils


class TestPortBinding(base.TestOVNFunctionalBase):

    def setUp(self):
        super(TestPortBinding, self).setUp()
        self.ovs_host = 'ovs-host'
        self.dpdk_host = 'dpdk-host'
        self.invalid_dpdk_host = 'invalid-host'
        self.add_fake_chassis(self.ovs_host)
        self.add_fake_chassis(
            self.dpdk_host,
            external_ids={'datapath-type': 'netdev',
                          'iface-types': 'dummy,dummy-internal,dpdkvhostuser'})

        self.add_fake_chassis(
            self.invalid_dpdk_host,
            external_ids={'datapath-type': 'netdev',
                          'iface-types': 'dummy,dummy-internal,geneve,vxlan'})
        self.n1 = self._make_network(self.fmt, 'n1', True)
        res = self._create_subnet(self.fmt, self.n1['network']['id'],
                                  '10.0.0.0/24')
        self.deserialize(self.fmt, res)

    def _create_or_update_port(self, port_id=None, hostname=None):

        if port_id is None:
            port_data = {
                'port': {'network_id': self.n1['network']['id'],
                         'tenant_id': self._tenant_id}}

            if hostname:
                port_data['port']['device_id'] = uuidutils.generate_uuid()
                port_data['port']['device_owner'] = 'compute:None'
                port_data['port']['binding:host_id'] = hostname

            port_req = self.new_create_request('ports', port_data, self.fmt)
            port_res = port_req.get_response(self.api)
            p = self.deserialize(self.fmt, port_res)
            port_id = p['port']['id']
        else:
            port_data = {
                'port': {'device_id': uuidutils.generate_uuid(),
                         'device_owner': 'compute:None',
                         'binding:host_id': hostname}}
            port_req = self.new_update_request('ports', port_data, port_id,
                                               self.fmt)
            port_res = port_req.get_response(self.api)
            self.deserialize(self.fmt, port_res)

        return port_id

    def _verify_vif_details(self, port_id, expected_host_name,
                            expected_vif_type, expected_vif_details):
        port_req = self.new_show_request('ports', port_id)
        port_res = port_req.get_response(self.api)
        p = self.deserialize(self.fmt, port_res)
        self.assertEqual(expected_host_name, p['port']['binding:host_id'])
        self.assertEqual(expected_vif_type, p['port']['binding:vif_type'])
        self.assertEqual(expected_vif_details,
                         p['port']['binding:vif_details'])

    def test_port_binding_create_port(self):
        port_id = self._create_or_update_port(hostname=self.ovs_host)
        self._verify_vif_details(port_id, self.ovs_host, 'ovs',
                                 {'port_filter': True})

        port_id = self._create_or_update_port(hostname=self.dpdk_host)
        expected_vif_details = {'port_filter': False,
                                'vhostuser_mode': 'client',
                                'vhostuser_ovs_plug': True}
        expected_vif_details['vhostuser_socket'] = (
            utils.ovn_vhu_sockpath(cfg.CONF.ovn.vhost_sock_dir, port_id))
        self._verify_vif_details(port_id, self.dpdk_host, 'vhostuser',
                                 expected_vif_details)

        port_id = self._create_or_update_port(hostname=self.invalid_dpdk_host)
        self._verify_vif_details(port_id, self.invalid_dpdk_host, 'ovs',
                                 {'port_filter': True})

    def test_port_binding_update_port(self):
        port_id = self._create_or_update_port()
        self._verify_vif_details(port_id, '', 'unbound', {})
        port_id = self._create_or_update_port(port_id=port_id,
                                              hostname=self.ovs_host)
        self._verify_vif_details(port_id, self.ovs_host, 'ovs',
                                 {'port_filter': True})

        port_id = self._create_or_update_port(port_id=port_id,
                                              hostname=self.dpdk_host)
        expected_vif_details = {'port_filter': False,
                                'vhostuser_mode': 'client',
                                'vhostuser_ovs_plug': True}
        expected_vif_details['vhostuser_socket'] = (
            utils.ovn_vhu_sockpath(cfg.CONF.ovn.vhost_sock_dir, port_id))
        self._verify_vif_details(port_id, self.dpdk_host, 'vhostuser',
                                 expected_vif_details)

        port_id = self._create_or_update_port(port_id=port_id,
                                              hostname=self.invalid_dpdk_host)
        self._verify_vif_details(port_id, self.invalid_dpdk_host, 'ovs',
                                 {'port_filter': True})


class TestPortBindingOverTcp(TestPortBinding):
    def get_ovsdb_server_protocol(self):
        return 'tcp'


class TestPortBindingOverSsl(TestPortBinding):
    def get_ovsdb_server_protocol(self):
        return 'ssl'




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\test_metadata_agent.py
===========File Type===========
.py
===========File Content===========
# Copyright 2018 Red Hat, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import threading

import mock
from oslo_config import fixture as fixture_config
from ovsdbapp.backend.ovs_idl import event
from ovsdbapp import event as ovsdb_event

from networking_ovn.agent.metadata import agent
from networking_ovn.agent.metadata import ovsdb
from networking_ovn.agent.metadata import server as metadata_server
from networking_ovn.common import constants as ovn_const
from networking_ovn.conf.agent.metadata import config as meta
from networking_ovn.tests.functional import base


class MetadataAgentHealthEvent(event.RowEvent):
    event_name = 'MetadataAgentHealthEvent'
    ONETIME = True

    def __init__(self, chassis, sb_cfg, timeout=5):
        self.chassis = chassis
        self.sb_cfg = sb_cfg
        self.event = threading.Event()
        self.timeout = timeout
        super(MetadataAgentHealthEvent, self).__init__(
            (self.ROW_UPDATE,), 'Chassis', (('name', '=', self.chassis),))

    def matches(self, event, row, old=None):
        if not super(MetadataAgentHealthEvent, self).matches(event, row, old):
            return False
        return int(row.external_ids.get(
            ovn_const.OVN_AGENT_METADATA_SB_CFG_KEY, 0)) >= self.sb_cfg

    def run(self, event, row, old):
        self.event.set()

    def wait(self):
        return self.event.wait(self.timeout)


class TestMetadataAgent(base.TestOVNFunctionalBase):

    def setUp(self):
        super(TestMetadataAgent, self).setUp()
        self.handler = ovsdb_event.RowEventHandler()
        self.sb_api.idl.notify = self.handler.notify
        self._start_metadata_agent()

    def _start_metadata_agent(self):
        conf = self.useFixture(fixture_config.Config()).conf
        conf.register_opts(meta.SHARED_OPTS)
        conf.register_opts(meta.UNIX_DOMAIN_METADATA_PROXY_OPTS)
        conf.register_opts(meta.METADATA_PROXY_HANDLER_OPTS)
        conf.register_opts(meta.OVS_OPTS, group='ovs')
        meta.setup_privsep()

        ovn_sb_db = self.ovsdb_server_mgr.get_ovsdb_connection_path('sb')
        conf.set_override('ovn_sb_connection', ovn_sb_db, group='ovn')

        # We don't need the HA proxy server running for now
        p = mock.patch.object(metadata_server, 'UnixDomainMetadataProxy')
        p.start()
        self.addCleanup(p.stop)

        # We only have OVN NB and OVN SB running for functional tests
        p = mock.patch.object(ovsdb, 'MetadataAgentOvsIdl')
        p.start()
        self.addCleanup(p.stop)

        self.chassis_name = self.add_fake_chassis('ovs-host-fake')
        with mock.patch.object(agent.MetadataAgent,
                               '_get_own_chassis_name') as mock_get_ch_name:
            mock_get_ch_name.return_value = self.chassis_name
            agt = agent.MetadataAgent(conf)
            agt.start()
            # Metadata agent will open connections to OVS and SB databases.
            # Close connections to them when the test ends,
            self.addCleanup(agt.ovs_idl.ovsdb_connection.stop)
            self.addCleanup(agt.sb_idl.ovsdb_connection.stop)

    def test_metadata_agent_healthcheck(self):
        chassis_row = self.sb_api.db_find(
            'Chassis', ('name', '=', self.chassis_name)).execute(
            check_error=True)[0]

        # Assert that, prior to creating a resource the metadata agent
        # didn't populate the external_ids from the Chassis
        self.assertNotIn(ovn_const.OVN_AGENT_METADATA_SB_CFG_KEY,
                         chassis_row['external_ids'])

        # Let's create a network to force a transaction (actually 2: one for
        # the logical switch and another one for the metadata port) on NB db
        # which will automatically increment the nb_cfg counter on NB_Global
        # and make ovn-controller copy it over to SB_Global. Upon this event,
        # Metadata agent will update the external_ids on its Chassis row to
        # signal that it's healthy.
        row_event = MetadataAgentHealthEvent(self.chassis_name, 2)
        self.handler.watch_event(row_event)
        self._make_network(self.fmt, 'n1', True)

        # If we do not time out waiting for the event, then we are assured
        # that the metadata agent has populated the external_ids from the
        # chassis with the nb_cfg, 2 revisions, one for the network transaction
        # and another one for the port
        self.assertTrue(row_event.wait())




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\test_ovn_db_resources.py
===========File Type===========
.py
===========File Content===========
# Copyright 2016 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


import mock
import netaddr

from neutron_lib.api.definitions import dns as dns_apidef
from neutron_lib.utils import net as n_net
from oslo_config import cfg
from ovsdbapp.backend.ovs_idl import idlutils

from networking_ovn.common import config as ovn_config
from networking_ovn.common import constants as ovn_const
from networking_ovn.common import utils
from networking_ovn.tests.functional import base


class TestNBDbResources(base.TestOVNFunctionalBase):

    def setUp(self):
        super(TestNBDbResources, self).setUp()
        self.orig_get_random_mac = n_net.get_random_mac
        cfg.CONF.set_override('quota_subnet', -1, group='QUOTAS')
        ovn_config.cfg.CONF.set_override('ovn_metadata_enabled',
                                         False,
                                         group='ovn')

    def tearDown(self):
        super(TestNBDbResources, self).tearDown()

    # FIXME(lucasagomes): Map the revision numbers properly instead
    # of stripping them out. Currently, tests like test_dhcp_options()
    # are quite complex making it difficult to map the exact the revision
    # number that the DHCP Option will be at assertion time, we need to
    # refactor it a little to make it easier for mapping these updates.
    def _strip_revision_number(self, ext_ids):
        ext_ids.pop(ovn_const.OVN_REV_NUM_EXT_ID_KEY, None)
        return ext_ids

    def _verify_dhcp_option_rows(self, expected_dhcp_options_rows):
        expected_dhcp_options_rows = list(expected_dhcp_options_rows.values())
        observed_dhcp_options_rows = []
        for row in self.nb_api.tables['DHCP_Options'].rows.values():
            ext_ids = self._strip_revision_number(row.external_ids)
            observed_dhcp_options_rows.append({
                'cidr': row.cidr, 'external_ids': ext_ids,
                'options': row.options})

        self.assertItemsEqual(expected_dhcp_options_rows,
                              observed_dhcp_options_rows)

    def _verify_dhcp_option_row_for_port(self, port_id,
                                         expected_lsp_dhcpv4_options,
                                         expected_lsp_dhcpv6_options=None):
        lsp = idlutils.row_by_value(self.nb_api.idl,
                                    'Logical_Switch_Port', 'name', port_id,
                                    None)

        if lsp.dhcpv4_options:
            ext_ids = self._strip_revision_number(
                lsp.dhcpv4_options[0].external_ids)
            observed_lsp_dhcpv4_options = {
                'cidr': lsp.dhcpv4_options[0].cidr,
                'external_ids': ext_ids,
                'options': lsp.dhcpv4_options[0].options}
        else:
            observed_lsp_dhcpv4_options = {}

        if lsp.dhcpv6_options:
            ext_ids = self._strip_revision_number(
                lsp.dhcpv6_options[0].external_ids)
            observed_lsp_dhcpv6_options = {
                'cidr': lsp.dhcpv6_options[0].cidr,
                'external_ids': ext_ids,
                'options': lsp.dhcpv6_options[0].options}
        else:
            observed_lsp_dhcpv6_options = {}

        if expected_lsp_dhcpv6_options is None:
            expected_lsp_dhcpv6_options = {}

        self.assertEqual(expected_lsp_dhcpv4_options,
                         observed_lsp_dhcpv4_options)
        self.assertEqual(expected_lsp_dhcpv6_options,
                         observed_lsp_dhcpv6_options)

    def _get_subnet_dhcp_mac(self, subnet):
        mac_key = 'server_id' if subnet['ip_version'] == 6 else 'server_mac'
        dhcp_options = self.mech_driver._nb_ovn.get_subnet_dhcp_options(
            subnet['id'])['subnet']
        return dhcp_options.get('options', {}).get(
            mac_key) if dhcp_options else None

    def test_dhcp_options(self):
        """Test for DHCP_Options table rows

        When a new subnet is created, a new row has to be created in the
        DHCP_Options table for this subnet with the dhcp options stored
        in the DHCP_Options.options column.
        When ports are created for this subnet (with IPv4 address set and
        DHCP enabled in the subnet), the
        Logical_Switch_Port.dhcpv4_options column should refer to the
        appropriate row of DHCP_Options.

        In cases where a port has extra DHCPv4 options defined, a new row
        in the DHCP_Options table should be created for this port and
        Logical_Switch_Port.dhcpv4_options colimn should refer to this row.

        In order to map the DHCP_Options row to the subnet (and to a port),
        subnet_id is stored in DHCP_Options.external_ids column.
        For DHCP_Options row which belongs to a port, port_id is also stored
        in the DHCP_Options.external_ids along with the subnet_id.
        """

        n1 = self._make_network(self.fmt, 'n1', True)
        created_subnets = {}
        expected_dhcp_options_rows = {}
        dhcp_mac = {}

        for cidr in ['10.0.0.0/24', '20.0.0.0/24', '30.0.0.0/24',
                     '40.0.0.0/24', 'aef0::/64', 'bef0::/64']:
            ip_version = netaddr.IPNetwork(cidr).ip.version

            res = self._create_subnet(self.fmt, n1['network']['id'], cidr,
                                      ip_version=ip_version)
            subnet = self.deserialize(self.fmt, res)['subnet']
            created_subnets[cidr] = subnet
            dhcp_mac[subnet['id']] = self._get_subnet_dhcp_mac(subnet)

            if ip_version == 4:
                options = {'server_id': cidr.replace('0/24', '1'),
                           'server_mac': dhcp_mac[subnet['id']],
                           'lease_time': str(12 * 60 * 60),
                           'dns_server': '{10.10.10.10}',
                           'mtu': str(n1['network']['mtu']),
                           'router': subnet['gateway_ip']}
            else:
                options = {'server_id': dhcp_mac[subnet['id']]}

            expected_dhcp_options_rows[subnet['id']] = {
                'cidr': cidr,
                'external_ids': {'subnet_id': subnet['id']},
                'options': options}

        for (cidr, enable_dhcp, gateway_ip) in [
                ('50.0.0.0/24', False, '50.0.0.1'),
                ('60.0.0.0/24', True, None),
                ('cef0::/64', False, 'cef0::1'),
                ('def0::/64', True, None)]:
            ip_version = netaddr.IPNetwork(cidr).ip.version
            res = self._create_subnet(self.fmt, n1['network']['id'], cidr,
                                      ip_version=ip_version,
                                      enable_dhcp=enable_dhcp,
                                      gateway_ip=gateway_ip)
            subnet = self.deserialize(self.fmt, res)['subnet']
            created_subnets[cidr] = subnet
            dhcp_mac[subnet['id']] = self._get_subnet_dhcp_mac(subnet)
            if enable_dhcp:
                if ip_version == 4:
                    options = {}
                else:
                    options = {'server_id': dhcp_mac[subnet['id']]}
                expected_dhcp_options_rows[subnet['id']] = {
                    'cidr': cidr,
                    'external_ids': {'subnet_id': subnet['id']},
                    'options': options}

        # create a subnet with dns nameservers and host routes
        n2 = self._make_network(self.fmt, 'n2', True)
        res = self._create_subnet(
            self.fmt, n2['network']['id'], '10.0.0.0/24',
            dns_nameservers=['7.7.7.7', '8.8.8.8'],
            host_routes=[{'destination': '30.0.0.0/24',
                          'nexthop': '10.0.0.4'},
                         {'destination': '40.0.0.0/24',
                          'nexthop': '10.0.0.8'}])

        subnet = self.deserialize(self.fmt, res)['subnet']
        dhcp_mac[subnet['id']] = self._get_subnet_dhcp_mac(subnet)

        static_routes = ('{30.0.0.0/24,10.0.0.4, 40.0.0.0/24,'
                         '10.0.0.8, 0.0.0.0/0,10.0.0.1}')
        expected_dhcp_options_rows[subnet['id']] = {
            'cidr': '10.0.0.0/24',
            'external_ids': {'subnet_id': subnet['id']},
            'options': {'server_id': '10.0.0.1',
                        'server_mac': dhcp_mac[subnet['id']],
                        'lease_time': str(12 * 60 * 60),
                        'mtu': str(n2['network']['mtu']),
                        'router': subnet['gateway_ip'],
                        'dns_server': '{7.7.7.7, 8.8.8.8}',
                        'classless_static_route': static_routes}}

        # create an IPv6 subnet with dns nameservers
        res = self._create_subnet(
            self.fmt, n2['network']['id'], 'ae10::/64', ip_version=6,
            dns_nameservers=['be10::7', 'be10::8'])

        subnet = self.deserialize(self.fmt, res)['subnet']
        dhcp_mac[subnet['id']] = self._get_subnet_dhcp_mac(subnet)

        expected_dhcp_options_rows[subnet['id']] = {
            'cidr': 'ae10::/64',
            'external_ids': {'subnet_id': subnet['id']},
            'options': {'server_id': dhcp_mac[subnet['id']],
                        'dns_server': '{be10::7, be10::8}'}}

        # Verify that DHCP_Options rows are created for these subnets or not
        self._verify_dhcp_option_rows(expected_dhcp_options_rows)

        for cidr in ['20.0.0.0/24', 'aef0::/64']:
            subnet = created_subnets[cidr]
            # Disable dhcp in subnet and verify DHCP_Options
            data = {'subnet': {'enable_dhcp': False}}
            req = self.new_update_request('subnets', data, subnet['id'])
            req.get_response(self.api)
            options = expected_dhcp_options_rows.pop(subnet['id'])
            self._verify_dhcp_option_rows(expected_dhcp_options_rows)

            # Re-enable dhcp in subnet and verify DHCP_Options
            n_net.get_random_mac = mock.Mock()
            n_net.get_random_mac.return_value = dhcp_mac[subnet['id']]
            data = {'subnet': {'enable_dhcp': True}}
            req = self.new_update_request('subnets', data, subnet['id'])
            req.get_response(self.api)
            expected_dhcp_options_rows[subnet['id']] = options
            self._verify_dhcp_option_rows(expected_dhcp_options_rows)

        n_net.get_random_mac = self.orig_get_random_mac

        # Create a port and verify if Logical_Switch_Port.dhcpv4_options
        # is properly set or not
        subnet = created_subnets['40.0.0.0/24']
        subnet_v6 = created_subnets['aef0::/64']
        p = self._make_port(
            self.fmt, n1['network']['id'],
            fixed_ips=[
                {'subnet_id': subnet['id']},
                {'subnet_id': subnet_v6['id']}])

        self._verify_dhcp_option_row_for_port(
            p['port']['id'], expected_dhcp_options_rows[subnet['id']],
            expected_dhcp_options_rows[subnet_v6['id']])
        self._verify_dhcp_option_rows(expected_dhcp_options_rows)

        # create a port with dhcp disabled subnet
        subnet = created_subnets['50.0.0.0/24']

        p = self._make_port(self.fmt, n1['network']['id'],
                            fixed_ips=[{'subnet_id': subnet['id']}])

        self._verify_dhcp_option_row_for_port(p['port']['id'], {})
        self._verify_dhcp_option_rows(expected_dhcp_options_rows)

        # Delete the first subnet created
        subnet = created_subnets['10.0.0.0/24']
        req = self.new_delete_request('subnets', subnet['id'])
        req.get_response(self.api)

        # Verify that DHCP_Options rows are deleted or not
        del expected_dhcp_options_rows[subnet['id']]
        self._verify_dhcp_option_rows(expected_dhcp_options_rows)

    def test_port_dhcp_options(self):
        dhcp_mac = {}
        n1 = self._make_network(self.fmt, 'n1', True)
        res = self._create_subnet(self.fmt, n1['network']['id'], '10.0.0.0/24')
        subnet = self.deserialize(self.fmt, res)['subnet']
        dhcp_mac[subnet['id']] = self._get_subnet_dhcp_mac(subnet)
        res = self._create_subnet(self.fmt, n1['network']['id'], 'aef0::/64',
                                  ip_version=6)
        subnet_v6 = self.deserialize(self.fmt, res)['subnet']
        dhcp_mac[subnet_v6['id']] = self._get_subnet_dhcp_mac(subnet_v6)

        expected_dhcp_options_rows = {
            subnet['id']: {
                'cidr': '10.0.0.0/24',
                'external_ids': {'subnet_id': subnet['id']},
                'options': {'server_id': '10.0.0.1',
                            'server_mac': dhcp_mac[subnet['id']],
                            'lease_time': str(12 * 60 * 60),
                            'dns_server': '{10.10.10.10}',
                            'mtu': str(n1['network']['mtu']),
                            'router': subnet['gateway_ip']}},
            subnet_v6['id']: {
                'cidr': 'aef0::/64',
                'external_ids': {'subnet_id': subnet_v6['id']},
                'options': {'server_id': dhcp_mac[subnet_v6['id']]}}}
        expected_dhcp_v4_options_rows = {
            subnet['id']: expected_dhcp_options_rows[subnet['id']]}
        expected_dhcp_v6_options_rows = {
            subnet_v6['id']: expected_dhcp_options_rows[subnet_v6['id']]}
        data = {
            'port': {'network_id': n1['network']['id'],
                     'tenant_id': self._tenant_id,
                     'device_owner': 'compute:None',
                     'fixed_ips': [{'subnet_id': subnet['id']}],
                     'extra_dhcp_opts': [{'ip_version': 4, 'opt_name': 'mtu',
                                          'opt_value': '1100'},
                                         {'ip_version': 4,
                                          'opt_name': 'ntp-server',
                                          'opt_value': '8.8.8.8'}]}}
        port_req = self.new_create_request('ports', data, self.fmt)
        port_res = port_req.get_response(self.api)
        p1 = self.deserialize(self.fmt, port_res)

        expected_dhcp_options_rows['v4-' + p1['port']['id']] = {
            'cidr': '10.0.0.0/24',
            'external_ids': {'subnet_id': subnet['id'],
                             'port_id': p1['port']['id']},
            'options': {'server_id': '10.0.0.1',
                        'server_mac': dhcp_mac[subnet['id']],
                        'lease_time': str(12 * 60 * 60),
                        'dns_server': '{10.10.10.10}',
                        'mtu': '1100',
                        'router': subnet['gateway_ip'],
                        'ntp_server': '8.8.8.8'}}
        expected_dhcp_v4_options_rows['v4-' + p1['port']['id']] = \
            expected_dhcp_options_rows['v4-' + p1['port']['id']]
        data = {
            'port': {'network_id': n1['network']['id'],
                     'tenant_id': self._tenant_id,
                     'device_owner': 'compute:None',
                     'fixed_ips': [{'subnet_id': subnet['id']}],
                     'extra_dhcp_opts': [{'ip_version': 4,
                                          'opt_name': 'ip-forward-enable',
                                          'opt_value': '1'},
                                         {'ip_version': 4,
                                          'opt_name': 'tftp-server',
                                          'opt_value': '10.0.0.100'},
                                         {'ip_version': 4,
                                          'opt_name': 'dns-server',
                                          'opt_value': '20.20.20.20'}]}}

        port_req = self.new_create_request('ports', data, self.fmt)
        port_res = port_req.get_response(self.api)
        p2 = self.deserialize(self.fmt, port_res)

        expected_dhcp_options_rows['v4-' + p2['port']['id']] = {
            'cidr': '10.0.0.0/24',
            'external_ids': {'subnet_id': subnet['id'],
                             'port_id': p2['port']['id']},
            'options': {'server_id': '10.0.0.1',
                        'server_mac': dhcp_mac[subnet['id']],
                        'lease_time': str(12 * 60 * 60),
                        'mtu': str(n1['network']['mtu']),
                        'router': subnet['gateway_ip'],
                        'ip_forward_enable': '1',
                        'tftp_server': '10.0.0.100',
                        'dns_server': '20.20.20.20'}}
        expected_dhcp_v4_options_rows['v4-' + p2['port']['id']] = \
            expected_dhcp_options_rows['v4-' + p2['port']['id']]
        data = {
            'port': {'network_id': n1['network']['id'],
                     'tenant_id': self._tenant_id,
                     'device_owner': 'compute:None',
                     'fixed_ips': [{'subnet_id': subnet_v6['id']}],
                     'extra_dhcp_opts': [{'ip_version': 6,
                                          'opt_name': 'dns-server',
                                          'opt_value': 'aef0::1'},
                                         {'ip_version': 6,
                                          'opt_name': 'domain-search',
                                          'opt_value': 'foo-domain'}]}}
        port_req = self.new_create_request('ports', data, self.fmt)
        port_res = port_req.get_response(self.api)
        p3 = self.deserialize(self.fmt, port_res)
        expected_dhcp_options_rows['v6-' + p3['port']['id']] = {
            'cidr': 'aef0::/64',
            'external_ids': {'subnet_id': subnet_v6['id'],
                             'port_id': p3['port']['id']},
            'options': {'server_id': dhcp_mac[subnet_v6['id']],
                        'dns_server': 'aef0::1',
                        'domain_search': 'foo-domain'}}
        expected_dhcp_v6_options_rows['v6-' + p3['port']['id']] = \
            expected_dhcp_options_rows['v6-' + p3['port']['id']]
        data = {
            'port': {'network_id': n1['network']['id'],
                     'tenant_id': self._tenant_id,
                     'device_owner': 'compute:None',
                     'fixed_ips': [{'subnet_id': subnet['id']},
                                   {'subnet_id': subnet_v6['id']}],
                     'extra_dhcp_opts': [{'ip_version': 4,
                                          'opt_name': 'tftp-server',
                                          'opt_value': '100.0.0.100'},
                                         {'ip_version': 6,
                                          'opt_name': 'dns-server',
                                          'opt_value': 'aef0::100'},
                                         {'ip_version': 6,
                                          'opt_name': 'domain-search',
                                          'opt_value': 'bar-domain'}]}}
        port_req = self.new_create_request('ports', data, self.fmt)
        port_res = port_req.get_response(self.api)
        p4 = self.deserialize(self.fmt, port_res)
        expected_dhcp_options_rows['v6-' + p4['port']['id']] = {
            'cidr': 'aef0::/64',
            'external_ids': {'subnet_id': subnet_v6['id'],
                             'port_id': p4['port']['id']},
            'options': {'server_id': dhcp_mac[subnet_v6['id']],
                        'dns_server': 'aef0::100',
                        'domain_search': 'bar-domain'}}
        expected_dhcp_options_rows['v4-' + p4['port']['id']] = {
            'cidr': '10.0.0.0/24',
            'external_ids': {'subnet_id': subnet['id'],
                             'port_id': p4['port']['id']},
            'options': {'server_id': '10.0.0.1',
                        'server_mac': dhcp_mac[subnet['id']],
                        'lease_time': str(12 * 60 * 60),
                        'dns_server': '{10.10.10.10}',
                        'mtu': str(n1['network']['mtu']),
                        'router': subnet['gateway_ip'],
                        'tftp_server': '100.0.0.100'}}
        expected_dhcp_v4_options_rows['v4-' + p4['port']['id']] = \
            expected_dhcp_options_rows['v4-' + p4['port']['id']]
        expected_dhcp_v6_options_rows['v6-' + p4['port']['id']] = \
            expected_dhcp_options_rows['v6-' + p4['port']['id']]

        # test port without extra_dhcp_opts but using subnet DHCP options
        data = {
            'port': {'network_id': n1['network']['id'],
                     'tenant_id': self._tenant_id,
                     'device_owner': 'compute:None',
                     'fixed_ips': [{'subnet_id': subnet['id']},
                                   {'subnet_id': subnet_v6['id']}]}}
        port_req = self.new_create_request('ports', data, self.fmt)
        port_res = port_req.get_response(self.api)
        p5 = self.deserialize(self.fmt, port_res)

        self._verify_dhcp_option_rows(expected_dhcp_options_rows)

        self._verify_dhcp_option_row_for_port(
            p1['port']['id'],
            expected_dhcp_options_rows['v4-' + p1['port']['id']])
        self._verify_dhcp_option_row_for_port(
            p2['port']['id'],
            expected_dhcp_options_rows['v4-' + p2['port']['id']])
        self._verify_dhcp_option_row_for_port(
            p3['port']['id'], {},
            expected_lsp_dhcpv6_options=expected_dhcp_options_rows[
                'v6-' + p3['port']['id']])
        self._verify_dhcp_option_row_for_port(
            p4['port']['id'],
            expected_dhcp_options_rows['v4-' + p4['port']['id']],
            expected_lsp_dhcpv6_options=expected_dhcp_options_rows[
                'v6-' + p4['port']['id']])
        self._verify_dhcp_option_row_for_port(
            p5['port']['id'],
            expected_dhcp_options_rows[subnet['id']],
            expected_lsp_dhcpv6_options=expected_dhcp_options_rows[
                subnet_v6['id']])

        # Update the subnet with dns_server. It should get propagated
        # to the DHCP options of the p1. Note that it should not get
        # propagate to DHCP options of port p2 because, it has overridden
        # dns-server in the Extra DHCP options.
        data = {'subnet': {'dns_nameservers': ['7.7.7.7', '8.8.8.8']}}
        req = self.new_update_request('subnets', data, subnet['id'])
        req.get_response(self.api)

        for i in [subnet['id'], 'v4-' + p1['port']['id'],
                  'v4-' + p4['port']['id']]:
            expected_dhcp_options_rows[i]['options']['dns_server'] = (
                '{7.7.7.7, 8.8.8.8}')

        self._verify_dhcp_option_rows(expected_dhcp_options_rows)

        # Update the port p2 by removing dns-server and tfp-server in the
        # extra DHCP options. dns-server option from the subnet DHCP options
        # should be updated in the p2 DHCP options
        data = {'port': {'extra_dhcp_opts': [{'ip_version': 4,
                                              'opt_name': 'ip-forward-enable',
                                              'opt_value': '0'},
                                             {'ip_version': 4,
                                              'opt_name': 'tftp-server',
                                              'opt_value': None},
                                             {'ip_version': 4,
                                              'opt_name': 'dns-server',
                                              'opt_value': None}]}}
        port_req = self.new_update_request('ports', data, p2['port']['id'])
        port_req.get_response(self.api)
        p2_expected = expected_dhcp_options_rows['v4-' + p2['port']['id']]
        p2_expected['options']['dns_server'] = '{7.7.7.7, 8.8.8.8}'

        p2_expected['options']['ip_forward_enable'] = '0'

        del p2_expected['options']['tftp_server']
        self._verify_dhcp_option_rows(expected_dhcp_options_rows)

        # Test subnet DHCP disabling and enabling
        for (subnet_id, expect_subnet_rows_disabled, expect_port_row_disabled
             ) in [
            (subnet['id'], expected_dhcp_v6_options_rows,
             [(p4, {}, expected_dhcp_options_rows['v6-' + p4['port']['id']]),
              (p5, {}, expected_dhcp_options_rows[subnet_v6['id']])]),
            (subnet_v6['id'], expected_dhcp_v4_options_rows,
             [(p4, expected_dhcp_options_rows['v4-' + p4['port']['id']], {}),
              (p5, expected_dhcp_options_rows[subnet['id']], {})])]:
            # Disable subnet's DHCP and verify DHCP_Options,
            data = {'subnet': {'enable_dhcp': False}}
            req = self.new_update_request('subnets', data, subnet_id)
            req.get_response(self.api)
            # DHCP_Options belonging to the subnet or it's ports should be all
            # removed, current DHCP_Options should be equal to
            # expect_subnet_rows_disabled
            self._verify_dhcp_option_rows(expect_subnet_rows_disabled)
            # Verify that the corresponding port DHCP options were cleared
            # and the others were not affected.
            for p in expect_port_row_disabled:
                self._verify_dhcp_option_row_for_port(
                    p[0]['port']['id'], p[1], p[2])
            # Re-enable dhcpv4 in subnet and verify DHCP_Options
            n_net.get_random_mac = mock.Mock()
            n_net.get_random_mac.return_value = dhcp_mac[subnet_id]
            data = {'subnet': {'enable_dhcp': True}}
            req = self.new_update_request('subnets', data, subnet_id)
            req.get_response(self.api)
            self._verify_dhcp_option_rows(expected_dhcp_options_rows)
            self._verify_dhcp_option_row_for_port(
                p4['port']['id'],
                expected_dhcp_options_rows['v4-' + p4['port']['id']],
                expected_dhcp_options_rows['v6-' + p4['port']['id']])
            self._verify_dhcp_option_row_for_port(
                p5['port']['id'],
                expected_dhcp_options_rows[subnet['id']],
                expected_lsp_dhcpv6_options=expected_dhcp_options_rows[
                    subnet_v6['id']])
        n_net.get_random_mac = self.orig_get_random_mac

        # Disable dhcp in p2
        data = {'port': {'extra_dhcp_opts': [{'ip_version': 4,
                                              'opt_name': 'dhcp_disabled',
                                              'opt_value': 'true'}]}}
        port_req = self.new_update_request('ports', data, p2['port']['id'])
        port_req.get_response(self.api)

        del expected_dhcp_options_rows['v4-' + p2['port']['id']]
        self._verify_dhcp_option_rows(expected_dhcp_options_rows)

        # delete port p1.
        port_req = self.new_delete_request('ports', p1['port']['id'])
        port_req.get_response(self.api)

        del expected_dhcp_options_rows['v4-' + p1['port']['id']]
        self._verify_dhcp_option_rows(expected_dhcp_options_rows)

        # delete the IPv6 extra DHCP options for p4
        data = {'port': {'extra_dhcp_opts': [{'ip_version': 6,
                                              'opt_name': 'dns-server',
                                              'opt_value': None},
                                             {'ip_version': 6,
                                              'opt_name': 'domain-search',
                                              'opt_value': None}]}}
        port_req = self.new_update_request('ports', data, p4['port']['id'])
        port_req.get_response(self.api)
        del expected_dhcp_options_rows['v6-' + p4['port']['id']]

        self._verify_dhcp_option_rows(expected_dhcp_options_rows)

    def test_port_dhcp_opts_add_and_remove_extra_dhcp_opts(self):
        """Orphaned DHCP_Options row.

        In this test case a port is created with extra DHCP options.
        Since it has extra DHCP options a new row in the DHCP_Options is
        created for this port.
        Next the port is updated to delete the extra DHCP options.
        After the update, the Logical_Switch_Port.dhcpv4_options for this port
        should refer to the subnet DHCP_Options and the DHCP_Options row
        created for this port earlier should be deleted.
        """
        dhcp_mac = {}
        n1 = self._make_network(self.fmt, 'n1', True)
        res = self._create_subnet(self.fmt, n1['network']['id'], '10.0.0.0/24')
        subnet = self.deserialize(self.fmt, res)['subnet']
        dhcp_mac[subnet['id']] = self._get_subnet_dhcp_mac(subnet)
        res = self._create_subnet(self.fmt, n1['network']['id'], 'aef0::/64',
                                  ip_version=6)
        subnet_v6 = self.deserialize(self.fmt, res)['subnet']
        dhcp_mac[subnet_v6['id']] = self._get_subnet_dhcp_mac(subnet_v6)
        expected_dhcp_options_rows = {
            subnet['id']: {
                'cidr': '10.0.0.0/24',
                'external_ids': {'subnet_id': subnet['id']},
                'options': {'server_id': '10.0.0.1',
                            'server_mac': dhcp_mac[subnet['id']],
                            'lease_time': str(12 * 60 * 60),
                            'dns_server': '{10.10.10.10}',
                            'mtu': str(n1['network']['mtu']),
                            'router': subnet['gateway_ip']}},
            subnet_v6['id']: {
                'cidr': 'aef0::/64',
                'external_ids': {'subnet_id': subnet_v6['id']},
                'options': {'server_id': dhcp_mac[subnet_v6['id']]}}}

        data = {
            'port': {'network_id': n1['network']['id'],
                     'tenant_id': self._tenant_id,
                     'device_owner': 'compute:None',
                     'extra_dhcp_opts': [{'ip_version': 4, 'opt_name': 'mtu',
                                          'opt_value': '1100'},
                                         {'ip_version': 4,
                                          'opt_name': 'ntp-server',
                                          'opt_value': '8.8.8.8'},
                                         {'ip_version': 6,
                                          'opt_name': 'dns-server',
                                          'opt_value': 'aef0::100'}]}}
        port_req = self.new_create_request('ports', data, self.fmt)
        port_res = port_req.get_response(self.api)
        p1 = self.deserialize(self.fmt, port_res)['port']

        expected_dhcp_options_rows['v4-' + p1['id']] = {
            'cidr': '10.0.0.0/24',
            'external_ids': {'subnet_id': subnet['id'],
                             'port_id': p1['id']},
            'options': {'server_id': '10.0.0.1',
                        'server_mac': dhcp_mac[subnet['id']],
                        'lease_time': str(12 * 60 * 60),
                        'dns_server': '{10.10.10.10}',
                        'mtu': '1100',
                        'router': subnet['gateway_ip'],
                        'ntp_server': '8.8.8.8'}}

        expected_dhcp_options_rows['v6-' + p1['id']] = {
            'cidr': 'aef0::/64',
            'external_ids': {'subnet_id': subnet_v6['id'],
                             'port_id': p1['id']},
            'options': {'server_id': dhcp_mac[subnet_v6['id']],
                        'dns_server': 'aef0::100'}}

        self._verify_dhcp_option_rows(expected_dhcp_options_rows)
        # The Logical_Switch_Port.dhcp(v4/v6)_options should refer to the
        # port DHCP options.
        self._verify_dhcp_option_row_for_port(
            p1['id'], expected_dhcp_options_rows['v4-' + p1['id']],
            expected_dhcp_options_rows['v6-' + p1['id']])

        # Now update the port to delete the extra DHCP options
        data = {'port': {'extra_dhcp_opts': [{'ip_version': 4,
                                              'opt_name': 'mtu',
                                              'opt_value': None},
                                             {'ip_version': 4,
                                              'opt_name': 'ntp-server',
                                              'opt_value': None}]}}
        port_req = self.new_update_request('ports', data, p1['id'])
        port_req.get_response(self.api)

        # DHCP_Options row created for the port earlier should have been
        # deleted.
        del expected_dhcp_options_rows['v4-' + p1['id']]
        self._verify_dhcp_option_rows(expected_dhcp_options_rows)
        # The Logical_Switch_Port.dhcpv4_options for this port should refer to
        # the subnet DHCP options.
        self._verify_dhcp_option_row_for_port(
            p1['id'], expected_dhcp_options_rows[subnet['id']],
            expected_dhcp_options_rows['v6-' + p1['id']])

        # update the port again with extra DHCP options.
        data = {'port': {'extra_dhcp_opts': [{'ip_version': 4,
                                              'opt_name': 'mtu',
                                              'opt_value': '1200'},
                                             {'ip_version': 4,
                                              'opt_name': 'tftp-server',
                                              'opt_value': '8.8.8.8'}]}}

        port_req = self.new_update_request('ports', data, p1['id'])
        port_req.get_response(self.api)

        expected_dhcp_options_rows['v4-' + p1['id']] = {
            'cidr': '10.0.0.0/24',
            'external_ids': {'subnet_id': subnet['id'],
                             'port_id': p1['id']},
            'options': {'server_id': '10.0.0.1',
                        'server_mac': dhcp_mac[subnet['id']],
                        'lease_time': str(12 * 60 * 60),
                        'dns_server': '{10.10.10.10}',
                        'mtu': '1200',
                        'router': subnet['gateway_ip'],
                        'tftp_server': '8.8.8.8'}}
        self._verify_dhcp_option_rows(expected_dhcp_options_rows)
        self._verify_dhcp_option_row_for_port(
            p1['id'], expected_dhcp_options_rows['v4-' + p1['id']],
            expected_dhcp_options_rows['v6-' + p1['id']])

        # Disable DHCPv4 for this port. The DHCP_Options row created for this
        # port should be get deleted.
        data = {'port': {'extra_dhcp_opts': [{'ip_version': 4,
                                              'opt_name': 'dhcp_disabled',
                                              'opt_value': 'true'}]}}
        port_req = self.new_update_request('ports', data, p1['id'])
        port_req.get_response(self.api)

        del expected_dhcp_options_rows['v4-' + p1['id']]
        self._verify_dhcp_option_rows(expected_dhcp_options_rows)
        # The Logical_Switch_Port.dhcpv4_options for this port should be
        # empty.
        self._verify_dhcp_option_row_for_port(
            p1['id'], {}, expected_dhcp_options_rows['v6-' + p1['id']])

        # Disable DHCPv6 for this port. The DHCP_Options row created for this
        # port should be get deleted.
        data = {'port': {'extra_dhcp_opts': [{'ip_version': 6,
                                              'opt_name': 'dhcp_disabled',
                                              'opt_value': 'true'}]}}
        port_req = self.new_update_request('ports', data, p1['id'])
        port_req.get_response(self.api)

        del expected_dhcp_options_rows['v6-' + p1['id']]
        self._verify_dhcp_option_rows(expected_dhcp_options_rows)
        # The Logical_Switch_Port.dhcpv4_options for this port should be
        # empty.
        self._verify_dhcp_option_row_for_port(p1['id'], {})


class TestPortSecurity(base.TestOVNFunctionalBase):

    def _get_port_related_acls(self, port_id):
        ovn_port = self.nb_api.lookup('Logical_Switch_Port', port_id)
        port_acls = []
        for pg in self.nb_api.tables['Port_Group'].rows.values():
            for p in pg.ports:
                if ovn_port.uuid != p.uuid:
                    continue
                for a in pg.acls:
                    port_acls.append({'match': a.match,
                                      'action': a.action,
                                      'priority': a.priority,
                                      'direction': a.direction})
        return port_acls

    def _get_port_related_acls_port_group_not_supported(self, port_id):
        port_acls = []
        for acl in self.nb_api.tables['ACL'].rows.values():
            ext_ids = getattr(acl, 'external_ids', {})
            if ext_ids.get('neutron:lport') == port_id:
                port_acls.append({'match': acl.match,
                                  'action': acl.action,
                                  'priority': acl.priority,
                                  'direction': acl.direction})
        return port_acls

    def _verify_port_acls(self, port_id, expected_acls):
        if self.nb_api.is_port_groups_supported():
            port_acls = self._get_port_related_acls(port_id)
        else:
            port_acls = self._get_port_related_acls_port_group_not_supported(
                port_id)
        self.assertItemsEqual(expected_acls, port_acls)

    @mock.patch('networking_ovn.ovsdb.impl_idl_ovn.OvsdbNbOvnIdl.'
                'is_port_groups_supported', lambda *args: False)
    def test_port_security_port_group_not_supported(self):
        n1 = self._make_network(self.fmt, 'n1', True)
        res = self._create_subnet(self.fmt, n1['network']['id'], '10.0.0.0/24')
        subnet = self.deserialize(self.fmt, res)['subnet']
        p = self._make_port(self.fmt, n1['network']['id'],
                            fixed_ips=[{'subnet_id': subnet['id']}])
        port_id = p['port']['id']
        sg_id = p['port']['security_groups'][0].replace('-', '_')
        expected_acls_with_sg_ps_enabled = [
            {'match': 'inport == "' + str(port_id) + '" && ip',
             'action': 'drop',
             'priority': 1001,
             'direction': 'from-lport'},
            {'match': 'outport == "' + str(port_id) + '" && ip',
             'action': 'drop',
             'priority': 1001,
             'direction': 'to-lport'},
            {'match': 'inport == "' + str(port_id) + '" && ip4 && ip4.dst == '
                      '{255.255.255.255, 10.0.0.0/24} && udp && udp.src == 68 '
                      '&& udp.dst == 67',
             'action': 'allow',
             'priority': 1002,
             'direction': 'from-lport'},
            {'match': 'inport == "' + str(port_id) + '" && ip6',
             'action': 'allow-related',
             'priority': 1002,
             'direction': 'from-lport'},
            {'match': 'inport == "' + str(port_id) + '" && ip4',
             'action': 'allow-related',
             'priority': 1002,
             'direction': 'from-lport'},
            {'match': 'outport == "' + str(port_id) + '" && ip4 && '
                      'ip4.src == $as_ip4_' + str(sg_id),
             'action': 'allow-related',
             'priority': 1002,
             'direction': 'to-lport'},
            {'match': 'outport == "' + str(port_id) + '" && ip6 && '
                      'ip6.src == $as_ip6_' + str(sg_id),
             'action': 'allow-related',
             'priority': 1002,
             'direction': 'to-lport'},
        ]
        self._verify_port_acls(port_id, expected_acls_with_sg_ps_enabled)

        # clear the security groups.
        data = {'port': {'security_groups': []}}
        port_req = self.new_update_request('ports', data, p['port']['id'])
        port_req.get_response(self.api)

        # No security groups and port security enabled - > ACLs should be
        # added to drop the packets.
        expected_acls_with_no_sg_ps_enabled = [
            {'match': 'inport == "' + str(port_id) + '" && ip',
             'action': 'drop',
             'priority': 1001,
             'direction': 'from-lport'},
            {'match': 'outport == "' + str(port_id) + '" && ip',
             'action': 'drop',
             'priority': 1001,
             'direction': 'to-lport'},
        ]
        self._verify_port_acls(port_id, expected_acls_with_no_sg_ps_enabled)

        # Disable port security
        data = {'port': {'port_security_enabled': False}}
        port_req = self.new_update_request('ports', data, p['port']['id'])
        port_req.get_response(self.api)
        # No security groups and port security disabled - > No ACLs should be
        # added (allowing all the traffic).
        self._verify_port_acls(port_id, [])

        # Enable port security again with no security groups - > ACLs should
        # be added back to drop the packets.
        data = {'port': {'port_security_enabled': True}}
        port_req = self.new_update_request('ports', data, p['port']['id'])
        port_req.get_response(self.api)
        self._verify_port_acls(port_id, expected_acls_with_no_sg_ps_enabled)

        # Set security groups back
        data = {'port': {'security_groups': p['port']['security_groups']}}
        port_req = self.new_update_request('ports', data, p['port']['id'])
        port_req.get_response(self.api)
        self._verify_port_acls(port_id, expected_acls_with_sg_ps_enabled)

    def test_port_security_port_group(self):
        if not self.nb_api.is_port_groups_supported():
            self.skipTest('Port groups is not supported')

        n1 = self._make_network(self.fmt, 'n1', True)
        res = self._create_subnet(self.fmt, n1['network']['id'], '10.0.0.0/24')
        subnet = self.deserialize(self.fmt, res)['subnet']
        p = self._make_port(self.fmt, n1['network']['id'],
                            fixed_ips=[{'subnet_id': subnet['id']}])
        port_id = p['port']['id']
        sg_id = p['port']['security_groups'][0].replace('-', '_')
        pg_name = utils.ovn_port_group_name(sg_id)
        expected_acls_with_sg_ps_enabled = [
            {'match': 'inport == @neutron_pg_drop && ip',
             'action': 'drop',
             'priority': 1001,
             'direction': 'from-lport'},
            {'match': 'outport == @neutron_pg_drop && ip',
             'action': 'drop',
             'priority': 1001,
             'direction': 'to-lport'},
            {'match': 'inport == @' + pg_name + ' && ip6',
             'action': 'allow-related',
             'priority': 1002,
             'direction': 'from-lport'},
            {'match': 'inport == @' + pg_name + ' && ip4',
             'action': 'allow-related',
             'priority': 1002,
             'direction': 'from-lport'},
            {'match': 'outport == @' + pg_name + ' && ip4 && '
                      'ip4.src == $' + pg_name + '_ip4',
             'action': 'allow-related',
             'priority': 1002,
             'direction': 'to-lport'},
            {'match': 'outport == @' + pg_name + ' && ip6 && '
                      'ip6.src == $' + pg_name + '_ip6',
             'action': 'allow-related',
             'priority': 1002,
             'direction': 'to-lport'},
        ]
        self._verify_port_acls(port_id, expected_acls_with_sg_ps_enabled)

        # clear the security groups.
        data = {'port': {'security_groups': []}}
        port_req = self.new_update_request('ports', data, p['port']['id'])
        port_req.get_response(self.api)

        # No security groups and port security enabled - > ACLs should be
        # added to drop the packets.
        expected_acls_with_no_sg_ps_enabled = [
            {'match': 'inport == @neutron_pg_drop && ip',
             'action': 'drop',
             'priority': 1001,
             'direction': 'from-lport'},
            {'match': 'outport == @neutron_pg_drop && ip',
             'action': 'drop',
             'priority': 1001,
             'direction': 'to-lport'},
        ]
        self._verify_port_acls(port_id, expected_acls_with_no_sg_ps_enabled)

        # Disable port security
        data = {'port': {'port_security_enabled': False}}
        port_req = self.new_update_request('ports', data, p['port']['id'])
        port_req.get_response(self.api)
        # No security groups and port security disabled - > No ACLs should be
        # added (allowing all the traffic).
        self._verify_port_acls(port_id, [])

        # Enable port security again with no security groups - > ACLs should
        # be added back to drop the packets.
        data = {'port': {'port_security_enabled': True}}
        port_req = self.new_update_request('ports', data, p['port']['id'])
        port_req.get_response(self.api)
        self._verify_port_acls(port_id, expected_acls_with_no_sg_ps_enabled)

        # Set security groups back
        data = {'port': {'security_groups': p['port']['security_groups']}}
        port_req = self.new_update_request('ports', data, p['port']['id'])
        port_req.get_response(self.api)
        self._verify_port_acls(port_id, expected_acls_with_sg_ps_enabled)


class TestDNSRecords(base.TestOVNFunctionalBase):
    _extension_drivers = ['port_security', 'dns']

    def _validate_dns_records(self, expected_dns_records):
        observed_dns_records = []
        for dns_row in self.nb_api.tables['DNS'].rows.values():
            observed_dns_records.append(
                {'external_ids': dns_row.external_ids,
                 'records': dns_row.records})
        self.assertItemsEqual(expected_dns_records, observed_dns_records)

    def _validate_ls_dns_records(self, lswitch_name, expected_dns_records):
        ls = idlutils.row_by_value(self.nb_api.idl,
                                   'Logical_Switch', 'name', lswitch_name)
        observed_dns_records = []
        for dns_row in ls.dns_records:
            observed_dns_records.append(
                {'external_ids': dns_row.external_ids,
                 'records': dns_row.records})
        self.assertItemsEqual(expected_dns_records, observed_dns_records)

    def setUp(self):
        ovn_config.cfg.CONF.set_override('dns_domain', 'ovn.test')
        super(TestDNSRecords, self).setUp()

    def test_dns_records(self):
        expected_dns_records = []
        nets = []
        for n, cidr in [('n1', '10.0.0.0/24'), ('n2', '20.0.0.0/24')]:
            net_kwargs = {}
            if n == 'n1':
                net_kwargs = {dns_apidef.DNSDOMAIN: 'net-' + n + '.'}
                net_kwargs['arg_list'] = (dns_apidef.DNSDOMAIN,)
            res = self._create_network(self.fmt, n, True, **net_kwargs)
            net = self.deserialize(self.fmt, res)
            nets.append(net)
            res = self._create_subnet(self.fmt, net['network']['id'], cidr)
            self.deserialize(self.fmt, res)

        # At this point no dns records should be created
        n1_lswitch_name = utils.ovn_name(nets[0]['network']['id'])
        n2_lswitch_name = utils.ovn_name(nets[1]['network']['id'])
        self._validate_dns_records(expected_dns_records)
        self._validate_ls_dns_records(n1_lswitch_name, expected_dns_records)
        self._validate_ls_dns_records(n2_lswitch_name, expected_dns_records)

        port_kwargs = {'arg_list': (dns_apidef.DNSNAME,),
                       dns_apidef.DNSNAME: 'n1p1'}
        res = self._create_port(self.fmt, nets[0]['network']['id'],
                                device_id='n1p1', **port_kwargs)
        n1p1 = self.deserialize(self.fmt, res)
        port_ips = " ".join([f['ip_address']
                             for f in n1p1['port']['fixed_ips']])
        expected_dns_records = [
            {'external_ids': {'ls_name': n1_lswitch_name},
             'records': {'n1p1': port_ips, 'n1p1.ovn.test': port_ips,
                         'n1p1.net-n1': port_ips}}
        ]

        self._validate_dns_records(expected_dns_records)
        self._validate_ls_dns_records(n1_lswitch_name,
                                      [expected_dns_records[0]])
        self._validate_ls_dns_records(n2_lswitch_name, [])

        # Create another port, but don't set dns_name. dns record should not
        # be updated.
        res = self._create_port(self.fmt, nets[1]['network']['id'],
                                device_id='n2p1')
        n2p1 = self.deserialize(self.fmt, res)
        self._validate_dns_records(expected_dns_records)

        # Update port p2 with dns_name. The dns record should be updated.
        body = {'dns_name': 'n2p1'}
        data = {'port': body}
        req = self.new_update_request('ports', data, n2p1['port']['id'])
        res = req.get_response(self.api)
        self.assertEqual(200, res.status_int)

        port_ips = " ".join([f['ip_address']
                             for f in n2p1['port']['fixed_ips']])
        expected_dns_records.append(
            {'external_ids': {'ls_name': n2_lswitch_name},
             'records': {'n2p1': port_ips, 'n2p1.ovn.test': port_ips}})
        self._validate_dns_records(expected_dns_records)
        self._validate_ls_dns_records(n1_lswitch_name,
                                      [expected_dns_records[0]])
        self._validate_ls_dns_records(n2_lswitch_name,
                                      [expected_dns_records[1]])

        # Create n1p2
        port_kwargs = {'arg_list': (dns_apidef.DNSNAME,),
                       dns_apidef.DNSNAME: 'n1p2'}
        res = self._create_port(self.fmt, nets[0]['network']['id'],
                                device_id='n1p1', **port_kwargs)
        n1p2 = self.deserialize(self.fmt, res)
        port_ips = " ".join([f['ip_address']
                             for f in n1p2['port']['fixed_ips']])
        expected_dns_records[0]['records']['n1p2'] = port_ips
        expected_dns_records[0]['records']['n1p2.ovn.test'] = port_ips
        expected_dns_records[0]['records']['n1p2.net-n1'] = port_ips
        self._validate_dns_records(expected_dns_records)
        self._validate_ls_dns_records(n1_lswitch_name,
                                      [expected_dns_records[0]])
        self._validate_ls_dns_records(n2_lswitch_name,
                                      [expected_dns_records[1]])

        # Remove device_id from n1p1
        body = {'device_id': ''}
        data = {'port': body}
        req = self.new_update_request('ports', data, n1p1['port']['id'])
        res = req.get_response(self.api)
        self.assertEqual(200, res.status_int)
        expected_dns_records[0]['records'].pop('n1p1')
        expected_dns_records[0]['records'].pop('n1p1.ovn.test')
        expected_dns_records[0]['records'].pop('n1p1.net-n1')
        self._validate_dns_records(expected_dns_records)
        self._validate_ls_dns_records(n1_lswitch_name,
                                      [expected_dns_records[0]])
        self._validate_ls_dns_records(n2_lswitch_name,
                                      [expected_dns_records[1]])

        # Delete n2p1
        self._delete('ports', n2p1['port']['id'])
        expected_dns_records[1]['records'] = {}
        self._validate_dns_records(expected_dns_records)
        self._validate_ls_dns_records(n1_lswitch_name,
                                      [expected_dns_records[0]])
        self._validate_ls_dns_records(n2_lswitch_name,
                                      [expected_dns_records[1]])

        # Delete n2
        self._delete('networks', nets[1]['network']['id'])
        del expected_dns_records[1]
        self._validate_dns_records(expected_dns_records)
        self._validate_ls_dns_records(n1_lswitch_name,
                                      [expected_dns_records[0]])

        # Delete n1p1 and n1p2 and n1
        self._delete('ports', n1p1['port']['id'])
        self._delete('ports', n1p2['port']['id'])
        self._delete('networks', nets[0]['network']['id'])
        self._validate_dns_records([])


class TestNBDbResourcesOverTcp(TestNBDbResources):
    def get_ovsdb_server_protocol(self):
        return 'tcp'


class TestNBDbResourcesOverSsl(TestNBDbResources):
    def get_ovsdb_server_protocol(self):
        return 'ssl'




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\test_ovn_db_sync.py
===========File Type===========
.py
===========File Content===========
# Copyright 2016 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mock

from neutron.services.segments import db as segments_db
from neutron.tests.unit.api import test_extensions
from neutron.tests.unit.extensions import test_extraroute
from neutron.tests.unit.extensions import test_securitygroup
from neutron_lib.api.definitions import dns as dns_apidef
from neutron_lib.api.definitions import l3
from neutron_lib import constants
from neutron_lib import context
from neutron_lib.plugins import directory
from oslo_utils import uuidutils
from ovsdbapp.backend.ovs_idl import idlutils

from networking_ovn.common import acl as acl_utils
from networking_ovn.common import config as ovn_config
from networking_ovn.common import constants as ovn_const
from networking_ovn.common import utils
from networking_ovn import ovn_db_sync
from networking_ovn.tests.functional import base


class TestOvnNbSync(base.TestOVNFunctionalBase):

    _extension_drivers = ['port_security', 'dns']

    def setUp(self):
        ovn_config.cfg.CONF.set_override('dns_domain', 'ovn.test')
        super(TestOvnNbSync, self).setUp()
        ext_mgr = test_extraroute.ExtraRouteTestExtensionManager()
        self.ext_api = test_extensions.setup_extensions_middleware(ext_mgr)
        sg_mgr = test_securitygroup.SecurityGroupTestExtensionManager()
        self._sg_api = test_extensions.setup_extensions_middleware(sg_mgr)
        self.create_lswitches = []
        self.create_lswitch_ports = []
        self.create_lrouters = []
        self.create_lrouter_ports = []
        self.create_lrouter_routes = []
        self.create_lrouter_nats = []
        self.update_lrouter_ports = []
        self.create_acls = []
        self.delete_lswitches = []
        self.delete_lswitch_ports = []
        self.delete_lrouters = []
        self.delete_lrouter_ports = []
        self.delete_lrouter_routes = []
        self.delete_lrouter_nats = []
        self.delete_acls = []
        self.create_address_sets = []
        self.delete_address_sets = []
        self.update_address_sets = []
        self.create_port_groups = []
        self.delete_port_groups = []
        self.expected_dhcp_options_rows = []
        self.reset_lport_dhcpv4_options = []
        self.reset_lport_dhcpv6_options = []
        self.stale_lport_dhcpv4_options = []
        self.stale_lport_dhcpv6_options = []
        self.orphaned_lport_dhcp_options = []
        self.lport_dhcpv4_disabled = {}
        self.lport_dhcpv6_disabled = {}
        self.missed_dhcp_options = []
        self.dirty_dhcp_options = []
        self.lport_dhcp_ignored = []
        self.match_old_mac_dhcp_subnets = []
        self.expected_dns_records = []
        ovn_config.cfg.CONF.set_override('ovn_metadata_enabled', True,
                                         group='ovn')

    def _api_for_resource(self, resource):
        if resource in ['security-groups']:
            return self._sg_api
        else:
            return super(TestOvnNbSync, self)._api_for_resource(resource)

    def _create_resources(self, restart_ovsdb_processes=False):
        net_kwargs = {dns_apidef.DNSDOMAIN: 'ovn.test.'}
        net_kwargs['arg_list'] = (dns_apidef.DNSDOMAIN,)
        res = self._create_network(self.fmt, 'n1', True, **net_kwargs)
        n1 = self.deserialize(self.fmt, res)

        self.expected_dns_records = [
            {'external_ids': {'ls_name': utils.ovn_name(n1['network']['id'])},
             'records': {}}
        ]

        res = self._create_subnet(self.fmt, n1['network']['id'],
                                  '10.0.0.0/24')
        n1_s1 = self.deserialize(self.fmt, res)
        res = self._create_subnet(self.fmt, n1['network']['id'],
                                  '2001:dba::/64', ip_version=6,
                                  enable_dhcp=True)
        n1_s2 = self.deserialize(self.fmt, res)
        res = self._create_subnet(self.fmt, n1['network']['id'],
                                  '2001:dbb::/64', ip_version=6,
                                  ipv6_address_mode='slaac',
                                  ipv6_ra_mode='slaac')
        n1_s3 = self.deserialize(self.fmt, res)
        self.expected_dhcp_options_rows.append({
            'cidr': '10.0.0.0/24',
            'external_ids': {'subnet_id': n1_s1['subnet']['id'],
                             ovn_const.OVN_REV_NUM_EXT_ID_KEY: '0'},
            'options': {'classless_static_route':
                        '{169.254.169.254/32,10.0.0.2, 0.0.0.0/0,10.0.0.1}',
                        'server_id': '10.0.0.1',
                        'server_mac': '01:02:03:04:05:06',
                        'dns_server': '{10.10.10.10}',
                        'lease_time': str(12 * 60 * 60),
                        'mtu': str(n1['network']['mtu']),
                        'router': n1_s1['subnet']['gateway_ip']}})
        self.expected_dhcp_options_rows.append({
            'cidr': '2001:dba::/64',
            'external_ids': {'subnet_id': n1_s2['subnet']['id'],
                             ovn_const.OVN_REV_NUM_EXT_ID_KEY: '0'},
            'options': {'server_id': '01:02:03:04:05:06'}})

        n1_s1_dhcp_options_uuid = (
            self.mech_driver._nb_ovn.get_subnet_dhcp_options(
                n1_s1['subnet']['id'])['subnet']['uuid'])
        n1_s2_dhcpv6_options_uuid = (
            self.mech_driver._nb_ovn.get_subnet_dhcp_options(
                n1_s2['subnet']['id'])['subnet']['uuid'])
        update_port_ids_v4 = []
        update_port_ids_v6 = []
        n1_port_dict = {}
        for p in ['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7']:
            if p in ['p1', 'p5']:
                port_kwargs = {'arg_list': (dns_apidef.DNSNAME,),
                               dns_apidef.DNSNAME: 'n1-' + p,
                               'device_id': 'n1-' + p}
            else:
                port_kwargs = {}

            res = self._create_port(self.fmt, n1['network']['id'],
                                    name='n1-' + p,
                                    device_owner='compute:None',
                                    **port_kwargs)
            port = self.deserialize(self.fmt, res)
            n1_port_dict[p] = port['port']['id']
            lport_name = port['port']['id']
            lswitch_name = 'neutron-' + n1['network']['id']
            if p in ['p1', 'p5']:
                port_ips = " ".join([f['ip_address']
                                     for f in port['port']['fixed_ips']])
                hname = 'n1-' + p
                self.expected_dns_records[0]['records'][hname] = port_ips
                hname = 'n1-' + p + '.ovn.test'
                self.expected_dns_records[0]['records'][hname] = port_ips
            if p == 'p1':
                fake_subnet = {'cidr': '11.11.11.11/24'}
                dhcp_acls = acl_utils.add_acl_dhcp(port['port'], fake_subnet)
                for dhcp_acl in dhcp_acls:
                    self.create_acls.append(dhcp_acl)
            elif p == 'p2':
                self.delete_lswitch_ports.append((lport_name, lswitch_name))
                update_port_ids_v4.append(port['port']['id'])
                update_port_ids_v6.append(port['port']['id'])
                self.expected_dhcp_options_rows.append({
                    'cidr': '10.0.0.0/24',
                    'external_ids': {'subnet_id': n1_s1['subnet']['id'],
                                     ovn_const.OVN_REV_NUM_EXT_ID_KEY: '0',
                                     'port_id': port['port']['id']},
                    'options': {
                        'classless_static_route':
                        '{169.254.169.254/32,10.0.0.2, 0.0.0.0/0,10.0.0.1}',
                        'server_id': '10.0.0.1',
                        'server_mac': '01:02:03:04:05:06',
                        'lease_time': str(12 * 60 * 60),
                        'mtu': str(n1['network']['mtu']),
                        'router': n1_s1['subnet']['gateway_ip'],
                        'tftp_server': '20.0.0.20',
                        'dns_server': '8.8.8.8'}})
                self.expected_dhcp_options_rows.append({
                    'cidr': '2001:dba::/64',
                    'external_ids': {'subnet_id': n1_s2['subnet']['id'],
                                     ovn_const.OVN_REV_NUM_EXT_ID_KEY: '0',
                                     'port_id': port['port']['id']},
                    'options': {'server_id': '01:02:03:04:05:06',
                                'domain_search': 'foo-domain'}})
                self.dirty_dhcp_options.append({
                    'subnet_id': n1_s1['subnet']['id'],
                    'port_id': lport_name})
                self.dirty_dhcp_options.append({
                    'subnet_id': n1_s2['subnet']['id'],
                    'port_id': lport_name})
            elif p == 'p3':
                self.delete_acls.append((lport_name, lswitch_name))
                self.reset_lport_dhcpv4_options.append(lport_name)
                self.lport_dhcpv6_disabled.update({
                    lport_name: n1_s2_dhcpv6_options_uuid})
                data = {'port': {
                    'extra_dhcp_opts': [{'ip_version': 6,
                                         'opt_name': 'dhcp_disabled',
                                         'opt_value': 'True'}]}}
                port_req = self.new_update_request('ports', data, lport_name)
                port_req.get_response(self.api)
            elif p == 'p4':
                self.lport_dhcpv4_disabled.update({
                    lport_name: n1_s1_dhcp_options_uuid})
                data = {'port': {
                    'extra_dhcp_opts': [{'ip_version': 4,
                                         'opt_name': 'dhcp_disabled',
                                         'opt_value': 'True'}]}}
                port_req = self.new_update_request('ports', data, lport_name)
                port_req.get_response(self.api)
                self.reset_lport_dhcpv6_options.append(lport_name)
            elif p == 'p5':
                self.stale_lport_dhcpv4_options.append({
                    'subnet_id': n1_s1['subnet']['id'],
                    'port_id': port['port']['id'],
                    'cidr': '10.0.0.0/24',
                    'options': {'server_id': '10.0.0.254',
                                'server_mac': '01:02:03:04:05:06',
                                'lease_time': str(3 * 60 * 60),
                                'mtu': str(n1['network']['mtu'] / 2),
                                'router': '10.0.0.254',
                                'tftp_server': '20.0.0.234',
                                'dns_server': '8.8.8.8'},
                    'external_ids': {'subnet_id': n1_s1['subnet']['id'],
                                     'port_id': port['port']['id']},
                    })
            elif p == 'p6':
                self.delete_lswitch_ports.append((lport_name, lswitch_name))
            elif p == 'p7':
                update_port_ids_v4.append(port['port']['id'])
                update_port_ids_v6.append(port['port']['id'])
                self.expected_dhcp_options_rows.append({
                    'cidr': '10.0.0.0/24',
                    'external_ids': {'subnet_id': n1_s1['subnet']['id'],
                                     ovn_const.OVN_REV_NUM_EXT_ID_KEY: '0',
                                     'port_id': port['port']['id']},
                    'options': {
                        'classless_static_route':
                        '{169.254.169.254/32,10.0.0.2, 0.0.0.0/0,10.0.0.1}',
                        'server_id': '10.0.0.1',
                        'server_mac': '01:02:03:04:05:06',
                        'dns_server': '{10.10.10.10}',
                        'lease_time': str(12 * 60 * 60),
                        'mtu': str(n1['network']['mtu']),
                        'router': n1_s1['subnet']['gateway_ip'],
                        'tftp_server': '20.0.0.20',
                        'dns_server': '8.8.8.8'}})
                self.expected_dhcp_options_rows.append({
                    'cidr': '2001:dba::/64',
                    'external_ids': {'subnet_id': n1_s2['subnet']['id'],
                                     ovn_const.OVN_REV_NUM_EXT_ID_KEY: '0',
                                     'port_id': port['port']['id']},
                    'options': {'server_id': '01:02:03:04:05:06',
                                'domain_search': 'foo-domain'}})
                self.reset_lport_dhcpv4_options.append(lport_name)
                self.reset_lport_dhcpv6_options.append(lport_name)
        self.dirty_dhcp_options.append({'subnet_id': n1_s1['subnet']['id']})
        self.dirty_dhcp_options.append({'subnet_id': n1_s2['subnet']['id']})

        res = self._create_network(self.fmt, 'n2', True, **net_kwargs)
        n2 = self.deserialize(self.fmt, res)
        res = self._create_subnet(self.fmt, n2['network']['id'],
                                  '20.0.0.0/24')
        n2_s1 = self.deserialize(self.fmt, res)
        res = self._create_subnet(self.fmt, n2['network']['id'],
                                  '2001:dbd::/64', ip_version=6)
        n2_s2 = self.deserialize(self.fmt, res)
        self.expected_dhcp_options_rows.append({
            'cidr': '20.0.0.0/24',
            'external_ids': {'subnet_id': n2_s1['subnet']['id'],
                             ovn_const.OVN_REV_NUM_EXT_ID_KEY: '0'},
            'options': {'classless_static_route':
                        '{169.254.169.254/32,20.0.0.2, 0.0.0.0/0,20.0.0.1}',
                        'server_id': '20.0.0.1',
                        'server_mac': '01:02:03:04:05:06',
                        'dns_server': '{10.10.10.10}',
                        'lease_time': str(12 * 60 * 60),
                        'mtu': str(n2['network']['mtu']),
                        'router': n2_s1['subnet']['gateway_ip']}})
        self.expected_dhcp_options_rows.append({
            'cidr': '2001:dbd::/64',
            'external_ids': {'subnet_id': n2_s2['subnet']['id'],
                             ovn_const.OVN_REV_NUM_EXT_ID_KEY: '0'},
            'options': {'server_id': '01:02:03:04:05:06'}})

        for p in ['p1', 'p2']:
            port = self._make_port(self.fmt, n2['network']['id'],
                                   name='n2-' + p,
                                   device_owner='compute:None')
            if p == 'p1':
                update_port_ids_v4.append(port['port']['id'])
                self.expected_dhcp_options_rows.append({
                    'cidr': '20.0.0.0/24',
                    'external_ids': {'subnet_id': n2_s1['subnet']['id'],
                                     ovn_const.OVN_REV_NUM_EXT_ID_KEY: '0',
                                     'port_id': port['port']['id']},
                    'options': {
                        'classless_static_route':
                        '{169.254.169.254/32,20.0.0.2, 0.0.0.0/0,20.0.0.1}',
                        'server_id': '20.0.0.1',
                        'server_mac': '01:02:03:04:05:06',
                        'lease_time': str(12 * 60 * 60),
                        'mtu': str(n1['network']['mtu']),
                        'router': n2_s1['subnet']['gateway_ip'],
                        'tftp_server': '20.0.0.20',
                        'dns_server': '8.8.8.8'}})
        self.missed_dhcp_options.extend([
            opts['uuid']
            for opts in self.mech_driver._nb_ovn.get_subnets_dhcp_options(
                [n2_s1['subnet']['id'], n2_s2['subnet']['id']])])

        for port_id in update_port_ids_v4:
            data = {'port': {'extra_dhcp_opts': [{'ip_version': 4,
                                                  'opt_name': 'tftp-server',
                                                  'opt_value': '20.0.0.20'},
                                                 {'ip_version': 4,
                                                  'opt_name': 'dns-server',
                                                  'opt_value': '8.8.8.8'}]}}
            port_req = self.new_update_request('ports', data, port_id)
            port_req.get_response(self.api)
        for port_id in update_port_ids_v6:
            data = {'port': {'extra_dhcp_opts': [{'ip_version': 6,
                                                  'opt_name': 'domain-search',
                                                  'opt_value': 'foo-domain'}]}}
            port_req = self.new_update_request('ports', data, port_id)
            port_req.get_response(self.api)

        # External network and subnet
        e1 = self._make_network(self.fmt, 'e1', True,
                                arg_list=('router:external',
                                          'provider:network_type',
                                          'provider:physical_network'),
                                **{'router:external': True,
                                   'provider:network_type': 'flat',
                                   'provider:physical_network': 'public'})
        self.assertEqual(True, e1['network']['router:external'])
        self.assertEqual('flat', e1['network']['provider:network_type'])
        self.assertEqual('public', e1['network']['provider:physical_network'])
        res = self._create_subnet(self.fmt, e1['network']['id'],
                                  '100.0.0.0/24', gateway_ip='100.0.0.254',
                                  allocation_pools=[{'start': '100.0.0.2',
                                                     'end': '100.0.0.253'}],
                                  enable_dhcp=False)
        e1_s1 = self.deserialize(self.fmt, res)

        self.create_lswitches.append('neutron-' + uuidutils.generate_uuid())
        self.create_lswitch_ports.append(('neutron-' +
                                          uuidutils.generate_uuid(),
                                          'neutron-' + n1['network']['id']))
        self.create_lswitch_ports.append(('neutron-' +
                                          uuidutils.generate_uuid(),
                                          'neutron-' + n1['network']['id']))
        self.delete_lswitches.append('neutron-' + n2['network']['id'])
        self.delete_lswitch_ports.append(
            (utils.ovn_provnet_port_name(e1['network']['id']),
             utils.ovn_name(e1['network']['id'])))

        r1 = self.l3_plugin.create_router(
            self.context,
            {'router': {
                'name': 'r1', 'admin_state_up': True,
                'tenant_id': self._tenant_id,
                'external_gateway_info': {
                    'enable_snat': True,
                    'network_id': e1['network']['id'],
                    'external_fixed_ips': [
                        {'ip_address': '100.0.0.2',
                         'subnet_id': e1_s1['subnet']['id']}]}}})
        self.l3_plugin.add_router_interface(
            self.context, r1['id'], {'subnet_id': n1_s1['subnet']['id']})
        r1_p2 = self.l3_plugin.add_router_interface(
            self.context, r1['id'], {'subnet_id': n1_s2['subnet']['id']})
        self.l3_plugin.add_router_interface(
            self.context, r1['id'], {'subnet_id': n1_s3['subnet']['id']})
        r1_p3 = self.l3_plugin.add_router_interface(
            self.context, r1['id'], {'subnet_id': n2_s1['subnet']['id']})
        self.update_lrouter_ports.append(('lrp-' + r1_p2['port_id'],
                                          'neutron-' + r1['id'],
                                          n1_s2['subnet']['gateway_ip']))
        self.delete_lrouter_ports.append(('lrp-' + r1_p3['port_id'],
                                          'neutron-' + r1['id']))
        self.delete_lrouter_ports.append(('lrp-' + r1['gw_port_id'],
                                          'neutron-' + r1['id']))
        self.l3_plugin.update_router(
            self.context, r1['id'],
            {'router': {'routes': [{'destination': '10.10.0.0/24',
                                    'nexthop': '20.0.0.10'},
                                   {'destination': '10.11.0.0/24',
                                    'nexthop': '20.0.0.11'}]}})
        r1_f1 = self.l3_plugin.create_floatingip(
            self.context, {'floatingip': {
                'tenant_id': self._tenant_id,
                'floating_network_id': e1['network']['id'],
                'floating_ip_address': '100.0.0.20',
                'subnet_id': None,
                'port_id': n1_port_dict['p1']}})
        r1_f2 = self.l3_plugin.create_floatingip(
            self.context, {'floatingip': {
                'tenant_id': self._tenant_id,
                'floating_network_id': e1['network']['id'],
                'subnet_id': None,
                'floating_ip_address': '100.0.0.21'}})
        self.l3_plugin.update_floatingip(
            self.context, r1_f2['id'], {'floatingip': {
                'port_id': n1_port_dict['p2']}})

        # update External subnet gateway ip to test function _subnet_update
        #  of L3 OVN plugin.
        data = {'subnet': {'gateway_ip': '100.0.0.1'}}
        subnet_req = self.new_update_request(
            'subnets', data, e1_s1['subnet']['id'])
        subnet_req.get_response(self.api)

        # Static routes
        self.create_lrouter_routes.append(('neutron-' + r1['id'],
                                           '10.12.0.0/24',
                                           '20.0.0.12'))
        self.create_lrouter_routes.append(('neutron-' + r1['id'],
                                           '10.13.0.0/24',
                                           '20.0.0.13'))
        self.delete_lrouter_routes.append(('neutron-' + r1['id'],
                                           '10.10.0.0/24',
                                           '20.0.0.10'))
        # Gateway default route
        self.delete_lrouter_routes.append(('neutron-' + r1['id'],
                                           '0.0.0.0/0',
                                           '100.0.0.1'))
        # Gateway sNATs
        self.create_lrouter_nats.append(('neutron-' + r1['id'],
                                         {'external_ip': '100.0.0.100',
                                          'logical_ip': '200.0.0.0/24',
                                          'type': 'snat'}))
        self.delete_lrouter_nats.append(('neutron-' + r1['id'],
                                         {'external_ip': '100.0.0.2',
                                          'logical_ip': '10.0.0.0/24',
                                          'type': 'snat'}))
        # Floating IPs
        self.create_lrouter_nats.append(('neutron-' + r1['id'],
                                         {'external_ip': '100.0.0.200',
                                          'logical_ip': '200.0.0.200',
                                          'type': 'dnat_and_snat'}))
        self.create_lrouter_nats.append(('neutron-' + r1['id'],
                                         {'external_ip': '100.0.0.201',
                                          'logical_ip': '200.0.0.201',
                                          'type': 'dnat_and_snat',
                                          'external_mac': '01:02:03:04:05:06',
                                          'logical_port': 'vm1'
                                          }))
        self.delete_lrouter_nats.append(('neutron-' + r1['id'],
                                         {'external_ip':
                                             r1_f1['floating_ip_address'],
                                          'logical_ip':
                                             r1_f1['fixed_ip_address'],
                                          'type': 'dnat_and_snat'}))

        res = self._create_network(self.fmt, 'n4', True, **net_kwargs)
        n4 = self.deserialize(self.fmt, res)
        res = self._create_subnet(self.fmt, n4['network']['id'],
                                  '40.0.0.0/24', enable_dhcp=False)
        self.expected_dns_records.append(
            {'external_ids': {'ls_name': utils.ovn_name(n4['network']['id'])},
             'records': {}}
        )
        n4_s1 = self.deserialize(self.fmt, res)
        n4_port_dict = {}
        for p in ['p1', 'p2', 'p3']:
            if p in ['p1', 'p2']:
                port_kwargs = {'arg_list': (dns_apidef.DNSNAME,),
                               dns_apidef.DNSNAME: 'n4-' + p,
                               'device_id': 'n4-' + p}
            else:
                port_kwargs = {}

            res = self._create_port(self.fmt, n4['network']['id'],
                                    name='n4-' + p,
                                    device_owner='compute:None',
                                    **port_kwargs)
            port = self.deserialize(self.fmt, res)

            if p in ['p1', 'p2']:
                port_ips = " ".join([f['ip_address']
                                     for f in port['port']['fixed_ips']])
                hname = 'n4-' + p
                self.expected_dns_records[1]['records'][hname] = port_ips
                hname = 'n4-' + p + '.ovn.test'
                self.expected_dns_records[1]['records'][hname] = port_ips

            n4_port_dict[p] = port['port']['id']
            self.lport_dhcp_ignored.append(port['port']['id'])

        r2 = self.l3_plugin.create_router(
            self.context,
            {'router': {'name': 'r2', 'admin_state_up': True,
                        'tenant_id': self._tenant_id}})
        n1_prtr = self._make_port(self.fmt, n1['network']['id'],
                                  name='n1-p-rtr')
        self.l3_plugin.add_router_interface(
            self.context, r2['id'], {'port_id': n1_prtr['port']['id']})
        self.l3_plugin.add_router_interface(
            self.context, r2['id'], {'subnet_id': n4_s1['subnet']['id']})
        self.l3_plugin.update_router(
            self.context, r2['id'],
            # FIXME(lucasagomes): Add "routes" back, it has been
            # removed to avoid a race condition that was happening from
            # time to time. The error was: "Invalid format for routes:
            # [{'destination': '10.20.0.0/24', 'nexthop': '10.0.0.20'}],
            # the nexthop is used by route". It seems to be a race within
            # the tests itself, running the functional tests without
            # any concurrency doesn't fail when the "routes" are set.
            #
            # {'router': {'routes': [{'destination': '10.20.0.0/24',
            #                         'nexthop': '10.0.0.20'}],
            #             ...
            {'router': {'external_gateway_info': {
                        'enable_snat': False,
                        'network_id': e1['network']['id'],
                        'external_fixed_ips': [
                            {'ip_address': '100.0.0.3',
                             'subnet_id': e1_s1['subnet']['id']}]}}})
        self.l3_plugin.create_floatingip(
            self.context, {'floatingip': {
                'tenant_id': self._tenant_id,
                'floating_network_id': e1['network']['id'],
                'floating_ip_address': '100.0.0.30',
                'subnet_id': None,
                'port_id': n4_port_dict['p1']}})
        self.l3_plugin.create_floatingip(
            self.context, {'floatingip': {
                'tenant_id': self._tenant_id,
                'floating_network_id': e1['network']['id'],
                'floating_ip_address': '100.0.0.31',
                'subnet_id': None,
                'port_id': n4_port_dict['p2']}})
        # To test l3_plugin.disassociate_floatingips, associating floating IP
        # to port p3 and then deleting p3.
        self.l3_plugin.create_floatingip(
            self.context, {'floatingip': {
                'tenant_id': self._tenant_id,
                'floating_network_id': e1['network']['id'],
                'floating_ip_address': '100.0.0.32',
                'subnet_id': None,
                'port_id': n4_port_dict['p3']}})
        self._delete('ports', n4_port_dict['p3'])

        self.create_lrouters.append('neutron-' + uuidutils.generate_uuid())
        self.create_lrouter_ports.append(('lrp-' + uuidutils.generate_uuid(),
                                          'neutron-' + r1['id']))
        self.create_lrouter_ports.append(('lrp-' + uuidutils.generate_uuid(),
                                          'neutron-' + r1['id']))
        self.delete_lrouters.append('neutron-' + r2['id'])

        address_set_name = n1_prtr['port']['security_groups'][0]
        self.create_address_sets.extend([('fake_sg', 'ip4'),
                                         ('fake_sg', 'ip6')])
        self.delete_address_sets.append((address_set_name, 'ip6'))
        address_adds = ['10.0.0.101', '10.0.0.102']
        address_dels = []
        for address in n1_prtr['port']['fixed_ips']:
            address_dels.append(address['ip_address'])
        self.update_address_sets.append((address_set_name, 'ip4',
                                         address_adds, address_dels))

        self.create_port_groups.extend([{'name': 'pg1', 'acls': []},
                                        {'name': 'pg2', 'acls': []}])
        self.delete_port_groups.append(
            utils.ovn_port_group_name(n1_prtr['port']['security_groups'][0]))
        # Create a network and subnet with orphaned OVN resources.
        n3 = self._make_network(self.fmt, 'n3', True)
        res = self._create_subnet(self.fmt, n3['network']['id'],
                                  '30.0.0.0/24')
        n3_s1 = self.deserialize(self.fmt, res)
        res = self._create_subnet(self.fmt, n3['network']['id'],
                                  '2001:dbc::/64', ip_version=6)
        n3_s2 = self.deserialize(self.fmt, res)
        if not restart_ovsdb_processes:
            # Test using original mac when syncing.
            dhcp_mac_v4 = (self.mech_driver._nb_ovn.get_subnet_dhcp_options(
                n3_s1['subnet']['id'])['subnet'].get('options', {})
                .get('server_mac'))
            dhcp_mac_v6 = (self.mech_driver._nb_ovn.get_subnet_dhcp_options(
                n3_s2['subnet']['id'])['subnet'].get('options', {})
                .get('server_id'))
            self.assertTrue(dhcp_mac_v4 is not None)
            self.assertTrue(dhcp_mac_v6 is not None)
            self.match_old_mac_dhcp_subnets.append(n3_s1['subnet']['id'])
            self.match_old_mac_dhcp_subnets.append(n3_s2['subnet']['id'])
        else:
            dhcp_mac_v4 = '01:02:03:04:05:06'
            dhcp_mac_v6 = '01:02:03:04:05:06'
        self.expected_dhcp_options_rows.append({
            'cidr': '30.0.0.0/24',
            'external_ids': {'subnet_id': n3_s1['subnet']['id'],
                             ovn_const.OVN_REV_NUM_EXT_ID_KEY: '0'},
            'options': {'classless_static_route':
                        '{169.254.169.254/32,30.0.0.2, 0.0.0.0/0,30.0.0.1}',
                        'server_id': '30.0.0.1',
                        'dns_server': '{10.10.10.10}',
                        'server_mac': dhcp_mac_v4,
                        'lease_time': str(12 * 60 * 60),
                        'mtu': str(n3['network']['mtu']),
                        'router': n3_s1['subnet']['gateway_ip']}})
        self.expected_dhcp_options_rows.append({
            'cidr': '2001:dbc::/64',
            'external_ids': {'subnet_id': n3_s2['subnet']['id'],
                             ovn_const.OVN_REV_NUM_EXT_ID_KEY: '0'},
            'options': {'server_id': dhcp_mac_v6}})
        fake_port_id1 = uuidutils.generate_uuid()
        fake_port_id2 = uuidutils.generate_uuid()
        self.create_lswitch_ports.append(('neutron-' + fake_port_id1,
                                          'neutron-' + n3['network']['id']))
        self.create_lswitch_ports.append(('neutron-' + fake_port_id2,
                                          'neutron-' + n3['network']['id']))
        stale_dhcpv4_options1 = {
            'subnet_id': n3_s1['subnet']['id'],
            'port_id': fake_port_id1,
            'cidr': '30.0.0.0/24',
            'options': {'server_id': '30.0.0.254',
                        'server_mac': dhcp_mac_v4,
                        'lease_time': str(3 * 60 * 60),
                        'mtu': str(n3['network']['mtu'] / 2),
                        'router': '30.0.0.254',
                        'tftp_server': '30.0.0.234',
                        'dns_server': '8.8.8.8'},
            'external_ids': {'subnet_id': n3_s1['subnet']['id'],
                             'port_id': fake_port_id1},
            }
        self.stale_lport_dhcpv4_options.append(stale_dhcpv4_options1)
        stale_dhcpv4_options2 = stale_dhcpv4_options1.copy()
        stale_dhcpv4_options2.update({
            'port_id': fake_port_id2,
            'external_ids': {'subnet_id': n3_s1['subnet']['id'],
                             'port_id': fake_port_id2}})
        self.stale_lport_dhcpv4_options.append(stale_dhcpv4_options2)
        self.orphaned_lport_dhcp_options.append(fake_port_id2)
        stale_dhcpv6_options1 = {
            'subnet_id': n3_s2['subnet']['id'],
            'port_id': fake_port_id1,
            'cidr': '2001:dbc::/64',
            'options': {'server_id': dhcp_mac_v6,
                        'domain-search': 'foo-domain'},
            'external_ids': {'subnet_id': n3_s2['subnet']['id'],
                             'port_id': fake_port_id1},
            }
        self.stale_lport_dhcpv6_options.append(stale_dhcpv6_options1)
        stale_dhcpv6_options2 = stale_dhcpv6_options1.copy()
        stale_dhcpv6_options2.update({
            'port_id': fake_port_id2,
            'external_ids': {'subnet_id': n3_s2['subnet']['id'],
                             'port_id': fake_port_id2}})
        self.stale_lport_dhcpv6_options.append(stale_dhcpv6_options2)
        fake_port = {'id': fake_port_id1, 'network_id': n3['network']['id']}
        dhcp_acls = acl_utils.add_acl_dhcp(fake_port, n3_s1['subnet'])
        for dhcp_acl in dhcp_acls:
            self.create_acls.append(dhcp_acl)
        columns = list(self.nb_api.tables['ACL'].columns)
        if not (('name' in columns) and ('severity' in columns)):
            for acl in self.create_acls:
                acl.pop('name')
                acl.pop('severity')

    def _modify_resources_in_nb_db(self):
        self._delete_metadata_ports()

        with self.nb_api.transaction(check_error=True) as txn:
            for lswitch_name in self.create_lswitches:
                external_ids = {ovn_const.OVN_NETWORK_NAME_EXT_ID_KEY:
                                lswitch_name}
                txn.add(self.nb_api.ls_add(lswitch_name, True,
                                           external_ids=external_ids))

            for lswitch_name in self.delete_lswitches:
                txn.add(self.nb_api.ls_del(lswitch_name, True))

            for lport_name, lswitch_name in self.create_lswitch_ports:
                external_ids = {ovn_const.OVN_PORT_NAME_EXT_ID_KEY:
                                lport_name}
                txn.add(self.nb_api.create_lswitch_port(
                    lport_name, lswitch_name, True, external_ids=external_ids))

            for lport_name, lswitch_name in self.delete_lswitch_ports:
                txn.add(self.nb_api.delete_lswitch_port(lport_name,
                                                        lswitch_name, True))

            for lrouter_name in self.create_lrouters:
                external_ids = {ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY:
                                lrouter_name}
                txn.add(self.nb_api.create_lrouter(lrouter_name, True,
                                                   external_ids=external_ids))

            for lrouter_name in self.delete_lrouters:
                txn.add(self.nb_api.delete_lrouter(lrouter_name, True))

            for lrport, lrouter_name in self.create_lrouter_ports:
                txn.add(self.nb_api.add_lrouter_port(lrport, lrouter_name))

            for lrport, lrouter_name, networks in self.update_lrouter_ports:
                txn.add(self.nb_api.update_lrouter_port(
                    lrport, True, **{'networks': [networks],
                                     'ipv6_ra_configs': {'foo': 'bar'}}))

            for lrport, lrouter_name in self.delete_lrouter_ports:
                txn.add(self.nb_api.delete_lrouter_port(lrport,
                                                        lrouter_name, True))

            for lrouter_name, ip_prefix, nexthop in self.create_lrouter_routes:
                txn.add(self.nb_api.add_static_route(lrouter_name,
                                                     ip_prefix=ip_prefix,
                                                     nexthop=nexthop))

            for lrouter_name, ip_prefix, nexthop in self.delete_lrouter_routes:
                txn.add(self.nb_api.delete_static_route(lrouter_name,
                                                        ip_prefix, nexthop,
                                                        True))

            for lrouter_name, nat_dict in(
                    self.create_lrouter_nats):
                txn.add(self.nb_api.add_nat_rule_in_lrouter(
                    lrouter_name, **nat_dict))

            for lrouter_name, nat_dict in(
                    self.delete_lrouter_nats):
                txn.add(self.nb_api.delete_nat_rule_in_lrouter(
                    lrouter_name, if_exists=True, **nat_dict))

            for acl in self.create_acls:
                txn.add(self.nb_api.add_acl(**acl))

            for lport_name, lswitch_name in self.delete_acls:
                txn.add(self.nb_api.delete_acl(lswitch_name,
                                               lport_name, True))

            for name, ip_version in self.create_address_sets:
                ovn_name = utils.ovn_addrset_name(name, ip_version)
                external_ids = {ovn_const.OVN_SG_EXT_ID_KEY: name}
                txn.add(self.nb_api.create_address_set(
                    ovn_name, True, external_ids=external_ids))

            for name, ip_version in self.delete_address_sets:
                ovn_name = utils.ovn_addrset_name(name, ip_version)
                txn.add(self.nb_api.delete_address_set(ovn_name, True))

            for name, ip_version, ip_adds, ip_dels in self.update_address_sets:
                ovn_name = utils.ovn_addrset_name(name, ip_version)
                txn.add(self.nb_api.update_address_set(ovn_name,
                                                       ip_adds, ip_dels, True))

            if self.nb_api.is_port_groups_supported():
                for pg in self.create_port_groups:
                    txn.add(self.nb_api.pg_add(**pg))
                for pg in self.delete_port_groups:
                    txn.add(self.nb_api.pg_del(pg))

            for lport_name in self.reset_lport_dhcpv4_options:
                txn.add(self.nb_api.set_lswitch_port(lport_name, True,
                                                     dhcpv4_options=[]))

            for lport_name in self.reset_lport_dhcpv6_options:
                txn.add(self.nb_api.set_lswitch_port(lport_name, True,
                                                     dhcpv6_options=[]))

            for dhcp_opts in self.stale_lport_dhcpv4_options:
                dhcpv4_opts = txn.add(self.nb_api.add_dhcp_options(
                    dhcp_opts['subnet_id'],
                    port_id=dhcp_opts['port_id'],
                    cidr=dhcp_opts['cidr'],
                    options=dhcp_opts['options'],
                    external_ids=dhcp_opts['external_ids'],
                    may_exist=False))
                if dhcp_opts['port_id'] in self.orphaned_lport_dhcp_options:
                    continue
                txn.add(self.nb_api.set_lswitch_port(
                    lport_name, True, dhcpv4_options=dhcpv4_opts))

            for dhcp_opts in self.stale_lport_dhcpv6_options:
                dhcpv6_opts = txn.add(self.nb_api.add_dhcp_options(
                    dhcp_opts['subnet_id'],
                    port_id=dhcp_opts['port_id'],
                    cidr=dhcp_opts['cidr'],
                    options=dhcp_opts['options'],
                    external_ids=dhcp_opts['external_ids'],
                    may_exist=False))
                if dhcp_opts['port_id'] in self.orphaned_lport_dhcp_options:
                    continue
                txn.add(self.nb_api.set_lswitch_port(
                    lport_name, True, dhcpv6_options=dhcpv6_opts))

            for row_uuid in self.missed_dhcp_options:
                txn.add(self.nb_api.delete_dhcp_options(row_uuid))

            for dhcp_opts in self.dirty_dhcp_options:
                external_ids = {'subnet_id': dhcp_opts['subnet_id']}
                if dhcp_opts.get('port_id'):
                    external_ids['port_id'] = dhcp_opts['port_id']
                txn.add(self.nb_api.add_dhcp_options(
                    dhcp_opts['subnet_id'],
                    port_id=dhcp_opts.get('port_id'),
                    external_ids=external_ids,
                    options={'foo': 'bar'}))

            for port_id in self.lport_dhcpv4_disabled:
                txn.add(self.nb_api.set_lswitch_port(
                    port_id, True,
                    dhcpv4_options=[self.lport_dhcpv4_disabled[port_id]]))

            for port_id in self.lport_dhcpv6_disabled:
                txn.add(self.nb_api.set_lswitch_port(
                    port_id, True,
                    dhcpv6_options=[self.lport_dhcpv6_disabled[port_id]]))

            # Delete the first DNS record and clear the second row records
            i = 0
            for dns_row in self.nb_api.tables['DNS'].rows.values():
                if i == 0:
                    txn.add(self.nb_api.dns_del(dns_row.uuid))
                else:
                    txn.add(self.nb_api.dns_set_records(dns_row.uuid, **{}))
                i += 1

    def _validate_networks(self, should_match=True):
        db_networks = self._list('networks')
        db_net_ids = [net['id'] for net in db_networks['networks']]
        db_provnet_ports = [utils.ovn_provnet_port_name(net['id'])
                            for net in db_networks['networks']
                            if net.get('provider:physical_network')]

        # Get the list of lswitch ids stored in the OVN plugin IDL
        _plugin_nb_ovn = self.mech_driver._nb_ovn
        plugin_lswitch_ids = [
            row.name.replace('neutron-', '') for row in (
                _plugin_nb_ovn._tables['Logical_Switch'].rows.values())]

        # Get the list of lswitch ids stored in the monitor IDL connection
        monitor_lswitch_ids = [
            row.name.replace('neutron-', '') for row in (
                self.nb_api.tables['Logical_Switch'].rows.values())]

        # Get the list of provnet ports stored in the OVN plugin IDL
        plugin_provnet_ports = [row.name for row in (
            _plugin_nb_ovn._tables['Logical_Switch_Port'].rows.values())
            if row.name.startswith(ovn_const.OVN_PROVNET_PORT_NAME_PREFIX)]

        # Get the list of provnet ports stored in the monitor IDL connection
        monitor_provnet_ports = [row.name for row in (
            self.nb_api.tables['Logical_Switch_Port'].rows.values())
            if row.name.startswith(ovn_const.OVN_PROVNET_PORT_NAME_PREFIX)]

        if should_match:
            self.assertItemsEqual(db_net_ids, plugin_lswitch_ids)
            self.assertItemsEqual(db_net_ids, monitor_lswitch_ids)
            self.assertItemsEqual(db_provnet_ports, plugin_provnet_ports)
            self.assertItemsEqual(db_provnet_ports, monitor_provnet_ports)
        else:
            self.assertRaises(
                AssertionError, self.assertItemsEqual, db_net_ids,
                plugin_lswitch_ids)

            self.assertRaises(
                AssertionError, self.assertItemsEqual, db_net_ids,
                monitor_lswitch_ids)

            self.assertRaises(
                AssertionError, self.assertItemsEqual, db_provnet_ports,
                plugin_provnet_ports)

            self.assertRaises(
                AssertionError, self.assertItemsEqual, db_provnet_ports,
                monitor_provnet_ports)

    def _validate_metadata_ports(self, should_match=True):
        """Validate metadata ports.

        This method will check that all networks have one and only one metadata
        port and that every metadata port in Neutron also exists in OVN.
        """
        db_ports = self._list('ports')
        db_metadata_ports_ids = []
        db_metadata_ports_nets = []
        for port in db_ports['ports']:
            if port['device_owner'] == constants.DEVICE_OWNER_DHCP:
                db_metadata_ports_ids.append(port['id'])
                db_metadata_ports_nets.append(port['network_id'])
        db_networks = self._list('networks')
        db_net_ids = [net['id'] for net in db_networks['networks']]

        # Retrieve all localports in OVN
        _plugin_nb_ovn = self.mech_driver._nb_ovn
        plugin_metadata_ports = [row.name for row in (
            _plugin_nb_ovn._tables['Logical_Switch_Port'].rows.values())
            if row.type == ovn_const.OVN_NEUTRON_OWNER_TO_PORT_TYPE.get(
                constants.DEVICE_OWNER_DHCP)]

        if should_match:
            # Check that metadata ports exist in both Neutron and OVN dbs.
            self.assertItemsEqual(db_metadata_ports_ids, plugin_metadata_ports)
            # Check that all networks have one and only one metadata port.
            self.assertItemsEqual(db_metadata_ports_nets, db_net_ids)
        else:
            metadata_sync = (sorted(db_metadata_ports_ids) ==
                             sorted(plugin_metadata_ports))
            metadata_unique = (sorted(db_net_ids) ==
                               sorted(db_metadata_ports_nets))
            self.assertFalse(metadata_sync and metadata_unique)

    def _validate_ports(self, should_match=True):
        db_ports = self._list('ports')
        db_port_ids = [port['id'] for port in db_ports['ports'] if
                       not utils.is_lsp_ignored(port)]
        db_port_ids_dhcp_valid = set(
            port['id'] for port in db_ports['ports']
            if not utils.is_network_device_port(port) and
            port['id'] not in self.lport_dhcp_ignored)

        _plugin_nb_ovn = self.mech_driver._nb_ovn
        plugin_lport_ids = [
            row.name for row in (
                _plugin_nb_ovn._tables['Logical_Switch_Port'].rows.values())
            if ovn_const.OVN_PORT_NAME_EXT_ID_KEY in row.external_ids]
        plugin_lport_ids_dhcpv4_enabled = [
            row.name for row in (
                _plugin_nb_ovn._tables['Logical_Switch_Port'].rows.values())
            if row.dhcpv4_options]
        plugin_lport_ids_dhcpv6_enabled = [
            row.name for row in (
                _plugin_nb_ovn._tables['Logical_Switch_Port'].rows.values())
            if row.dhcpv6_options]

        monitor_lport_ids = [
            row.name for row in (
                self.nb_api.tables['Logical_Switch_Port'].
                rows.values())
            if ovn_const.OVN_PORT_NAME_EXT_ID_KEY in row.external_ids]
        monitor_lport_ids_dhcpv4_enabled = [
            row.name for row in (
                _plugin_nb_ovn._tables['Logical_Switch_Port'].rows.values())
            if row.dhcpv4_options]
        monitor_lport_ids_dhcpv6_enabled = [
            row.name for row in (
                _plugin_nb_ovn._tables['Logical_Switch_Port'].rows.values())
            if row.dhcpv6_options]

        if should_match:
            self.assertItemsEqual(db_port_ids, plugin_lport_ids)
            self.assertItemsEqual(db_port_ids, monitor_lport_ids)

            expected_dhcpv4_options_ports_ids = (
                db_port_ids_dhcp_valid.difference(
                    set(self.lport_dhcpv4_disabled.keys())))
            self.assertItemsEqual(expected_dhcpv4_options_ports_ids,
                                  plugin_lport_ids_dhcpv4_enabled)
            self.assertItemsEqual(expected_dhcpv4_options_ports_ids,
                                  monitor_lport_ids_dhcpv4_enabled)

            expected_dhcpv6_options_ports_ids = (
                db_port_ids_dhcp_valid.difference(
                    set(self.lport_dhcpv6_disabled.keys())))
            self.assertItemsEqual(expected_dhcpv6_options_ports_ids,
                                  plugin_lport_ids_dhcpv6_enabled)
            self.assertItemsEqual(expected_dhcpv6_options_ports_ids,
                                  monitor_lport_ids_dhcpv6_enabled)
        else:
            self.assertRaises(
                AssertionError, self.assertItemsEqual, db_port_ids,
                plugin_lport_ids)

            self.assertRaises(
                AssertionError, self.assertItemsEqual, db_port_ids,
                monitor_lport_ids)

            self.assertRaises(
                AssertionError, self.assertItemsEqual, db_port_ids,
                plugin_lport_ids_dhcpv4_enabled)

            self.assertRaises(
                AssertionError, self.assertItemsEqual, db_port_ids,
                monitor_lport_ids_dhcpv4_enabled)

    @staticmethod
    def _build_acl_for_pgs(priority, direction, log, name, action,
                           severity, match, port_group, **kwargs):
        return {
            'priority': priority,
            'direction': direction,
            'log': log,
            'name': name,
            'action': action,
            'severity': severity,
            'match': match,
            'external_ids': kwargs,
            }

    def _validate_dhcp_opts(self, should_match=True):
        observed_plugin_dhcp_options_rows = []
        _plugin_nb_ovn = self.mech_driver._nb_ovn
        for row in _plugin_nb_ovn._tables['DHCP_Options'].rows.values():
            opts = dict(row.options)
            ids = dict(row.external_ids)
            if ids.get('subnet_id') not in self.match_old_mac_dhcp_subnets:
                if 'server_mac' in opts:
                    opts['server_mac'] = '01:02:03:04:05:06'
                else:
                    opts['server_id'] = '01:02:03:04:05:06'
            observed_plugin_dhcp_options_rows.append({
                'cidr': row.cidr, 'external_ids': row.external_ids,
                'options': opts})

        observed_monitor_dhcp_options_rows = []
        for row in self.nb_api.tables['DHCP_Options'].rows.values():
            opts = dict(row.options)
            ids = dict(row.external_ids)
            if ids.get('subnet_id') not in self.match_old_mac_dhcp_subnets:
                if 'server_mac' in opts:
                    opts['server_mac'] = '01:02:03:04:05:06'
                else:
                    opts['server_id'] = '01:02:03:04:05:06'
            observed_monitor_dhcp_options_rows.append({
                'cidr': row.cidr, 'external_ids': row.external_ids,
                'options': opts})

        if should_match:
            self.assertItemsEqual(self.expected_dhcp_options_rows,
                                  observed_plugin_dhcp_options_rows)
            self.assertItemsEqual(self.expected_dhcp_options_rows,
                                  observed_monitor_dhcp_options_rows)
        else:
            self.assertRaises(
                AssertionError, self.assertItemsEqual,
                self.expected_dhcp_options_rows,
                observed_plugin_dhcp_options_rows)

            self.assertRaises(
                AssertionError, self.assertItemsEqual,
                self.expected_dhcp_options_rows,
                observed_monitor_dhcp_options_rows)

    def _build_acl_to_compare(self, acl, extra_fields=None):
        acl_to_compare = {}
        for acl_key in getattr(acl, "_data", {}):
            try:
                acl_to_compare[acl_key] = getattr(acl, acl_key)
            except AttributeError:
                pass
        return acl_utils.filter_acl_dict(acl_to_compare, extra_fields)

    def _validate_acls(self, should_match=True):
        # Get the neutron DB ACLs.
        db_acls = []
        sg_cache = {}
        subnet_cache = {}

        _plugin_nb_ovn = self.mech_driver._nb_ovn
        if not _plugin_nb_ovn.is_port_groups_supported():
            for db_port in self._list('ports')['ports']:
                acls = acl_utils.add_acls(self.plugin,
                                          context.get_admin_context(),
                                          db_port,
                                          sg_cache,
                                          subnet_cache,
                                          self.mech_driver._nb_ovn)
                for acl in acls:
                    db_acls.append(acl_utils.filter_acl_dict(acl))
        else:
            # ACLs due to SGs and default drop port group
            for sg in self._list('security-groups')['security_groups']:
                for sgr in sg['security_group_rules']:
                    acl = acl_utils._add_sg_rule_acl_for_port_group(
                        utils.ovn_port_group_name(sg['id']), sgr,
                        self.mech_driver._nb_ovn)
                    db_acls.append(TestOvnNbSync._build_acl_for_pgs(**acl))

            for acl in acl_utils.add_acls_for_drop_port_group(
                    ovn_const.OVN_DROP_PORT_GROUP_NAME):
                db_acls.append(TestOvnNbSync._build_acl_for_pgs(**acl))

        # Get the list of ACLs stored in the OVN plugin IDL.
        plugin_acls = []
        for row in _plugin_nb_ovn._tables['Logical_Switch'].rows.values():
            for acl in getattr(row, 'acls', []):
                plugin_acls.append(self._build_acl_to_compare(acl))
        if self.nb_api.is_port_groups_supported():
            for row in _plugin_nb_ovn._tables['Port_Group'].rows.values():
                for acl in getattr(row, 'acls', []):
                    plugin_acls.append(
                        self._build_acl_to_compare(
                            acl, extra_fields=['external_ids']))

        # Get the list of ACLs stored in the OVN monitor IDL.
        monitor_acls = []
        for row in self.nb_api.tables['Logical_Switch'].rows.values():
            for acl in getattr(row, 'acls', []):
                monitor_acls.append(self._build_acl_to_compare(acl))
        if _plugin_nb_ovn.is_port_groups_supported():
            for row in self.nb_api.tables['Port_Group'].rows.values():
                for acl in getattr(row, 'acls', []):
                    monitor_acls.append(self._build_acl_to_compare(acl))

        if should_match:
            self.assertItemsEqual(db_acls, plugin_acls)
            self.assertItemsEqual(db_acls, monitor_acls)
        else:
            self.assertRaises(
                AssertionError, self.assertItemsEqual,
                db_acls, plugin_acls)
            self.assertRaises(
                AssertionError, self.assertItemsEqual,
                db_acls, monitor_acls)

    def _validate_routers_and_router_ports(self, should_match=True):
        db_routers = self._list('routers')
        db_router_ids = []
        db_routes = {}
        db_nats = {}
        for db_router in db_routers['routers']:
            db_router_ids.append(db_router['id'])
            db_routes[db_router['id']] = [db_route['destination'] +
                                          db_route['nexthop']
                                          for db_route in db_router['routes']]
            db_nats[db_router['id']] = []
            if db_router.get(l3.EXTERNAL_GW_INFO):
                gw_info = self.l3_plugin._ovn_client._get_gw_info(
                    self.context, db_router)
                # Add gateway default route and snats
                if gw_info.gateway_ip:
                    db_routes[db_router['id']].append('0.0.0.0/0' +
                                                      gw_info.gateway_ip)
                if gw_info.router_ip and utils.is_snat_enabled(db_router):
                    networks = self.l3_plugin._ovn_client.\
                        _get_v4_network_of_all_router_ports(self.context,
                                                            db_router['id'])
                    db_nats[db_router['id']].extend(
                        [gw_info.router_ip + network + 'snat'
                         for network in networks])
        fips = self._list('floatingips')
        fip_macs = {}
        if ovn_config.is_ovn_distributed_floating_ip():
            params = 'device_owner=%s' % constants.DEVICE_OWNER_FLOATINGIP
            fports = self._list('ports', query_params=params)['ports']
            fip_macs = {p['device_id']: p['mac_address'] for p in fports
                        if p['device_id']}
        for fip in fips['floatingips']:
            if fip['router_id']:
                mac_address = ''
                fip_port = ''
                if fip['id'] in fip_macs:
                    fip_port = fip['port_id']
                db_nats[fip['router_id']].append(
                    fip['floating_ip_address'] + fip['fixed_ip_address'] +
                    'dnat_and_snat' + mac_address + fip_port)

        _plugin_nb_ovn = self.mech_driver._nb_ovn
        plugin_lrouter_ids = [
            row.name.replace('neutron-', '') for row in (
                _plugin_nb_ovn._tables['Logical_Router'].rows.values())]

        monitor_lrouter_ids = [
            row.name.replace('neutron-', '') for row in (
                self.nb_api.tables['Logical_Router'].rows.values())]

        if should_match:
            self.assertItemsEqual(db_router_ids, plugin_lrouter_ids)
            self.assertItemsEqual(db_router_ids, monitor_lrouter_ids)
        else:
            self.assertRaises(
                AssertionError, self.assertItemsEqual, db_router_ids,
                plugin_lrouter_ids)

            self.assertRaises(
                AssertionError, self.assertItemsEqual, db_router_ids,
                monitor_lrouter_ids)

        def _get_networks_for_router_port(port_fixed_ips):
            _ovn_client = self.l3_plugin._ovn_client
            networks, _ = (
                _ovn_client._get_nets_and_ipv6_ra_confs_for_router_port(
                    port_fixed_ips))
            return networks

        def _get_ipv6_ra_configs_for_router_port(port_fixed_ips):
            _ovn_client = self.l3_plugin._ovn_client
            networks, ipv6_ra_configs = (
                _ovn_client._get_nets_and_ipv6_ra_confs_for_router_port(
                    port_fixed_ips))
            return ipv6_ra_configs

        for router_id in db_router_ids:
            r_ports = self._list('ports',
                                 query_params='device_id=%s' % (router_id))
            r_port_ids = [p['id'] for p in r_ports['ports']]
            r_port_networks = {
                p['id']:
                    _get_networks_for_router_port(p['fixed_ips'])
                    for p in r_ports['ports']}
            r_port_ipv6_ra_configs = {
                p['id']: _get_ipv6_ra_configs_for_router_port(p['fixed_ips'])
                for p in r_ports['ports']}
            r_routes = db_routes[router_id]
            r_nats = db_nats[router_id]

            try:
                lrouter = idlutils.row_by_value(
                    self.mech_driver._nb_ovn.idl, 'Logical_Router', 'name',
                    'neutron-' + str(router_id), None)
                lports = getattr(lrouter, 'ports', [])
                plugin_lrouter_port_ids = [lport.name.replace('lrp-', '')
                                           for lport in lports]
                plugin_lport_networks = {
                    lport.name.replace('lrp-', ''): lport.networks
                    for lport in lports}
                plugin_lport_ra_configs = {
                    lport.name.replace('lrp-', ''): lport.ipv6_ra_configs
                    for lport in lports}
                sroutes = getattr(lrouter, 'static_routes', [])
                plugin_routes = [sroute.ip_prefix + sroute.nexthop
                                 for sroute in sroutes]
                nats = getattr(lrouter, 'nat', [])
                plugin_nats = [
                    nat.external_ip + nat.logical_ip + nat.type +
                    (nat.external_mac[0] if nat.external_mac else '') +
                    (nat.logical_port[0] if nat.logical_port else '')
                    for nat in nats]
            except idlutils.RowNotFound:
                plugin_lrouter_port_ids = []
                plugin_routes = []
                plugin_nats = []

            try:
                lrouter = idlutils.row_by_value(
                    self.nb_api.idl, 'Logical_Router', 'name',
                    'neutron-' + router_id, None)
                lports = getattr(lrouter, 'ports', [])
                monitor_lrouter_port_ids = [lport.name.replace('lrp-', '')
                                            for lport in lports]
                monitor_lport_networks = {
                    lport.name.replace('lrp-', ''): lport.networks
                    for lport in lports}
                monitor_lport_ra_configs = {
                    lport.name.replace('lrp-', ''): lport.ipv6_ra_configs
                    for lport in lports}
                sroutes = getattr(lrouter, 'static_routes', [])
                monitor_routes = [sroute.ip_prefix + sroute.nexthop
                                  for sroute in sroutes]
                nats = getattr(lrouter, 'nat', [])
                monitor_nats = [
                    nat.external_ip + nat.logical_ip + nat.type +
                    (nat.external_mac[0] if nat.external_mac else '') +
                    (nat.logical_port[0] if nat.logical_port else '')
                    for nat in nats]
            except idlutils.RowNotFound:
                monitor_lrouter_port_ids = []
                monitor_routes = []
                monitor_nats = []

            if should_match:
                self.assertItemsEqual(r_port_ids, plugin_lrouter_port_ids)
                self.assertItemsEqual(r_port_ids, monitor_lrouter_port_ids)
                for p in plugin_lport_networks:
                    self.assertItemsEqual(r_port_networks[p],
                                          plugin_lport_networks[p])
                    self.assertItemsEqual(r_port_ipv6_ra_configs[p],
                                          plugin_lport_ra_configs[p])
                for p in monitor_lport_networks:
                    self.assertItemsEqual(r_port_networks[p],
                                          monitor_lport_networks[p])
                    self.assertItemsEqual(r_port_ipv6_ra_configs[p],
                                          monitor_lport_ra_configs[p])
                self.assertItemsEqual(r_routes, plugin_routes)
                self.assertItemsEqual(r_routes, monitor_routes)
                self.assertItemsEqual(r_nats, plugin_nats)
                self.assertItemsEqual(r_nats, monitor_nats)
            else:
                self.assertRaises(
                    AssertionError, self.assertItemsEqual, r_port_ids,
                    plugin_lrouter_port_ids)

                self.assertRaises(
                    AssertionError, self.assertItemsEqual, r_port_ids,
                    monitor_lrouter_port_ids)

                for _p in self.update_lrouter_ports:
                    p = _p[0].replace('lrp-', '')
                    if p in plugin_lport_networks:
                        self.assertRaises(
                            AssertionError, self.assertItemsEqual,
                            r_port_networks[p], plugin_lport_networks[p])
                        self.assertRaises(
                            AssertionError, self.assertItemsEqual,
                            r_port_ipv6_ra_configs[p],
                            plugin_lport_ra_configs[p])
                    if p in monitor_lport_networks:
                        self.assertRaises(
                            AssertionError, self.assertItemsEqual,
                            r_port_networks[p], monitor_lport_networks[p])
                        self.assertRaises(
                            AssertionError, self.assertItemsEqual,
                            r_port_ipv6_ra_configs[p],
                            monitor_lport_ra_configs[p])

                self.assertRaises(
                    AssertionError, self.assertItemsEqual, r_routes,
                    plugin_routes)

                self.assertRaises(
                    AssertionError, self.assertItemsEqual, r_routes,
                    monitor_routes)

                self.assertRaises(
                    AssertionError, self.assertItemsEqual, r_nats,
                    plugin_nats)

                self.assertRaises(
                    AssertionError, self.assertItemsEqual, r_nats,
                    monitor_nats)

    def _validate_address_sets(self, should_match=True):
        _plugin_nb_ovn = self.mech_driver._nb_ovn
        if _plugin_nb_ovn.is_port_groups_supported():
            # If Port Groups are supported, no Address Sets are expected.
            # This validation is still useful as we expect existing ones to
            # be deleted after the sync.
            db_sgs = []
        else:
            db_ports = self._list('ports')['ports']
            sgs = self._list('security-groups')['security_groups']
            db_sgs = {}
            for sg in sgs:
                for ip_version in ['ip4', 'ip6']:
                    name = utils.ovn_addrset_name(sg['id'], ip_version)
                    db_sgs[name] = []

            for port in db_ports:
                sg_ids = utils.get_lsp_security_groups(port)
                addresses = acl_utils.acl_port_ips(port)
                for sg_id in sg_ids:
                    for ip_version in addresses:
                        name = utils.ovn_addrset_name(sg_id, ip_version)
                        db_sgs[name].extend(addresses[ip_version])

        nb_address_sets = _plugin_nb_ovn.get_address_sets()
        nb_sgs = {}
        for nb_sgid, nb_values in nb_address_sets.items():
            nb_sgs[nb_sgid] = nb_values['addresses']

        mn_sgs = {}
        for row in self.nb_api.tables['Address_Set'].rows.values():
            mn_sgs[getattr(row, 'name')] = getattr(row, 'addresses')

        if should_match:
            self.assertItemsEqual(nb_sgs, db_sgs)
            self.assertItemsEqual(mn_sgs, db_sgs)
        else:
            # This condition is to cover the case when we use Port Groups
            # and we completely deleted the NB DB. At this point, the expected
            # number of Address Sets is 0 and the observed number in NB is
            # also 0 so we can't have the asserts below as both will be empty.
            if _plugin_nb_ovn.is_port_groups_supported() and nb_sgs:
                self.assertRaises(AssertionError, self.assertItemsEqual,
                                  nb_sgs, db_sgs)
                self.assertRaises(AssertionError, self.assertItemsEqual,
                                  mn_sgs, db_sgs)

    def _validate_port_groups(self, should_match=True):
        _plugin_nb_ovn = self.mech_driver._nb_ovn
        if not _plugin_nb_ovn.is_port_groups_supported():
            return

        db_pgs = []
        for sg in self._list('security-groups')['security_groups']:
            db_pgs.append(utils.ovn_port_group_name(sg['id']))
        db_pgs.append(ovn_const.OVN_DROP_PORT_GROUP_NAME)

        nb_pgs = _plugin_nb_ovn.get_port_groups()

        mn_pgs = []
        for row in self.nb_api.tables['Port_Group'].rows.values():
            mn_pgs.append(getattr(row, 'name', ''))

        if should_match:
            self.assertItemsEqual(nb_pgs, db_pgs)
            self.assertItemsEqual(mn_pgs, db_pgs)
        else:
            self.assertRaises(AssertionError, self.assertItemsEqual,
                              nb_pgs, db_pgs)
            self.assertRaises(AssertionError, self.assertItemsEqual,
                              mn_pgs, db_pgs)

    def _delete_metadata_ports(self):
        """Delete some metadata ports.

        This method will delete one half of the metadata ports from Neutron and
        the remaining ones only from OVN. This way we can exercise the metadata
        sync completely: ie., that metadata ports are recreated in Neutron when
        missing and that the corresponding OVN localports are also created.
        """
        db_ports = self._list('ports')
        db_metadata_ports = [port for port in db_ports['ports'] if
                             port['device_owner'] ==
                             constants.DEVICE_OWNER_DHCP]
        lswitches = {}
        ports_to_delete = len(db_metadata_ports) / 2
        for port in db_metadata_ports:
            lswitches[port['id']] = 'neutron-' + port['network_id']
            if ports_to_delete:
                self._delete('ports', port['id'])
                ports_to_delete -= 1

        _plugin_nb_ovn = self.mech_driver._nb_ovn
        plugin_metadata_ports = [row.name for row in (
            _plugin_nb_ovn._tables['Logical_Switch_Port'].rows.values())
            if row.type == ovn_const.OVN_NEUTRON_OWNER_TO_PORT_TYPE.get(
                constants.DEVICE_OWNER_DHCP)]

        with self.nb_api.transaction(check_error=True) as txn:
            for port in plugin_metadata_ports:
                txn.add(self.nb_api.delete_lswitch_port(port, lswitches[port],
                                                        True))

    def _validate_dns_records(self, should_match=True):
        observed_dns_records = []
        for dns_row in self.nb_api.tables['DNS'].rows.values():
            observed_dns_records.append(
                {'external_ids': dns_row.external_ids,
                 'records': dns_row.records})
        if should_match:
            self.assertItemsEqual(self.expected_dns_records,
                                  observed_dns_records)
        else:
            self.assertRaises(AssertionError, self.assertItemsEqual,
                              self.expected_dns_records, observed_dns_records)

    def _validate_resources(self, should_match=True):
        self._validate_networks(should_match=should_match)
        self._validate_metadata_ports(should_match=should_match)
        self._validate_ports(should_match=should_match)
        self._validate_dhcp_opts(should_match=should_match)
        self._validate_acls(should_match=should_match)
        self._validate_routers_and_router_ports(should_match=should_match)
        self._validate_address_sets(should_match=should_match)
        self._validate_port_groups(should_match=should_match)
        self._validate_dns_records(should_match=should_match)

    def _sync_resources(self, mode):
        nb_synchronizer = ovn_db_sync.OvnNbSynchronizer(
            self.plugin, self.mech_driver._nb_ovn, self.mech_driver._sb_ovn,
            mode, self.mech_driver)
        self.addCleanup(nb_synchronizer.stop)
        nb_synchronizer.do_sync()

    def _test_ovn_nb_sync_helper(self, mode, modify_resources=True,
                                 restart_ovsdb_processes=False,
                                 should_match_after_sync=True):
        self._create_resources(restart_ovsdb_processes)
        self._validate_resources(should_match=True)

        if modify_resources:
            self._modify_resources_in_nb_db()

        if restart_ovsdb_processes:
            # Restart the ovsdb-server and plugin idl.
            # This causes a new ovsdb-server to be started with empty
            # OVN NB DB
            self.restart()

        if modify_resources or restart_ovsdb_processes:
            self._validate_resources(should_match=False)

        self._sync_resources(mode)
        self._validate_resources(should_match=should_match_after_sync)

    def test_ovn_nb_sync_repair(self):
        self._test_ovn_nb_sync_helper('repair')

    def test_ovn_nb_sync_repair_delete_ovn_nb_db(self):
        # In this test case, the ovsdb-server for OVN NB DB is restarted
        # with empty OVN NB DB.
        self._test_ovn_nb_sync_helper('repair', modify_resources=False,
                                      restart_ovsdb_processes=True)

    def test_ovn_nb_sync_log(self):
        self._test_ovn_nb_sync_helper('log', should_match_after_sync=False)

    def test_ovn_nb_sync_off(self):
        self._test_ovn_nb_sync_helper('off', should_match_after_sync=False)


class TestOvnSbSync(base.TestOVNFunctionalBase):

    def setUp(self):
        super(TestOvnSbSync, self).setUp(ovn_worker=False)
        self.segments_plugin = directory.get_plugin('segments')
        self.sb_synchronizer = ovn_db_sync.OvnSbSynchronizer(
            self.plugin, self.mech_driver._sb_ovn, self.mech_driver)
        self.addCleanup(self.sb_synchronizer.stop)
        self.ctx = context.get_admin_context()

    def get_additional_service_plugins(self):
        p = super(TestOvnSbSync, self).get_additional_service_plugins()
        p.update({'segments': 'neutron.services.segments.plugin.Plugin'})
        return p

    def _sync_resources(self):
        self.sb_synchronizer.sync_hostname_and_physical_networks(self.ctx)

    def create_segment(self, network_id, physical_network, segmentation_id):
        segment_data = {'network_id': network_id,
                        'physical_network': physical_network,
                        'segmentation_id': segmentation_id,
                        'network_type': 'vlan',
                        'name': constants.ATTR_NOT_SPECIFIED,
                        'description': constants.ATTR_NOT_SPECIFIED}
        return self.segments_plugin.create_segment(
            self.ctx, segment={'segment': segment_data})

    def test_ovn_sb_sync_add_new_host(self):
        with self.network() as network:
            network_id = network['network']['id']
        self.create_segment(network_id, 'physnet1', 50)
        self.add_fake_chassis('host1', ['physnet1'])
        segment_hosts = segments_db.get_hosts_mapped_with_segments(self.ctx)
        self.assertFalse(segment_hosts)
        self._sync_resources()
        segment_hosts = segments_db.get_hosts_mapped_with_segments(self.ctx)
        self.assertEqual({'host1'}, segment_hosts)

    def test_ovn_sb_sync_update_existing_host(self):
        with self.network() as network:
            network_id = network['network']['id']
        segment = self.create_segment(network_id, 'physnet1', 50)
        segments_db.update_segment_host_mapping(
            self.ctx, 'host1', {segment['id']})
        segment_hosts = segments_db.get_hosts_mapped_with_segments(self.ctx)
        self.assertEqual({'host1'}, segment_hosts)
        self.add_fake_chassis('host1', ['physnet2'])
        self._sync_resources()
        segment_hosts = segments_db.get_hosts_mapped_with_segments(self.ctx)
        self.assertFalse(segment_hosts)

    def test_ovn_sb_sync_delete_stale_host(self):
        with self.network() as network:
            network_id = network['network']['id']
        segment = self.create_segment(network_id, 'physnet1', 50)
        segments_db.update_segment_host_mapping(
            self.ctx, 'host1', {segment['id']})
        segment_hosts = segments_db.get_hosts_mapped_with_segments(self.ctx)
        self.assertEqual({'host1'}, segment_hosts)
        # Since there is no chassis in the sb DB, host1 is the stale host
        # recorded in neutron DB. It should be deleted after sync.
        self._sync_resources()
        segment_hosts = segments_db.get_hosts_mapped_with_segments(self.ctx)
        self.assertFalse(segment_hosts)

    def test_ovn_sb_sync(self):
        with self.network() as network:
            network_id = network['network']['id']
        seg1 = self.create_segment(network_id, 'physnet1', 50)
        self.create_segment(network_id, 'physnet2', 51)
        segments_db.update_segment_host_mapping(
            self.ctx, 'host1', {seg1['id']})
        segments_db.update_segment_host_mapping(
            self.ctx, 'host2', {seg1['id']})
        segments_db.update_segment_host_mapping(
            self.ctx, 'host3', {seg1['id']})
        segment_hosts = segments_db.get_hosts_mapped_with_segments(self.ctx)
        self.assertEqual({'host1', 'host2', 'host3'}, segment_hosts)
        self.add_fake_chassis('host2', ['physnet2'])
        self.add_fake_chassis('host3', ['physnet3'])
        self.add_fake_chassis('host4', ['physnet1'])
        self._sync_resources()
        segment_hosts = segments_db.get_hosts_mapped_with_segments(self.ctx)
        # host1 should be cleared since it is not in the chassis DB. host3
        # should be cleared since there is no segment for mapping.
        self.assertEqual({'host2', 'host4'}, segment_hosts)


class TestOvnNbSyncOverTcp(TestOvnNbSync):
    def setUp(self):
        super(TestOvnNbSyncOverTcp, self).setUp()
        ovn_config.cfg.CONF.set_override(
            'enable_distributed_floating_ip', True, group='ovn')

    def get_ovsdb_server_protocol(self):
        return 'tcp'


class TestOvnSbSyncOverTcp(TestOvnSbSync):
    def get_ovsdb_server_protocol(self):
        return 'tcp'


class TestOvnNbSyncOverSsl(TestOvnNbSync):
    def get_ovsdb_server_protocol(self):
        return 'ssl'


class TestOvnSbSyncOverSsl(TestOvnSbSync):
    def get_ovsdb_server_protocol(self):
        return 'ssl'


@mock.patch('networking_ovn.ovsdb.impl_idl_ovn.OvsdbNbOvnIdl.'
            'is_port_groups_supported', lambda *args: False)
class TestOvnNbSyncNoPgs(TestOvnNbSync):
    pass




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\test_ovsdb_monitor.py
===========File Type===========
.py
===========File Content===========
# Copyright 2016 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import threading

import mock
from oslo_utils import uuidutils

from networking_ovn.ovsdb import ovsdb_monitor
from networking_ovn.tests.functional import base
from neutron.common import utils as n_utils
from neutron_lib.api.definitions import portbindings
from neutron_lib.plugins import constants as plugin_constants
from neutron_lib.plugins import directory
from ovsdbapp.backend.ovs_idl import event


class WaitForMACBindingDeleteEvent(event.RowEvent):
    # TODO(dalvarez): Use WaitEvent from ovsdbapp once this patch
    # https://review.openstack.org/#/c/613121 merges.
    event_name = 'WaitForMACBindingDeleteEvent'
    ONETIME = True

    def __init__(self, entry):
        self.event = threading.Event()
        self.timeout = 15
        table = 'MAC_Binding'
        events = (self.ROW_DELETE)
        conditions = (('_uuid', '=', entry),)
        super(WaitForMACBindingDeleteEvent, self).__init__(
            events, table, conditions)

    def run(self, event, row, old):
        self.event.set()

    def wait(self):
        return self.event.wait(self.timeout)


class TestNBDbMonitor(base.TestOVNFunctionalBase):

    def setUp(self):
        super(TestNBDbMonitor, self).setUp(ovn_worker=True)
        self.chassis = self.add_fake_chassis('ovs-host1')
        self.l3_plugin = directory.get_plugin(plugin_constants.L3)

    def create_port(self):
        net = self._make_network(self.fmt, 'net1', True)
        self._make_subnet(self.fmt, net, '20.0.0.1',
                          '20.0.0.0/24', ip_version=4)
        arg_list = ('device_owner', 'device_id', portbindings.HOST_ID)
        host_arg = {'device_owner': 'compute:nova',
                    'device_id': uuidutils.generate_uuid(),
                    portbindings.HOST_ID: 'ovs-host1'}
        port_res = self._create_port(self.fmt, net['network']['id'],
                                     arg_list=arg_list, **host_arg)
        port = self.deserialize(self.fmt, port_res)['port']
        return port

    def _create_fip(self, port, fip_address):
        e1 = self._make_network(self.fmt, 'e1', True,
                                arg_list=('router:external',
                                          'provider:network_type',
                                          'provider:physical_network'),
                                **{'router:external': True,
                                   'provider:network_type': 'flat',
                                   'provider:physical_network': 'public'})
        res = self._create_subnet(self.fmt, e1['network']['id'],
                                  '100.0.0.0/24', gateway_ip='100.0.0.254',
                                  allocation_pools=[{'start': '100.0.0.2',
                                                     'end': '100.0.0.253'}],
                                  enable_dhcp=False)
        e1_s1 = self.deserialize(self.fmt, res)
        r1 = self.l3_plugin.create_router(
            self.context,
            {'router': {
                'name': 'r1', 'admin_state_up': True,
                'tenant_id': self._tenant_id,
                'external_gateway_info': {
                    'enable_snat': True,
                    'network_id': e1['network']['id'],
                    'external_fixed_ips': [
                        {'ip_address': '100.0.0.2',
                         'subnet_id': e1_s1['subnet']['id']}]}}})
        self.l3_plugin.add_router_interface(
            self.context, r1['id'],
            {'subnet_id': port['fixed_ips'][0]['subnet_id']})
        r1_f2 = self.l3_plugin.create_floatingip(
            self.context, {'floatingip': {
                'tenant_id': self._tenant_id,
                'floating_network_id': e1['network']['id'],
                'subnet_id': None,
                'floating_ip_address': fip_address,
                'port_id': port['id']}})
        return r1_f2

    def test_floatingip_mac_bindings(self):
        """Check that MAC_Binding entries are cleared on FIP add/removal

        This test will:
        * Create a MAC_Binding entry for an IP address on the
        'network1' datapath.
        * Create a FIP with that same IP address on an external.
        network and associate it to a Neutron port on a private network.
        * Check that the MAC_Binding entry gets deleted.
        * Create a new MAC_Binding entry for the same IP address.
        * Delete the FIP.
        * Check that the MAC_Binding entry gets deleted.
        """
        self._make_network(self.fmt, 'network1', True)
        dp = self.sb_api.db_find(
            'Datapath_Binding',
            ('external_ids', '=', {'name2': 'network1'})).execute()
        macb_id = self.sb_api.db_create('MAC_Binding', datapath=dp[0]['_uuid'],
                                        ip='100.0.0.21').execute()
        port = self.create_port()

        # Ensure that the MAC_Binding entry gets deleted after creating a FIP
        row_event = WaitForMACBindingDeleteEvent(macb_id)
        self.mech_driver._sb_ovn.idl.notify_handler.watch_event(row_event)
        fip = self._create_fip(port, '100.0.0.21')
        self.assertTrue(row_event.wait())

        # Now that the FIP is created, add a new MAC_Binding entry with the
        # same IP address

        macb_id = self.sb_api.db_create('MAC_Binding', datapath=dp[0]['_uuid'],
                                        ip='100.0.0.21').execute()

        # Ensure that the MAC_Binding entry gets deleted after deleting the FIP
        row_event = WaitForMACBindingDeleteEvent(macb_id)
        self.mech_driver._sb_ovn.idl.notify_handler.watch_event(row_event)
        self.l3_plugin.delete_floatingip(self.context, fip['id'])
        self.assertTrue(row_event.wait())

    def _test_port_binding_and_status(self, port_id, action, status):
        # This function binds or unbinds port to chassis and
        # checks if port status matches with input status
        core_plugin = directory.get_plugin()
        self.sb_api.check_for_row_by_value_and_retry(
            'Port_Binding', 'logical_port', port_id)

        def check_port_status(status):
            port = core_plugin.get_ports(
                self.context, filters={'id': [port_id]})[0]
            return port['status'] == status
        if action == 'bind':
            self.sb_api.lsp_bind(port_id, self.chassis,
                                 may_exist=True).execute(check_error=True)
        else:
            self.sb_api.lsp_unbind(port_id).execute(check_error=True)
        n_utils.wait_until_true(lambda: check_port_status(status))

    def test_port_up_down_events(self):
        """Test the port up down events.

        This test case creates a port, binds the port to chassis,
        tests if the ovsdb monitor calls mech_driver to set port status
        to 'ACTIVE'. Then unbinds the port and checks if the port status
        is set to "DOWN'
        """
        port = self.create_port()
        self._test_port_binding_and_status(port['id'], 'bind', 'ACTIVE')
        self._test_port_binding_and_status(port['id'], 'unbind', 'DOWN')

    def test_ovsdb_monitor_lock(self):
        """Test case to test the ovsdb monitor lock used by OvnConnection.

        This test case created another IDL connection to the NB DB using
        the ovsdb_monitor.OvnConnection.

        With this we will have 2 'ovsdb_monitor.OvnConnection's. At the
        start the lock should be with the IDL connection created by the
        'TestOVNFunctionalBase' setup() function.

        The port up/down events should be handled by the first IDL connection.
        Then the first IDL connection will release the lock so that the 2nd IDL
        connection created in this test case gets the lock and it should
        handle the port up/down events. Later when 2nd IDL connection releases
        lock, first IDL connection will get the lock and handles the
        port up/down events.

        Please note that the "self.monitor_nb_idl_con" created by the base
        class is created using 'connection.Connection' and hence it will not
        contend for any lock.
        """
        fake_driver = mock.MagicMock()
        _idl = ovsdb_monitor.OvnNbIdl.from_server(
            self.ovsdb_server_mgr.get_ovsdb_connection_path(),
            'OVN_Northbound', fake_driver)
        tst_ovn_conn = self.useFixture(
            base.ConnectionFixture(idl=_idl, timeout=10)).connection
        tst_ovn_conn.start()

        port = self.create_port()

        # mech_driver will release the lock to fake test driver. During chassis
        # binding and unbinding, port status won't change(i.e will be DOWN)
        # as mech driver can't update it.
        self.mech_driver._nb_ovn.idl.set_lock(None)
        n_utils.wait_until_true(lambda: tst_ovn_conn.idl.has_lock)
        self.mech_driver._nb_ovn.idl.set_lock(
            self.mech_driver._nb_ovn.idl.event_lock_name)
        self._test_port_binding_and_status(port['id'], 'bind', 'DOWN')
        self._test_port_binding_and_status(port['id'], 'unbind', 'DOWN')

        # Fake driver will relase the lock to mech driver. Port status will be
        # updated to 'ACTIVE' for chassis binding and to 'DOWN' for chassis
        # unbinding.
        tst_ovn_conn.idl.set_lock(None)
        n_utils.wait_until_true(lambda: self.mech_driver._nb_ovn.idl.has_lock)
        self._test_port_binding_and_status(port['id'], 'bind', 'ACTIVE')
        self._test_port_binding_and_status(port['id'], 'unbind', 'DOWN')


class TestNBDbMonitorOverTcp(TestNBDbMonitor):
    def get_ovsdb_server_protocol(self):
        return 'tcp'


class TestNBDbMonitorOverSsl(TestNBDbMonitor):
    def get_ovsdb_server_protocol(self):
        return 'ssl'




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\test_qos_driver.py
===========File Type===========
.py
===========File Content===========
# Copyright 2017 DtDream Technology Co.,Ltd.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from networking_ovn.tests.functional import base
from neutron.extensions import qos as qos_ext
from neutron.tests.unit.api import test_extensions
from ovsdbapp.backend.ovs_idl import idlutils


class QoSTestExtensionManager(object):
    def get_resources(self):
        return qos_ext.Qos.get_resources()

    def get_actions(self):
        return []

    def get_request_extensions(self):
        return []


class TestOVNQosDriver(base.TestOVNFunctionalBase):
    _extension_drivers = ['qos']

    def setUp(self):
        super(TestOVNQosDriver, self).setUp()
        qos_mgr = QoSTestExtensionManager()
        self.resource_prefix_map = {'policies': '/qos'}
        self.qos_api = test_extensions.setup_extensions_middleware(qos_mgr)

    def get_additional_service_plugins(self):
        p = super(TestOVNQosDriver, self).get_additional_service_plugins()
        p.update({'qos_plugin_name': 'qos'})
        return p

    def _test_qos_policy_create(self):
        data = {'policy': {'name': 'test-policy',
                           'tenant_id': self._tenant_id}}
        policy_req = self.new_create_request('policies', data, self.fmt)
        policy_res = policy_req.get_response(self.qos_api)
        policy = self.deserialize(self.fmt, policy_res)['policy']
        return policy['id']

    def _test_qos_policy_rule_create(self, policy_id, max_burst, max_bw):
        data = {'bandwidth_limit_rule': {'max_burst_kbps': max_burst,
                                         'max_kbps': max_bw,
                                         'tenant_id': self._tenant_id}}
        policy_rule_req = self.new_create_request(
            'policies', data, self.fmt, policy_id, 'bandwidth_limit_rules')
        policy_rule_res = policy_rule_req.get_response(self.qos_api)
        policy_rule = self.deserialize(self.fmt,
                                       policy_rule_res)['bandwidth_limit_rule']
        return policy_rule['id']

    def _test_qos_policy_rule_update(
            self, policy_id, rule_id, max_burst, max_bw):
        data = {'bandwidth_limit_rule': {'max_burst_kbps': max_burst,
                                         'max_kbps': max_bw}}
        policy_rule_req = self.new_update_request(
            'policies', data, policy_id, self.fmt,
            subresource='bandwidth_limit_rules' + '/' + rule_id)
        policy_rule_req.get_response(self.qos_api)

    def _test_qos_policy_rule_delete(
            self, policy_id, rule_id):
        policy_rule_req = self.new_delete_request(
            'policies', policy_id, self.fmt,
            subresource='bandwidth_limit_rules', sub_id=rule_id)
        policy_rule_req.get_response(self.qos_api)

    def _test_port_create(self, network_id, policy_id=None):
        data = {'port': {'network_id': network_id,
                         'tenant_id': self._tenant_id,
                         'device_owner': 'compute:None',
                         'qos_policy_id': policy_id}}
        port_req = self.new_create_request('ports', data, self.fmt)
        port_res = port_req.get_response(self.api)
        p1 = self.deserialize(self.fmt, port_res)['port']
        return p1['id']

    def _test_port_update(self, port_id, policy_id):
        data = {'port': {'qos_policy_id': policy_id}}
        port_req = self.new_update_request('ports', data, port_id, self.fmt)
        port_req.get_response(self.api)

    def _verify_qos_option_row_for_port(self, port_id,
                                        expected_lsp_qos_options):
        lsp = idlutils.row_by_value(self.nb_api.idl,
                                    'Logical_Switch_Port', 'name', port_id,
                                    None)

        observed_lsp_qos_options = {}
        if lsp.options:
            if 'qos_burst' in lsp.options:
                observed_lsp_qos_options['qos_burst'] = lsp.options.get(
                    'qos_burst')
            if 'qos_max_rate' in lsp.options:
                observed_lsp_qos_options['qos_max_rate'] = lsp.options.get(
                    'qos_max_rate')

        self.assertEqual(expected_lsp_qos_options, observed_lsp_qos_options)

    def test_port_qos_options_add_and_remove(self):
        expected_burst = 100
        expected_max_rate = 1
        network_id = self._make_network(self.fmt, 'n1', True)['network']['id']
        self._create_subnet(self.fmt, network_id, '10.0.0.0/24')
        port_id = self._test_port_create(network_id)
        policy_id = self._test_qos_policy_create()
        self._test_qos_policy_rule_create(
            policy_id, expected_burst, expected_max_rate)

        # port add QoS policy
        self._test_port_update(port_id, policy_id)
        expected_options = {
            'qos_burst': str(expected_burst * 1000),
            'qos_max_rate': str(expected_max_rate * 1000),
        }
        self._verify_qos_option_row_for_port(port_id, expected_options)

        # port remove QoS policy
        self._test_port_update(port_id, None)
        self._verify_qos_option_row_for_port(port_id, {})

    def test_port_qos_options_with_rule(self):
        expected_burst = 100
        expected_max_rate = 1
        network_id = self._make_network(self.fmt, 'n1', True)['network']['id']
        self._create_subnet(self.fmt, network_id, '10.0.0.0/24')
        policy_id = self._test_qos_policy_create()
        policy_rule_id = self._test_qos_policy_rule_create(
            policy_id, expected_burst, expected_max_rate)
        port_id = self._test_port_create(network_id, policy_id)

        # check qos options
        expected_options = {
            'qos_burst': str(expected_burst * 1000),
            'qos_max_rate': str(expected_max_rate * 1000),
        }
        self._verify_qos_option_row_for_port(port_id, expected_options)

        # update qos rule
        self._test_qos_policy_rule_update(
            policy_id, policy_rule_id,
            expected_burst * 2, expected_max_rate * 2)
        expected_options = {
            'qos_burst': str(expected_burst * 2 * 1000),
            'qos_max_rate': str(expected_max_rate * 2 * 1000),
        }
        self._verify_qos_option_row_for_port(port_id, expected_options)

        # delete qos rule
        self._test_qos_policy_rule_delete(policy_id, policy_rule_id)
        self._verify_qos_option_row_for_port(port_id, {})




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\test_revision_numbers.py
===========File Type===========
.py
===========File Content===========
# Copyright 2017 Red Hat, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from networking_ovn.common import constants as ovn_const
from networking_ovn.tests.functional import base


class TestRevisionNumbers(base.TestOVNFunctionalBase):

    def _create_network(self, name):
        data = {'network': {'name': name, 'tenant_id': self._tenant_id}}
        req = self.new_create_request('networks', data, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['network']

    def _update_network_name(self, net_id, new_name):
        data = {'network': {'name': new_name}}
        req = self.new_update_request('networks', data, net_id, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['network']

    def _find_network_row_by_name(self, name):
        for row in self.nb_api._tables['Logical_Switch'].rows.values():
            if (row.external_ids.get(
                    ovn_const.OVN_NETWORK_NAME_EXT_ID_KEY) == name):
                return row

    def _create_port(self, name, net_id):
        data = {'port': {'name': name,
                         'tenant_id': self._tenant_id,
                         'network_id': net_id}}
        req = self.new_create_request('ports', data, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['port']

    def _update_port_name(self, port_id, new_name):
        data = {'port': {'name': new_name}}
        req = self.new_update_request('ports', data, port_id, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['port']

    def _find_port_row_by_name(self, name):
        for row in self.nb_api._tables['Logical_Switch_Port'].rows.values():
            if (row.external_ids.get(
                    ovn_const.OVN_PORT_NAME_EXT_ID_KEY) == name):
                return row

    def _create_router(self, name):
        data = {'router': {'name': name, 'tenant_id': self._tenant_id}}
        req = self.new_create_request('routers', data, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['router']

    def _update_router_name(self, net_id, new_name):
        data = {'router': {'name': new_name}}
        req = self.new_update_request('routers', data, net_id, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['router']

    def _find_router_row_by_name(self, name):
        for row in self.nb_api._tables['Logical_Router'].rows.values():
            if (row.external_ids.get(
                    ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY) == name):
                return row

    def _create_subnet(self, net_id, cidr, name='subnet1'):
        data = {'subnet': {'name': name,
                           'tenant_id': self._tenant_id,
                           'network_id': net_id,
                           'cidr': cidr,
                           'ip_version': 4,
                           'enable_dhcp': True}}
        req = self.new_create_request('subnets', data, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['subnet']

    def _update_subnet_name(self, subnet_id, new_name):
        data = {'subnet': {'name': new_name}}
        req = self.new_update_request('subnets', data, subnet_id, self.fmt)
        res = req.get_response(self.api)
        return self.deserialize(self.fmt, res)['subnet']

    def _find_subnet_row_by_id(self, subnet_id):
        for row in self.nb_api._tables['DHCP_Options'].rows.values():
            if (row.external_ids.get('subnet_id') == subnet_id and
               not row.external_ids.get('port_id')):
                return row

    def test_create_network(self):
        name = 'net1'
        neutron_net = self._create_network(name)
        ovn_net = self._find_network_row_by_name(name)

        ovn_revision = ovn_net.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]
        # Assert it matches with the newest returned by neutron API
        self.assertEqual(str(neutron_net['revision_number']), ovn_revision)

    def test_update_network(self):
        new_name = 'netnew1'
        neutron_net = self._create_network('net1')
        updated_net = self._update_network_name(neutron_net['id'], new_name)
        ovn_net = self._find_network_row_by_name(new_name)

        ovn_revision = ovn_net.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]
        # Assert it matches with the newest returned by neutron API
        self.assertEqual(str(updated_net['revision_number']), ovn_revision)

    def test_create_port(self):
        name = 'port1'
        neutron_net = self._create_network('net1')
        neutron_port = self._create_port(name, neutron_net['id'])
        ovn_port = self._find_port_row_by_name(name)

        ovn_revision = ovn_port.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]
        # Assert it matches with the newest returned by neutron API
        self.assertEqual(str(neutron_port['revision_number']), ovn_revision)

    def test_update_port(self):
        new_name = 'portnew1'
        neutron_net = self._create_network('net1')
        neutron_port = self._create_port('port1', neutron_net['id'])
        updated_port = self._update_port_name(neutron_port['id'], new_name)
        ovn_port = self._find_port_row_by_name(new_name)

        ovn_revision = ovn_port.external_ids[ovn_const.OVN_REV_NUM_EXT_ID_KEY]
        # Assert it matches with the newest returned by neutron API
        self.assertEqual(str(updated_port['revision_number']), ovn_revision)

    def test_create_router(self):
        name = 'router1'
        neutron_router = self._create_router(name)
        ovn_router = self._find_router_row_by_name(name)

        ovn_revision = ovn_router.external_ids[
            ovn_const.OVN_REV_NUM_EXT_ID_KEY]
        # Assert it matches with the newest returned by neutron API
        self.assertEqual(str(neutron_router['revision_number']), ovn_revision)

    def test_update_router(self):
        new_name = 'newrouter'
        neutron_router = self._create_router('router1')
        updated_router = self._update_router_name(neutron_router['id'],
                                                  new_name)
        ovn_router = self._find_router_row_by_name(new_name)

        ovn_revision = ovn_router.external_ids[
            ovn_const.OVN_REV_NUM_EXT_ID_KEY]
        # Assert it matches with the newest returned by neutron API
        self.assertEqual(str(updated_router['revision_number']), ovn_revision)

    def test_create_subnet(self):
        neutron_net = self._create_network('net1')
        neutron_subnet = self._create_subnet(neutron_net['id'], '10.0.0.0/24')
        ovn_subnet = self._find_subnet_row_by_id(neutron_subnet['id'])

        ovn_revision = ovn_subnet.external_ids[
            ovn_const.OVN_REV_NUM_EXT_ID_KEY]
        # Assert it matches with the newest returned by neutron API
        self.assertEqual(str(neutron_subnet['revision_number']), ovn_revision)

    def test_update_subnet(self):
        neutron_net = self._create_network('net1')
        neutron_subnet = self._create_subnet(neutron_net['id'], '10.0.0.0/24')
        updated_subnet = self._update_subnet_name(
            neutron_subnet['id'], 'newsubnet')
        ovn_subnet = self._find_subnet_row_by_id(neutron_subnet['id'])

        ovn_revision = ovn_subnet.external_ids[
            ovn_const.OVN_REV_NUM_EXT_ID_KEY]
        # Assert it matches with the newest returned by neutron API
        self.assertEqual(str(updated_subnet['revision_number']), ovn_revision)

    # TODO(lucasagomes): Add a test for floating IPs here when we get
    # the router stuff done.




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\test_router.py
===========File Type===========
.py
===========File Content===========
# Copyright 2016 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mock

from networking_ovn.common import constants as ovn_const
from networking_ovn.common import utils as ovn_utils
from networking_ovn.l3 import l3_ovn_scheduler as l3_sched
from networking_ovn.tests.functional import base

from neutron.common import utils as n_utils
from neutron_lib.api.definitions import external_net
from neutron_lib.api.definitions import l3 as l3_apidef
from neutron_lib.api.definitions import portbindings
from neutron_lib.api.definitions import provider_net as pnet
from neutron_lib import constants as n_consts
from neutron_lib.plugins import directory
from ovsdbapp.backend.ovs_idl import idlutils


class TestRouter(base.TestOVNFunctionalBase):
    def setUp(self):
        super(TestRouter, self).setUp(ovn_worker=True)
        self.chassis1 = self.add_fake_chassis(
            'ovs-host1', physical_nets=['physnet1', 'physnet3'])
        self.chassis2 = self.add_fake_chassis(
            'ovs-host2', physical_nets=['physnet2', 'physnet3'])

    def _create_router(self, name, gw_info=None):
        router = {'router':
                  {'name': name,
                   'admin_state_up': True,
                   'tenant_id': self._tenant_id}}
        if gw_info:
            router['router']['external_gateway_info'] = gw_info
        return self.l3_plugin.create_router(self.context, router)

    def _create_ext_network(self, name, net_type, physnet, seg,
                            gateway, cidr):
        arg_list = (pnet.NETWORK_TYPE, external_net.EXTERNAL,)
        net_arg = {pnet.NETWORK_TYPE: net_type,
                   external_net.EXTERNAL: True}
        if seg:
            arg_list = arg_list + (pnet.SEGMENTATION_ID,)
            net_arg[pnet.SEGMENTATION_ID] = seg
        if physnet:
            arg_list = arg_list + (pnet.PHYSICAL_NETWORK,)
            net_arg[pnet.PHYSICAL_NETWORK] = physnet
        network = self._make_network(self.fmt, name, True,
                                     arg_list=arg_list, **net_arg)
        self._make_subnet(self.fmt, network, gateway, cidr, ip_version=4)
        return network

    def _set_redirect_chassis_to_invalid_chassis(self, ovn_client):
        with ovn_client._nb_idl.transaction(check_error=True) as txn:
            for lrp in self.nb_api.tables[
                    'Logical_Router_Port'].rows.values():
                txn.add(ovn_client._nb_idl.update_lrouter_port(
                    lrp.name,
                    gateway_chassis=[ovn_const.OVN_GATEWAY_INVALID_CHASSIS]))

    def test_gateway_chassis_on_router_gateway_port(self):
        ext2 = self._create_ext_network(
            'ext2', 'flat', 'physnet3', None, "20.0.0.1", "20.0.0.0/24")
        gw_info = {'network_id': ext2['network']['id']}
        self._create_router('router1', gw_info=gw_info)
        expected = [row.name for row in
                    self.sb_api.tables['Chassis'].rows.values()]
        for row in self.nb_api.tables[
                'Logical_Router_Port'].rows.values():
            if self._l3_ha_supported():
                chassis = [gwc.chassis_name for gwc in row.gateway_chassis]
                self.assertItemsEqual(expected, chassis)
            else:
                rc = row.options.get(ovn_const.OVN_GATEWAY_CHASSIS_KEY)
                self.assertIn(rc, expected)

    def _check_gateway_chassis_candidates(self, candidates):
        # In this test, fake_select() is called once from _create_router()
        # and later from schedule_unhosted_gateways()
        ovn_client = self.l3_plugin._ovn_client
        ext1 = self._create_ext_network(
            'ext1', 'vlan', 'physnet1', 1, "10.0.0.1", "10.0.0.0/24")
        # mock select function and check if it is called with expected
        # candidates.

        def fake_select(*args, **kwargs):
            self.assertItemsEqual(candidates, kwargs['candidates'])
            # We are not interested in further processing, let us return
            # INVALID_CHASSIS to avoid erros
            return [ovn_const.OVN_GATEWAY_INVALID_CHASSIS]

        with mock.patch.object(ovn_client._ovn_scheduler, 'select',
                               side_effect=fake_select) as client_select,\
            mock.patch.object(self.l3_plugin.scheduler, 'select',
                              side_effect=fake_select) as plugin_select:
            gw_info = {'network_id': ext1['network']['id']}
            self._create_router('router1', gw_info=gw_info)
            self.assertFalse(plugin_select.called)
            self.assertTrue(client_select.called)
            client_select.reset_mock()
            plugin_select.reset_mock()

            # set redirect-chassis to neutron-ovn-invalid-chassis, so
            # that schedule_unhosted_gateways will try to schedule it
            self._set_redirect_chassis_to_invalid_chassis(ovn_client)
            self.l3_plugin.schedule_unhosted_gateways()
            self.assertFalse(client_select.called)
            self.assertTrue(plugin_select.called)

    def test_gateway_chassis_with_cms_and_bridge_mappings(self):
        # Both chassis1 and chassis3 are having proper bridge mappings,
        # but only chassis3 is having enable-chassis-as-gw.
        # Test if chassis3 is selected as candidate or not.
        self.chassis3 = self.add_fake_chassis(
            'ovs-host3', physical_nets=['physnet1'],
            external_ids={'ovn-cms-options': 'enable-chassis-as-gw'})
        self._check_gateway_chassis_candidates([self.chassis3])

    def test_gateway_chassis_with_cms_and_no_bridge_mappings(self):
        # chassis1 is having proper bridge mappings.
        # chassis3 is having enable-chassis-as-gw, but no bridge mappings.
        # Test if chassis1 is selected as candidate or not.
        self.chassis3 = self.add_fake_chassis(
            'ovs-host3',
            external_ids={'ovn-cms-options': 'enable-chassis-as-gw'})
        self._check_gateway_chassis_candidates([self.chassis1])

    def test_gateway_chassis_with_bridge_mappings_and_no_cms(self):
        # chassis1 is configured with proper bridge mappings,
        # but none of the chassis having enable-chassis-as-gw.
        # Test if chassis1 is selected as candidate or not.
        self._check_gateway_chassis_candidates([self.chassis1])

    def _l3_ha_supported(self):
        # If the Gateway_Chassis table exists in SB database, then it
        # means that L3 HA is supported.
        return self.nb_api.tables.get('Gateway_Chassis')

    def test_gateway_chassis_least_loaded_scheduler(self):
        # This test will create 4 routers each with its own gateway.
        # Using the least loaded policy for scheduling gateway ports, we
        # expect that they are equally distributed across the two available
        # chassis.
        ovn_client = self.l3_plugin._ovn_client
        ovn_client._ovn_scheduler = l3_sched.OVNGatewayLeastLoadedScheduler()
        ext1 = self._create_ext_network(
            'ext1', 'flat', 'physnet3', None, "20.0.0.1", "20.0.0.0/24")
        gw_info = {'network_id': ext1['network']['id']}

        # Create 4 routers with a gateway. Since we're using physnet3, the
        # chassis candidates will be chassis1 and chassis2.
        for i in range(1, 5):
            self._create_router('router%d' % i, gw_info=gw_info)

        # At this point we expect two gateways to be present in chassis1
        # and two in chassis2. If schema supports L3 HA, we expect each
        # chassis to host 2 priority 2 gateways and 2 priority 1 ones.
        if self._l3_ha_supported():
            # Each chassis contains a dict of (priority, # of ports hosted).
            # {1: 2, 2: 2} means that this chassis hosts 2 ports of prio 1
            # and two ports of prio 2.
            expected = {self.chassis1: {1: 2, 2: 2},
                        self.chassis2: {1: 2, 2: 2}}
        else:
            # For non L3 HA, each chassis should contain two gateway ports.
            expected = {self.chassis1: 2,
                        self.chassis2: 2}
        sched_info = {}
        for row in self.nb_api.tables[
                'Logical_Router_Port'].rows.values():
            if self._l3_ha_supported():
                for gwc in row.gateway_chassis:
                    chassis = sched_info.setdefault(gwc.chassis_name, {})
                    chassis[gwc.priority] = chassis.get(gwc.priority, 0) + 1
            else:
                rc = row.options.get(ovn_const.OVN_GATEWAY_CHASSIS_KEY)
                sched_info[rc] = sched_info.get(rc, 0) + 1
        self.assertEqual(expected, sched_info)

    def test_gateway_chassis_with_bridge_mappings(self):
        ovn_client = self.l3_plugin._ovn_client
        # Create external networks with vlan, flat and geneve network types
        ext1 = self._create_ext_network(
            'ext1', 'vlan', 'physnet1', 1, "10.0.0.1", "10.0.0.0/24")
        ext2 = self._create_ext_network(
            'ext2', 'flat', 'physnet3', None, "20.0.0.1", "20.0.0.0/24")
        ext3 = self._create_ext_network(
            'ext3', 'geneve', None, 10, "30.0.0.1", "30.0.0.0/24")
        # mock select function and check if it is called with expected
        # candidates.
        self.candidates = []

        def fake_select(*args, **kwargs):
            self.assertItemsEqual(self.candidates, kwargs['candidates'])
            # We are not interested in further processing, let us return
            # INVALID_CHASSIS to avoid erros
            return [ovn_const.OVN_GATEWAY_INVALID_CHASSIS]

        with mock.patch.object(ovn_client._ovn_scheduler, 'select',
                               side_effect=fake_select) as client_select,\
            mock.patch.object(self.l3_plugin.scheduler, 'select',
                              side_effect=fake_select) as plugin_select:
            self.candidates = [self.chassis1]
            gw_info = {'network_id': ext1['network']['id']}
            router1 = self._create_router('router1', gw_info=gw_info)

            # set redirect-chassis to neutron-ovn-invalid-chassis, so
            # that schedule_unhosted_gateways will try to schedule it
            self._set_redirect_chassis_to_invalid_chassis(ovn_client)
            self.l3_plugin.schedule_unhosted_gateways()

            self.candidates = [self.chassis1, self.chassis2]
            gw_info = {'network_id': ext2['network']['id']}
            self.l3_plugin.update_router(
                self.context, router1['id'],
                {'router': {l3_apidef.EXTERNAL_GW_INFO: gw_info}})
            self._set_redirect_chassis_to_invalid_chassis(ovn_client)
            self.l3_plugin.schedule_unhosted_gateways()

            self.candidates = []
            gw_info = {'network_id': ext3['network']['id']}
            self.l3_plugin.update_router(
                self.context, router1['id'],
                {'router': {l3_apidef.EXTERNAL_GW_INFO: gw_info}})
            self._set_redirect_chassis_to_invalid_chassis(ovn_client)
            self.l3_plugin.schedule_unhosted_gateways()

            # We can't test call_count for these mocks, as we have enabled
            # ovn_worker which will trigger chassis events and eventually
            # calling schedule_unhosted_gateways
            self.assertTrue(client_select.called)
            self.assertTrue(plugin_select.called)

    def test_router_gateway_port_binding_host_id(self):
        # Test setting chassis on chassisredirect port in Port_Binding table,
        # will update host_id of corresponding router gateway port
        # with this chassis.
        chassis = idlutils.row_by_value(self.sb_api.idl, 'Chassis',
                                        'name', self.chassis1)
        host_id = chassis.hostname
        ext = self._create_ext_network(
            'ext1', 'vlan', 'physnet1', 1, "10.0.0.1", "10.0.0.0/24")
        gw_info = {'network_id': ext['network']['id']}
        router = self._create_router('router1', gw_info=gw_info)
        core_plugin = directory.get_plugin()
        gw_port_id = router.get('gw_port_id')

        # Set chassis on chassisredirect port in Port_Binding table
        logical_port = 'cr-lrp-%s' % gw_port_id
        self.sb_api.lsp_bind(logical_port, self.chassis1,
                             may_exist=True).execute(check_error=True)

        def check_port_binding_host_id(port_id):
            port = core_plugin.get_ports(
                self.context, filters={'id': [port_id]})[0]
            return port[portbindings.HOST_ID] == host_id

        # Test if router gateway port updated with this chassis
        n_utils.wait_until_true(lambda: check_port_binding_host_id(
            gw_port_id))

    def _validate_router_ipv6_ra_configs(self, lrp_name, expected_ra_confs):
        lrp = idlutils.row_by_value(self.nb_api.idl,
                                    'Logical_Router_Port', 'name', lrp_name)
        self.assertEqual(expected_ra_confs, lrp.ipv6_ra_configs)

    def _test_router_port_ipv6_ra_configs_helper(
            self, cidr='aef0::/64', ip_version=6,
            address_mode=n_consts.IPV6_SLAAC,):
        router1 = self._create_router('router1')
        n1 = self._make_network(self.fmt, 'n1', True)
        if ip_version == 6:
            kwargs = {'ip_version': 6, 'cidr': 'aef0::/64',
                      'ipv6_address_mode': address_mode,
                      'ipv6_ra_mode': address_mode}
        else:
            kwargs = {'ip_version': 4, 'cidr': '10.0.0.0/24'}

        res = self._create_subnet(self.fmt, n1['network']['id'],
                                  **kwargs)

        n1_s1 = self.deserialize(self.fmt, res)
        n1_s1_id = n1_s1['subnet']['id']
        router_iface_info = self.l3_plugin.add_router_interface(
            self.context, router1['id'], {'subnet_id': n1_s1_id})

        lrp_name = ovn_utils.ovn_lrouter_port_name(
            router_iface_info['port_id'])
        if ip_version == 6:
            expected_ra_configs = {
                'address_mode': ovn_utils.get_ovn_ipv6_address_mode(
                    address_mode),
                'send_periodic': 'true',
                'mtu': '1450'}
        else:
            expected_ra_configs = {}
        self._validate_router_ipv6_ra_configs(lrp_name, expected_ra_configs)

    def test_router_port_ipv6_ra_configs_addr_mode_slaac(self):
        self._test_router_port_ipv6_ra_configs_helper()

    def test_router_port_ipv6_ra_configs_addr_mode_dhcpv6_stateful(self):
        self._test_router_port_ipv6_ra_configs_helper(
            address_mode=n_consts.DHCPV6_STATEFUL)

    def test_router_port_ipv6_ra_configs_addr_mode_dhcpv6_stateless(self):
        self._test_router_port_ipv6_ra_configs_helper(
            address_mode=n_consts.DHCPV6_STATELESS)

    def test_router_port_ipv6_ra_configs_ipv4(self):
        self._test_router_port_ipv6_ra_configs_helper(
            ip_version=4)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\test_trunk_driver.py
===========File Type===========
.py
===========File Content===========
# Copyright 2017 DT Dream Technology Co.,Ltd.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import contextlib

from networking_ovn.tests.functional import base
from neutron.services.trunk import constants as trunk_consts
from neutron.services.trunk import plugin as trunk_plugin
from neutron_lib import constants as n_consts
from neutron_lib.objects import registry as obj_reg
from neutron_lib.plugins import utils
from oslo_utils import uuidutils


class TestOVNTrunkDriver(base.TestOVNFunctionalBase):

    def setUp(self):
        super(TestOVNTrunkDriver, self).setUp()
        self.trunk_plugin = trunk_plugin.TrunkPlugin()
        self.trunk_plugin.add_segmentation_type(trunk_consts.VLAN,
                                                utils.is_valid_vlan_tag)

    @contextlib.contextmanager
    def trunk(self, sub_ports=None):
        sub_ports = sub_ports or []
        with self.network() as network:
            with self.subnet(network=network) as subnet:
                with self.port(subnet=subnet) as parent_port:
                    tenant_id = uuidutils.generate_uuid()
                    trunk = {'trunk': {
                        'port_id': parent_port['port']['id'],
                        'tenant_id': tenant_id, 'project_id': tenant_id,
                        'admin_state_up': True,
                        'name': 'trunk', 'sub_ports': sub_ports}}
                    trunk = self.trunk_plugin.create_trunk(self.context, trunk)
                    yield trunk

    @contextlib.contextmanager
    def subport(self):
        with self.port() as port:
            sub_port = {'segmentation_type': 'vlan',
                        'segmentation_id': 1000,
                        'port_id': port['port']['id']}
            yield sub_port

    def _get_ovn_trunk_info(self):
        ovn_trunk_info = []
        for row in self.nb_api.tables[
                'Logical_Switch_Port'].rows.values():
            if row.parent_name and row.tag:
                ovn_trunk_info.append({'port_id': row.name,
                                       'parent_port_id': row.parent_name,
                                       'tag': row.tag})
        return ovn_trunk_info

    def _verify_trunk_info(self, trunk, has_items):
        ovn_subports_info = self._get_ovn_trunk_info()
        neutron_subports_info = []
        for subport in trunk.get('sub_ports', []):
            neutron_subports_info.append({'port_id': subport['port_id'],
                                          'parent_port_id': [trunk['port_id']],
                                          'tag': [subport['segmentation_id']]})
            # Check that the subport has the binding is active.
            binding = obj_reg.load_class('PortBinding').get_object(
                self.context, port_id=subport['port_id'], host='')
            self.assertEqual(n_consts.PORT_STATUS_ACTIVE, binding['status'])

        self.assertItemsEqual(ovn_subports_info, neutron_subports_info)
        self.assertEqual(has_items, len(neutron_subports_info) != 0)

        if trunk.get('status'):
            self.assertEqual(trunk_consts.ACTIVE_STATUS, trunk['status'])

    def test_trunk_create(self):
        with self.trunk() as trunk:
            self._verify_trunk_info(trunk, has_items=False)

    def test_trunk_create_with_subports(self):
        with self.subport() as subport:
            with self.trunk([subport]) as trunk:
                self._verify_trunk_info(trunk, has_items=True)

    def test_subport_add(self):
        with self.subport() as subport:
            with self.trunk() as trunk:
                self.trunk_plugin.add_subports(self.context, trunk['id'],
                                               {'sub_ports': [subport]})
                new_trunk = self.trunk_plugin.get_trunk(self.context,
                                                        trunk['id'])
                self._verify_trunk_info(new_trunk, has_items=True)

    def test_subport_delete(self):
        with self.subport() as subport:
            with self.trunk([subport]) as trunk:
                self.trunk_plugin.remove_subports(self.context, trunk['id'],
                                                  {'sub_ports': [subport]})
                new_trunk = self.trunk_plugin.get_trunk(self.context,
                                                        trunk['id'])
                self._verify_trunk_info(new_trunk, has_items=False)

    def test_trunk_delete(self):
        with self.trunk() as trunk:
            self.trunk_plugin.delete_trunk(self.context, trunk['id'])
            self._verify_trunk_info({}, has_items=False)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\db\test_migrations.py
===========File Type===========
.py
===========File Content===========
# Copyright 2017 Red Hat, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from oslo_config import cfg

from neutron.db.migration.alembic_migrations import external
from neutron.db.migration import cli as migration
from neutron.tests.functional.db import test_migrations
from neutron.tests.unit import testlib_api

from networking_ovn.db import head


# EXTERNAL_TABLES should contain all names of tables that are not related to
# current repo.

EXTERNAL_TABLES = external.TABLES

VERSION_TABLE = 'ovn_alembic_version'


class _TestModelsMigrationsOVN(test_migrations._TestModelsMigrations):
    def db_sync(self, engine):
        cfg.CONF.set_override('connection', engine.url, group='database')
        for conf in migration.get_alembic_configs():
            self.alembic_config = conf
            self.alembic_config.neutron_config = cfg.CONF
            migration.do_alembic_command(conf, 'upgrade', 'heads')

    def get_metadata(self):
        return head.get_metadata()

    def include_object(self, object_, name, type_, reflected, compare_to):
        if type_ == 'table' and (name.startswith('alembic') or
                                 name == VERSION_TABLE or
                                 name in EXTERNAL_TABLES):
            return False
        if type_ == 'index' and reflected and name.startswith("idx_autoinc_"):
            return False
        return True


class TestModelsMigrationsMysql(testlib_api.MySQLTestCaseMixin,
                                _TestModelsMigrationsOVN,
                                testlib_api.SqlTestCaseLight):
    pass


class TestModelsMigrationsPostgresql(testlib_api.PostgreSQLTestCaseMixin,
                                     _TestModelsMigrationsOVN,
                                     testlib_api.SqlTestCaseLight):
    pass




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\db\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\octavia\test_ovn_driver.py
===========File Type===========
.py
===========File Content===========
# Copyright 2018 Red Hat, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mock

from neutron.common import utils as n_utils
from octavia.api.drivers import data_models as octavia_data_model
from octavia.api.drivers import exceptions as o_exceptions
from octavia.common import constants as o_constants
from octavia.common import utils as o_utils
from oslo_serialization import jsonutils
from oslo_utils import uuidutils

from networking_ovn.octavia import ovn_driver
from networking_ovn.tests.functional import base


class TestOctaviaOvnProviderDriver(base.TestOVNFunctionalBase):

    def setUp(self):
        super(TestOctaviaOvnProviderDriver, self).setUp()
        # ovn_driver.OvnProviderHelper.ovn_nbdb_api is a class variable.
        # Set it to None, so that when a worker starts the 2nd test we don't
        # use the old object.
        ovn_driver.OvnProviderHelper.ovn_nbdb_api = None
        self.ovn_driver = ovn_driver.OvnProviderDriver()
        self.ovn_driver._ovn_helper._octavia_driver_lib = mock.MagicMock()
        self._o_driver_lib = self.ovn_driver._ovn_helper._octavia_driver_lib
        self._o_driver_lib.update_loadbalancer_status = mock.Mock()
        self.fake_network_driver = mock.MagicMock()
        o_utils.get_network_driver = mock.MagicMock()
        o_utils.get_network_driver.return_value = self.fake_network_driver
        self.fake_network_driver.get_subnet = self._mock_get_subnet
        self.fake_network_driver.neutron_client.list_ports = (
            self._mock_list_ports)
        self._local_net_cache = {}
        self._local_port_cache = {'ports': []}

    def _mock_get_subnet(self, subnet_id):
        m_subnet = mock.MagicMock()
        m_subnet.network_id = self._local_net_cache[subnet_id]
        return m_subnet

    def _mock_list_ports(self, **kwargs):
        return self._local_port_cache

    def _create_lb_model(self, vip=None, vip_network_id=None,
                         admin_state_up=True):
        lb = octavia_data_model.LoadBalancer()
        lb.loadbalancer_id = uuidutils.generate_uuid()

        if vip:
            lb.vip_address = vip
        else:
            lb.vip_address = '10.0.0.4'

        if vip_network_id:
            lb.vip_network_id = vip_network_id
        lb.admin_state_up = admin_state_up
        return lb

    def _create_pool_model(self, loadbalancer_id, pool_name, protocol=None,
                           admin_state_up=True, listener_id=None):
        m_pool = octavia_data_model.Pool()
        if protocol:
            m_pool.protocol = protocol
        else:
            m_pool.protocol = o_constants.PROTOCOL_TCP
        m_pool.name = pool_name
        m_pool.pool_id = uuidutils.generate_uuid()
        m_pool.loadbalancer_id = loadbalancer_id
        m_pool.members = []
        m_pool.admin_state_up = admin_state_up
        if listener_id:
            m_pool.listener_id = listener_id
        return m_pool

    def _create_member_model(self, pool_id, subnet_id, address,
                             protocol_port=None, admin_state_up=True):
        m_member = octavia_data_model.Member()
        if protocol_port:
            m_member.protocol_port = protocol_port
        else:
            m_member.protocol_port = 80

        m_member.member_id = uuidutils.generate_uuid()
        m_member.pool_id = pool_id
        if subnet_id:
            m_member.subnet_id = subnet_id
        m_member.address = address
        m_member.admin_state_up = admin_state_up
        return m_member

    def _create_listener_model(self, loadbalancer_id, pool_id=None,
                               protocol_port=80, protocol=None,
                               admin_state_up=True):
        m_listener = octavia_data_model.Listener()
        if protocol:
            m_listener.protocol = protocol
        else:
            m_listener.protocol = o_constants.PROTOCOL_TCP

        m_listener.listener_id = uuidutils.generate_uuid()
        m_listener.loadbalancer_id = loadbalancer_id
        if pool_id:
            m_listener.default_pool_id = pool_id
        m_listener.protocol_port = protocol_port
        m_listener.admin_state_up = admin_state_up
        return m_listener

    def _get_loadbalancers(self):
        lbs = []
        for lb in self.nb_api.tables['Load_Balancer'].rows.values():
            external_ids = dict(lb.external_ids)
            ls_refs = external_ids.get(ovn_driver.LB_EXT_IDS_LS_REFS_KEY)
            if ls_refs:
                external_ids[
                    ovn_driver.LB_EXT_IDS_LS_REFS_KEY] = jsonutils.loads(
                        ls_refs)
            lbs.append({'name': lb.name, 'protocol': lb.protocol,
                        'vips': lb.vips, 'external_ids': external_ids})

        return lbs

    def _get_loadbalancer_id(self, lb_name):
        for lb in self.nb_api.tables['Load_Balancer'].rows.values():
            if lb.name == lb_name:
                return lb.uuid

    def _validate_loadbalancers(self, expected_lbs):
        observed_lbs = self._get_loadbalancers()
        self.assertItemsEqual(expected_lbs, observed_lbs)

    def _is_lb_associated_to_ls(self, lb_name, ls_name):
        lb_uuid = self._get_loadbalancer_id(lb_name)
        for ls in self.nb_api.tables['Logical_Switch'].rows.values():
            if ls.name == ls_name:
                ls_lbs = [lb.uuid for lb in ls.load_balancer]
                return lb_uuid in ls_lbs

        return False

    def _create_router(self, name, gw_info=None):
        router = {'router':
                  {'name': name,
                   'admin_state_up': True,
                   'tenant_id': self._tenant_id}}
        if gw_info:
            router['router']['external_gateway_info'] = gw_info
        router = self.l3_plugin.create_router(self.context, router)
        return router['id']

    def _create_net(self, name, cidr, router_id=None):
        n1 = self._make_network(self.fmt, name, True)
        res = self._create_subnet(self.fmt, n1['network']['id'],
                                  cidr)
        subnet = self.deserialize(self.fmt, res)['subnet']
        self._local_net_cache[subnet['id']] = n1['network']['id']

        port = self._make_port(self.fmt, n1['network']['id'])
        if router_id:
            self.l3_plugin.add_router_interface(
                self.context, router_id, {'subnet_id': subnet['id']})
        self._local_port_cache['ports'].append(port['port'])
        vip_port_address = port['port']['fixed_ips'][0]['ip_address']
        return (n1['network']['id'], subnet['id'], vip_port_address,
                port['port']['id'])

    def _update_ls_refs(self, lb_data, net_id, add_ref=True):
        if not net_id.startswith("neutron-"):
            net_id = "neutron-" + net_id

        if add_ref:
            if net_id in lb_data[ovn_driver.LB_EXT_IDS_LS_REFS_KEY]:
                ref_ct = lb_data[
                    ovn_driver.LB_EXT_IDS_LS_REFS_KEY][net_id] + 1
                lb_data[ovn_driver.LB_EXT_IDS_LS_REFS_KEY][net_id] = ref_ct
            else:
                lb_data[ovn_driver.LB_EXT_IDS_LS_REFS_KEY][net_id] = 1
        else:
            ref_ct = lb_data[ovn_driver.LB_EXT_IDS_LS_REFS_KEY][net_id] - 1
            if ref_ct > 0:
                lb_data[ovn_driver.LB_EXT_IDS_LS_REFS_KEY][net_id] = ref_ct
            else:
                del lb_data[ovn_driver.LB_EXT_IDS_LS_REFS_KEY][net_id]

    def _wait_for_status_and_validate(self, lb_data, expected_status):
        call_count = len(expected_status)
        expected_calls = [mock.call(status) for status in expected_status]
        update_loadbalancer_status = (
            self._o_driver_lib.update_loadbalancer_status)
        n_utils.wait_until_true(
            lambda: update_loadbalancer_status.call_count == call_count,
            timeout=10)
        try:
            self._o_driver_lib.update_loadbalancer_status.assert_has_calls(
                expected_calls, any_order=True)
        except Exception:
            raise Exception(
                self._o_driver_lib.update_loadbalancer_status.mock_calls)
        expected_lbs = self._make_expected_lbs(lb_data)
        self._validate_loadbalancers(expected_lbs)

    def _create_load_balancer_and_validate(self, lb_info,
                                           admin_state_up=True):
        self._o_driver_lib.update_loadbalancer_status.reset_mock()
        lb_data = {}
        router_id = self._create_router("r1")
        lb_data[ovn_driver.LB_EXT_IDS_LR_REF_KEY] = router_id
        net_info = self._create_net(lb_info['vip_network'], lb_info['cidr'],
                                    router_id=router_id)
        lb_data['vip_net_info'] = net_info
        lb_data['model'] = self._create_lb_model(vip=net_info[2],
                                                 vip_network_id=net_info[0],
                                                 admin_state_up=admin_state_up)
        lb_data[ovn_driver.LB_EXT_IDS_LS_REFS_KEY] = {}
        lb_data['listeners'] = []
        lb_data['pools'] = []
        self._update_ls_refs(lb_data, net_info[0])

        self.ovn_driver.loadbalancer_create(lb_data['model'])
        if lb_data['model'].admin_state_up:
            expected_status = {
                'loadbalancers': [{"id": lb_data['model'].loadbalancer_id,
                                   "provisioning_status": "ACTIVE",
                                   "operating_status": o_constants.ONLINE}]
            }
        else:
            expected_status = {
                'loadbalancers': [{"id": lb_data['model'].loadbalancer_id,
                                   "provisioning_status": "ACTIVE",
                                   "operating_status": o_constants.OFFLINE}]
            }
        self._wait_for_status_and_validate(lb_data, [expected_status])
        self.assertTrue(
            self._is_lb_associated_to_ls(lb_data['model'].loadbalancer_id,
                                         'neutron-' + net_info[0]))
        return lb_data

    def _update_load_balancer_and_validate(self, lb_data,
                                           admin_state_up=None):
        self._o_driver_lib.update_loadbalancer_status.reset_mock()
        if admin_state_up is not None:
            lb_data['model'].admin_state_up = admin_state_up
        self.ovn_driver.loadbalancer_update(
            lb_data['model'], lb_data['model'])

        if lb_data['model'].admin_state_up:
            expected_status = {
                'loadbalancers': [{"id": lb_data['model'].loadbalancer_id,
                                   "provisioning_status": "ACTIVE",
                                   "operating_status": o_constants.ONLINE}]
            }
        else:
            expected_status = {
                'loadbalancers': [{"id": lb_data['model'].loadbalancer_id,
                                   "provisioning_status": "ACTIVE",
                                   "operating_status": o_constants.OFFLINE}]
            }

        self._wait_for_status_and_validate(lb_data, [expected_status])

    def _delete_load_balancer_and_validate(self, lb_data, cascade=False):
        self._o_driver_lib.update_loadbalancer_status.reset_mock()
        self.ovn_driver.loadbalancer_delete(lb_data['model'], cascade)
        expected_status = {
            'loadbalancers': [{"id": lb_data['model'].loadbalancer_id,
                               "provisioning_status": "DELETED",
                               "operating_status": "OFFLINE"}]
        }
        if cascade:
            expected_status['pools'] = []
            expected_status['members'] = []
            expected_status['listeners'] = []
            for pool in lb_data['pools']:
                expected_status['pools'].append({
                    'id': pool.pool_id,
                    'provisioning_status': 'DELETED'})
                for member in pool.members:
                    expected_status['members'].append({
                        "id": member.member_id,
                        "provisioning_status": "DELETED"})
            for listener in lb_data['listeners']:
                expected_status['listeners'].append({
                    "id": listener.listener_id,
                    "provisioning_status": "DELETED",
                    "operating_status": "OFFLINE"})
            expected_status = {
                key: value for key, value in expected_status.items() if value}

        lb = lb_data['model']
        del lb_data['model']
        self._wait_for_status_and_validate(lb_data, [expected_status])
        self._validate_loadbalancers([])
        vip_net_id = lb_data['vip_net_info'][0]
        self.assertFalse(
            self._is_lb_associated_to_ls(lb.loadbalancer_id,
                                         'neutron-' + vip_net_id))

    def _make_expected_lbs(self, lb_data):
        if not lb_data or not lb_data.get('model'):
            return []

        vip_net_info = lb_data['vip_net_info']
        external_ids = {ovn_driver.LB_EXT_IDS_LS_REFS_KEY: {},
                        'neutron:vip': lb_data['model'].vip_address,
                        'neutron:vip_port_id': vip_net_info[3],
                        'enabled': str(lb_data['model'].admin_state_up)}

        pool_info = {}
        for p in lb_data.get('pools', []):
            p_members = ""
            for m in p.members:
                if not m.admin_state_up:
                    continue
                m_info = 'member_' + m.member_id + '_' + m.address
                m_info += ":" + str(m.protocol_port)
                if p_members:
                    p_members += "," + m_info
                else:
                    p_members = m_info
            pool_key = 'pool_' + p.pool_id
            if not p.admin_state_up:
                pool_key += ':D'
            external_ids[pool_key] = p_members
            pool_info[p.pool_id] = p_members

        for net_id, ref_ct in lb_data[
            ovn_driver.LB_EXT_IDS_LS_REFS_KEY].items():
            external_ids[ovn_driver.LB_EXT_IDS_LS_REFS_KEY][net_id] = ref_ct

        if lb_data.get(ovn_driver.LB_EXT_IDS_LR_REF_KEY):
            external_ids[
                ovn_driver.LB_EXT_IDS_LR_REF_KEY] = 'neutron-' + lb_data[
                    ovn_driver.LB_EXT_IDS_LR_REF_KEY]

        expected_vips = {}
        expected_protocol = ['tcp']
        for l in lb_data['listeners']:
            listener_k = 'listener_' + str(l.listener_id)
            if lb_data['model'].admin_state_up and l.admin_state_up:
                vip_k = lb_data['model'].vip_address + ":" + str(
                    l.protocol_port)
                if not isinstance(l.default_pool_id,
                                  octavia_data_model.UnsetType) and pool_info[
                                      l.default_pool_id]:
                    expected_vips[vip_k] = self._extract_member_info(
                        pool_info[l.default_pool_id])
            else:
                listener_k += ':D'
            external_ids[listener_k] = str(l.protocol_port) + ":"
            if not isinstance(l.default_pool_id,
                              octavia_data_model.UnsetType):
                external_ids[listener_k] += 'pool_' + (l.default_pool_id)
            elif lb_data.get('pools', []):
                external_ids[listener_k] += 'pool_' + lb_data[
                    'pools'][0].pool_id

        expected_lbs = [{'name': lb_data['model'].loadbalancer_id,
                         'protocol': expected_protocol,
                         'vips': expected_vips,
                         'external_ids': external_ids}]
        return expected_lbs

    def _extract_member_info(self, member):
        mem_info = ''
        if member:
            for item in member.split(','):
                mem_info += item.split('_')[2] + ","
        return mem_info[:-1]

    def _create_pool_and_validate(self, lb_data, pool_name,
                                  listener_id=None):
        lb_pools = lb_data['pools']
        m_pool = self._create_pool_model(lb_data['model'].loadbalancer_id,
                                         pool_name,
                                         listener_id=listener_id)
        lb_pools.append(m_pool)
        self._o_driver_lib.update_loadbalancer_status.reset_mock()
        self.ovn_driver.pool_create(m_pool)
        expected_status = {
            'pools': [{'id': m_pool.pool_id,
                       'provisioning_status': 'ACTIVE',
                       'operating_status': o_constants.ONLINE}],
            'loadbalancers': [{'id': m_pool.loadbalancer_id,
                               'provisioning_status': 'ACTIVE'}]
        }
        if listener_id:
            expected_status['listeners'] = [
                {'id': listener_id,
                 'provisioning_status': 'ACTIVE'}]

        self._wait_for_status_and_validate(lb_data, [expected_status])

        expected_lbs = self._make_expected_lbs(lb_data)
        self._validate_loadbalancers(expected_lbs)

    def _update_pool_and_validate(self, lb_data, pool_name,
                                  admin_state_up=None):
        self._o_driver_lib.update_loadbalancer_status.reset_mock()
        m_pool = self._get_pool_from_lb_data(lb_data, pool_name=pool_name)
        old_admin_state_up = m_pool.admin_state_up
        operating_status = 'ONLINE'
        if admin_state_up is not None:
            m_pool.admin_state_up = admin_state_up
            if not admin_state_up:
                operating_status = 'OFFLINE'

        pool_listeners = self._get_pool_listeners(lb_data, m_pool.pool_id)
        expected_listener_status = [
            {'id': l.listener_id, 'provisioning_status': 'ACTIVE'}
            for l in pool_listeners]
        self.ovn_driver.pool_update(m_pool, m_pool)
        expected_status = {
            'pools': [{'id': m_pool.pool_id,
                       'provisioning_status': 'ACTIVE',
                       'operating_status': operating_status}],
            'loadbalancers': [{'id': m_pool.loadbalancer_id,
                               'provisioning_status': 'ACTIVE'}],
            'listeners': expected_listener_status
        }

        if old_admin_state_up != m_pool.admin_state_up:
            if m_pool.admin_state_up:
                oper_status = o_constants.ONLINE
            else:
                oper_status = o_constants.OFFLINE
            expected_status['pools'][0]['operating_status'] = oper_status
        self._wait_for_status_and_validate(lb_data, [expected_status])

    def _delete_pool_and_validate(self, lb_data, pool_name,
                                  listener_id=None):
        self._o_driver_lib.update_loadbalancer_status.reset_mock()
        p = self._get_pool_from_lb_data(lb_data, pool_name=pool_name)
        self.ovn_driver.pool_delete(p)
        lb_data['pools'].remove(p)
        expected_status = []
        # When a pool is deleted and if it has any members, there are
        # expected to be deleted.
        for m in p.members:
            expected_status.append(
                {'pools': [{"id": p.pool_id,
                            "provisioning_status": "ACTIVE"}],
                 'members': [{"id": m.member_id,
                              "provisioning_status": "DELETED"}],
                 'loadbalancers': [{"id": p.loadbalancer_id,
                                    "provisioning_status": "ACTIVE"}],
                 'listeners': []})
            self._update_ls_refs(
                lb_data, self._local_net_cache[m.subnet_id], add_ref=False)

        pool_dict = {
            'pools': [{'id': p.pool_id,
                       'provisioning_status': 'DELETED'}],
            'loadbalancers': [{'id': p.loadbalancer_id,
                               'provisioning_status': 'ACTIVE'}]
        }
        if listener_id:
            pool_dict['listeners'] = [{'id': listener_id,
                                       'provisioning_status': 'ACTIVE'}]
        expected_status.append(pool_dict)
        self._wait_for_status_and_validate(lb_data, expected_status)

    def _get_pool_from_lb_data(self, lb_data, pool_id=None,
                               pool_name=None):
        for p in lb_data['pools']:
            if pool_id and p.pool_id == pool_id:
                return p

            if pool_name and p.name == pool_name:
                return p

    def _get_listener_from_lb_data(self, lb_data, protocol_port):
        for l in lb_data['listeners']:
            if l.protocol_port == protocol_port:
                return l

    def _get_pool_listeners(self, lb_data, pool_id):
        listeners = []
        for l in lb_data['listeners']:
            if l.default_pool_id == pool_id:
                listeners.append(l)

        return listeners

    def _create_member_and_validate(self, lb_data, pool_id, subnet_id,
                                    network_id, address):
        self._o_driver_lib.update_loadbalancer_status.reset_mock()
        pool = self._get_pool_from_lb_data(lb_data, pool_id=pool_id)

        m_member = self._create_member_model(pool.pool_id, subnet_id, address)
        pool.members.append(m_member)

        self.ovn_driver.member_create(m_member)
        self._update_ls_refs(lb_data, network_id)
        pool_listeners = self._get_pool_listeners(lb_data, pool_id)

        expected_listener_status = [
            {'id': l.listener_id, 'provisioning_status': 'ACTIVE'}
            for l in pool_listeners]

        expected_status = {
            'pools': [{'id': pool.pool_id,
                       'provisioning_status': 'ACTIVE'}],
            'members': [{"id": m_member.member_id,
                         "provisioning_status": "ACTIVE"}],
            'loadbalancers': [{'id': pool.loadbalancer_id,
                               'provisioning_status': 'ACTIVE'}],
            'listeners': expected_listener_status
        }
        self._wait_for_status_and_validate(lb_data, [expected_status])

    def _get_pool_member(self, pool, member_address):
        for m in pool.members:
            if m.address == member_address:
                return m

    def _update_member_and_validate(self, lb_data, pool_id, member_address):
        pool = self._get_pool_from_lb_data(lb_data, pool_id=pool_id)

        member = self._get_pool_member(pool, member_address)
        self._o_driver_lib.update_loadbalancer_status.reset_mock()
        self.ovn_driver.member_update(member, member)
        expected_status = {
            'pools': [{'id': pool.pool_id,
                       'provisioning_status': 'ACTIVE'}],
            'members': [{"id": member.member_id,
                         'provisioning_status': 'ACTIVE',
                         'operating_status': 'ONLINE'}],
            'loadbalancers': [{'id': pool.loadbalancer_id,
                               'provisioning_status': 'ACTIVE'}],
            'listeners': []
        }
        self._wait_for_status_and_validate(lb_data, [expected_status])

    def _update_members_in_batch_and_validate(self, lb_data, pool_id,
                                              member_addresses):
        pool = self._get_pool_from_lb_data(lb_data, pool_id=pool_id)

        members = []
        expected_status = []
        for addr in member_addresses:
            member = self._get_pool_member(pool, addr)
            members.append(member)
            expected_status.append(
                {'pools': [{'id': pool.pool_id,
                           'provisioning_status': 'ACTIVE'}],
                 'members': [{'id': member.member_id,
                              'provisioning_status': 'ACTIVE',
                              'operating_status': 'ONLINE'}],
                 'loadbalancers': [{'id': pool.loadbalancer_id,
                                   'provisioning_status': 'ACTIVE'}],
                 'listeners': []})
        self._o_driver_lib.update_loadbalancer_status.reset_mock()
        self.ovn_driver.member_batch_update(members)
        self._wait_for_status_and_validate(lb_data, expected_status)

    def _delete_member_and_validate(self, lb_data, pool_id, network_id,
                                    member_address):
        pool = self._get_pool_from_lb_data(lb_data, pool_id=pool_id)
        member = self._get_pool_member(pool, member_address)
        pool.members.remove(member)
        self._o_driver_lib.update_loadbalancer_status.reset_mock()
        self.ovn_driver.member_delete(member)
        expected_status = {
            'pools': [{"id": pool.pool_id,
                       "provisioning_status": "ACTIVE"}],
            'members': [{"id": member.member_id,
                         "provisioning_status": "DELETED"}],
            'loadbalancers': [{"id": pool.loadbalancer_id,
                               "provisioning_status": "ACTIVE"}],
            'listeners': []}

        self._update_ls_refs(lb_data, network_id, add_ref=False)
        self._wait_for_status_and_validate(lb_data, [expected_status])

    def _create_listener_and_validate(self, lb_data, pool_id=None,
                                      protocol_port=80,
                                      admin_state_up=True, protocol='TCP'):
        if pool_id:
            pool = self._get_pool_from_lb_data(lb_data, pool_id=pool_id)
            loadbalancer_id = pool.loadbalancer_id
            pool_id = pool.pool_id
        else:
            loadbalancer_id = lb_data['model'].loadbalancer_id
            pool_id = None
        m_listener = self._create_listener_model(loadbalancer_id,
                                                 pool_id, protocol_port,
                                                 protocol=protocol,
                                                 admin_state_up=admin_state_up)
        lb_data['listeners'].append(m_listener)

        self._o_driver_lib.update_loadbalancer_status.reset_mock()
        self.ovn_driver.listener_create(m_listener)
        expected_status = {
            'listeners': [{'id': m_listener.listener_id,
                           'provisioning_status': 'ACTIVE',
                           'operating_status': 'ONLINE'}],
            'loadbalancers': [{'id': m_listener.loadbalancer_id,
                               'provisioning_status': "ACTIVE"}]}

        self._wait_for_status_and_validate(lb_data, [expected_status])

    def _update_listener_and_validate(self, lb_data, protocol_port,
                                      admin_state_up=None, protocol='TCP'):
        m_listener = self._get_listener_from_lb_data(lb_data, protocol_port)
        self._o_driver_lib.update_loadbalancer_status.reset_mock()
        old_admin_state_up = m_listener.admin_state_up
        operating_status = 'ONLINE'
        if admin_state_up is not None:
            m_listener.admin_state_up = admin_state_up
            if not admin_state_up:
                operating_status = 'OFFLINE'
        m_listener.protocol = protocol

        self.ovn_driver.listener_update(m_listener, m_listener)
        pool_status = [{'id': m_listener.default_pool_id,
                        'provisioning_status': 'ACTIVE'}]
        expected_status = {
            'listeners': [{'id': m_listener.listener_id,
                           'provisioning_status': 'ACTIVE',
                           'operating_status': operating_status}],
            'loadbalancers': [{"id": m_listener.loadbalancer_id,
                               "provisioning_status": "ACTIVE"}],
            'pools': pool_status}

        if old_admin_state_up != m_listener.admin_state_up:
            if m_listener.admin_state_up:
                oper_status = o_constants.ONLINE
            else:
                oper_status = o_constants.OFFLINE
            expected_status['listeners'][0]['operating_status'] = oper_status

        self._wait_for_status_and_validate(lb_data, [expected_status])

    def _delete_listener_and_validate(self, lb_data, protocol_port=80):
        m_listener = self._get_listener_from_lb_data(lb_data, protocol_port)
        lb_data['listeners'].remove(m_listener)
        self._o_driver_lib.update_loadbalancer_status.reset_mock()
        self.ovn_driver.listener_delete(m_listener)
        expected_status = {
            'listeners': [{"id": m_listener.listener_id,
                           "provisioning_status": "DELETED",
                           "operating_status": "OFFLINE"}],
            'loadbalancers': [{"id": m_listener.loadbalancer_id,
                               "provisioning_status": "ACTIVE"}]}

        self._wait_for_status_and_validate(lb_data, [expected_status])

    def test_loadbalancer(self):
        lb_data = self._create_load_balancer_and_validate(
            {'vip_network': 'vip_network',
             'cidr': '10.0.0.0/24'})
        self._update_load_balancer_and_validate(lb_data, admin_state_up=False)
        self._update_load_balancer_and_validate(lb_data, admin_state_up=True)
        self._delete_load_balancer_and_validate(lb_data)
        # create load balance with admin state down
        lb_data = self._create_load_balancer_and_validate(
            {'vip_network': 'vip_network',
             'cidr': '10.0.0.0/24'}, admin_state_up=False)
        self._delete_load_balancer_and_validate(lb_data)

    def test_pool(self):
        lb_data = self._create_load_balancer_and_validate(
            {'vip_network': 'vip_network',
             'cidr': '10.0.0.0/24'})
        self._create_pool_and_validate(lb_data, "p1")
        self._update_pool_and_validate(lb_data, "p1")
        self._update_pool_and_validate(lb_data, "p1", admin_state_up=True)
        self._update_pool_and_validate(lb_data, "p1", admin_state_up=False)
        self._update_pool_and_validate(lb_data, "p1", admin_state_up=True)
        self._create_pool_and_validate(lb_data, "p2")
        self._delete_pool_and_validate(lb_data, "p2")
        self._delete_pool_and_validate(lb_data, "p1")
        self._delete_load_balancer_and_validate(lb_data)

    def test_member(self):
        lb_data = self._create_load_balancer_and_validate(
            {'vip_network': 'vip_network',
             'cidr': '10.0.0.0/24'})
        self._create_pool_and_validate(lb_data, "p1")
        pool_id = lb_data['pools'][0].pool_id
        self._create_member_and_validate(
            lb_data, pool_id, lb_data['vip_net_info'][1],
            lb_data['vip_net_info'][0], '10.0.0.10')
        self._update_member_and_validate(lb_data, pool_id, "10.0.0.10")

        self._create_member_and_validate(
            lb_data, pool_id, lb_data['vip_net_info'][1],
            lb_data['vip_net_info'][0], '10.0.0.11')

        # Disable loadbalancer
        self._update_load_balancer_and_validate(lb_data,
                                                admin_state_up=False)

        # Enable loadbalancer back
        self._update_load_balancer_and_validate(lb_data,
                                                admin_state_up=True)
        # Test member_batch_update
        self._update_members_in_batch_and_validate(lb_data, pool_id,
                                                   ['10.0.0.10', '10.0.0.11'])

        self._delete_member_and_validate(lb_data, pool_id,
                                         lb_data['vip_net_info'][0],
                                         '10.0.0.10')
        self._delete_member_and_validate(lb_data, pool_id,
                                         lb_data['vip_net_info'][0],
                                         '10.0.0.11')
        self._create_member_and_validate(
            lb_data, pool_id, lb_data['vip_net_info'][1],
            lb_data['vip_net_info'][0], '10.0.0.10')

        net20_info = self._create_net('net20', '20.0.0.0/24')
        net20 = net20_info[0]
        subnet20 = net20_info[1]
        self._create_member_and_validate(lb_data, pool_id, subnet20, net20,
                                         '20.0.0.4')
        self._create_member_and_validate(lb_data, pool_id, subnet20, net20,
                                         '20.0.0.6')
        net30_info = self._create_net('net30', '30.0.0.0/24')
        net30 = net30_info[0]
        subnet30 = net30_info[1]
        self._create_member_and_validate(lb_data, pool_id, subnet30, net30,
                                         '30.0.0.6')
        self._delete_member_and_validate(lb_data, pool_id, net20, '20.0.0.6')

        # Test creating Member without subnet
        m_member = self._create_member_model(pool_id,
                                             None,
                                             '30.0.0.7', 80)
        self.assertRaises(o_exceptions.UnsupportedOptionError,
                          self.ovn_driver.member_create, m_member)

        # Deleting the pool should also delete the members.
        self._delete_pool_and_validate(lb_data, "p1")
        self._delete_load_balancer_and_validate(lb_data)

    def test_listener(self):
        lb_data = self._create_load_balancer_and_validate(
            {'vip_network': 'vip_network',
             'cidr': '10.0.0.0/24'})
        self._create_pool_and_validate(lb_data, "p1")
        pool_id = lb_data['pools'][0].pool_id
        self._create_member_and_validate(
            lb_data, pool_id, lb_data['vip_net_info'][1],
            lb_data['vip_net_info'][0], '10.0.0.10')

        net_info = self._create_net('net1', '20.0.0.0/24')
        self._create_member_and_validate(lb_data, pool_id,
                                         net_info[1], net_info[0], '20.0.0.4')
        self._create_listener_and_validate(lb_data, pool_id, 80)
        self._update_listener_and_validate(lb_data, 80)
        self._update_listener_and_validate(lb_data, 80, admin_state_up=True)
        self._update_listener_and_validate(lb_data, 80, admin_state_up=False)
        self._update_listener_and_validate(lb_data, 80, admin_state_up=True)
        self._create_listener_and_validate(lb_data, pool_id, 82)

        self._delete_listener_and_validate(lb_data, 82)
        self._delete_listener_and_validate(lb_data, 80)

    def _test_cascade_delete(self, pool=True, listener=True, member=True):
        lb_data = self._create_load_balancer_and_validate(
            {'vip_network': 'vip_network',
             'cidr': '10.0.0.0/24'})
        if pool:
            self._create_pool_and_validate(lb_data, "p1")
            pool_id = lb_data['pools'][0].pool_id
            if member:
                self._create_member_and_validate(
                    lb_data, pool_id, lb_data['vip_net_info'][1],
                    lb_data['vip_net_info'][0], '10.0.0.10')
            if listener:
                self._create_listener_and_validate(lb_data, pool_id, 80)

        self._delete_load_balancer_and_validate(lb_data, cascade=True)

    def test_lb_listener_pools_cascade(self):
        self._test_cascade_delete(member=False)

    def test_lb_pool_cascade(self):
        self._test_cascade_delete(member=False, listener=False)

    def test_cascade_delete(self):
        self._test_cascade_delete()

    def test_for_unsupported_options(self):
        lb_data = self._create_load_balancer_and_validate(
            {'vip_network': 'vip_network',
             'cidr': '10.0.0.0/24'})

        m_pool = self._create_pool_model(lb_data['model'].loadbalancer_id,
                                         'lb1')
        m_pool.protocol = o_constants.PROTOCOL_HTTP
        self.assertRaises(o_exceptions.UnsupportedOptionError,
                          self.ovn_driver.pool_create, m_pool)

        m_listener = self._create_listener_model(
            lb_data['model'].loadbalancer_id, m_pool.pool_id, 80)
        m_listener.protocol = o_constants.PROTOCOL_HTTP
        self.assertRaises(o_exceptions.UnsupportedOptionError,
                          self.ovn_driver.listener_create, m_listener)
        self._create_listener_and_validate(lb_data)
        self.assertRaises(o_exceptions.UnsupportedOptionError,
                          self._create_listener_and_validate,
                          lb_data, protocol_port=80, protocol='UDP')
        self.assertRaises(o_exceptions.UnsupportedOptionError,
                          self._update_listener_and_validate,
                          lb_data, protocol_port=80, protocol='UDP')

    def test_lb_listener_pool_workflow(self):
        lb_data = self._create_load_balancer_and_validate(
            {'vip_network': 'vip_network',
             'cidr': '10.0.0.0/24'})
        self._create_listener_and_validate(lb_data)
        self._create_pool_and_validate(lb_data, "p1",
                                       lb_data['listeners'][0].listener_id)
        self._delete_pool_and_validate(lb_data, "p1",
                                       lb_data['listeners'][0].listener_id)
        self._delete_listener_and_validate(lb_data)
        self._delete_load_balancer_and_validate(lb_data)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\octavia\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\resources\process.py
===========File Type===========
.py
===========File Content===========
# Copyright 2016 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from distutils import spawn
import os

import fixtures
from neutron.agent.linux import utils
import psutil
import tenacity


class OvnNorthd(fixtures.Fixture):

    def __init__(self, temp_dir, ovn_nb_db, ovn_sb_db, protocol='unix'):
        super(OvnNorthd, self).__init__()
        self.temp_dir = temp_dir
        self.ovn_nb_db = ovn_nb_db
        self.ovn_sb_db = ovn_sb_db
        self.protocol = protocol
        self.unixctl_path = self.temp_dir + '/ovn_northd.ctl'
        self.log_file_path = self.temp_dir + '/ovn_northd.log'
        if self.protocol == 'ssl':
            self.private_key = os.path.join(self.temp_dir, 'ovn-privkey.pem')
            self.certificate = os.path.join(self.temp_dir, 'ovn-cert.pem')
            self.ca_cert = os.path.join(self.temp_dir, 'controllerca',
                                        'cacert.pem')

    def _setUp(self):
        self.addCleanup(self.stop)
        self.start()

    def start(self):
        # start the ovn-northd
        ovn_northd_cmd = [
            spawn.find_executable('ovn-northd'), '-vconsole:off',
            '--ovnnb-db=%s' % self.ovn_nb_db,
            '--ovnsb-db=%s' % self.ovn_sb_db,
            '--no-chdir',
            '--unixctl=%s' % self.unixctl_path,
            '--log-file=%s' % (self.log_file_path)]
        if self.protocol == 'ssl':
            ovn_northd_cmd.append('--private-key=%s' % self.private_key)
            ovn_northd_cmd.append('--certificate=%s' % self.certificate)
            ovn_northd_cmd.append('--ca-cert=%s' % self.ca_cert)
        utils.create_process(ovn_northd_cmd)

    def stop(self):
        try:
            stop_cmd = ['ovs-appctl', '-t', self.unixctl_path, 'exit']
            utils.execute(stop_cmd)
        except Exception:
            pass


class OvsdbServer(fixtures.Fixture):

    def __init__(self, temp_dir, ovs_dir, ovn_nb_db=True, ovn_sb_db=False,
                 protocol='unix'):
        super(OvsdbServer, self).__init__()
        self.temp_dir = temp_dir
        self.ovs_dir = ovs_dir
        self.ovn_nb_db = ovn_nb_db
        self.ovn_sb_db = ovn_sb_db
        # The value of the protocol must be unix or tcp or ssl
        self.protocol = protocol
        self.ovsdb_server_processes = []
        self.private_key = os.path.join(self.temp_dir, 'ovn-privkey.pem')
        self.certificate = os.path.join(self.temp_dir, 'ovn-cert.pem')
        self.ca_cert = os.path.join(self.temp_dir, 'controllerca',
                                    'cacert.pem')

    def _setUp(self):
        if self.ovn_nb_db:
            self.ovsdb_server_processes.append(
                {'db_path': self.temp_dir + '/ovn_nb.db',
                 'schema_path': self.ovs_dir + '/ovn-nb.ovsschema',
                 'remote_path': self.temp_dir + '/ovnnb_db.sock',
                 'protocol': self.protocol,
                 'remote_ip': '127.0.0.1',
                 'remote_port': '0',
                 'unixctl_path': self.temp_dir + '/ovnnb_db.ctl',
                 'log_file_path': self.temp_dir + '/ovn_nb.log',
                 'db_type': 'nb',
                 'connection': 'db:OVN_Northbound,NB_Global,connections',
                 'ctl_cmd': 'ovn-nbctl'})

        if self.ovn_sb_db:
            self.ovsdb_server_processes.append(
                {'db_path': self.temp_dir + '/ovn_sb.db',
                 'schema_path': self.ovs_dir + '/ovn-sb.ovsschema',
                 'remote_path': self.temp_dir + '/ovnsb_db.sock',
                 'protocol': self.protocol,
                 'remote_ip': '127.0.0.1',
                 'remote_port': '0',
                 'unixctl_path': self.temp_dir + '/ovnsb_db.ctl',
                 'log_file_path': self.temp_dir + '/ovn_sb.log',
                 'db_type': 'sb',
                 'connection': 'db:OVN_Southbound,SB_Global,connections',
                 'ctl_cmd': 'ovn-sbctl'})
        self.addCleanup(self.stop)
        self.start()

    def _init_ovsdb_pki(self):
        os.chdir(self.temp_dir)
        pki_init_cmd = [spawn.find_executable('ovs-pki'), 'init',
                        '-d', self.temp_dir, '-l',
                        os.path.join(self.temp_dir, 'pki.log'), '--force']
        utils.execute(pki_init_cmd)
        pki_req_sign = [spawn.find_executable('ovs-pki'), 'req+sign', 'ovn',
                        'controller', '-d', self.temp_dir, '-l',
                        os.path.join(self.temp_dir, 'pki.log'), '--force']
        utils.execute(pki_req_sign)

    def start(self):
        pki_done = False
        for ovsdb_process in self.ovsdb_server_processes:
            # create the db from the schema using ovsdb-tool
            ovsdb_tool_cmd = [spawn.find_executable('ovsdb-tool'),
                              'create', ovsdb_process['db_path'],
                              ovsdb_process['schema_path']]
            utils.execute(ovsdb_tool_cmd)

            # start the ovsdb-server
            ovsdb_server_cmd = [
                spawn.find_executable('ovsdb-server'), '-vconsole:off',
                '--log-file=%s' % (ovsdb_process['log_file_path']),
                '--remote=punix:%s' % (ovsdb_process['remote_path']),
                '--remote=%s' % (ovsdb_process['connection']),
                '--unixctl=%s' % (ovsdb_process['unixctl_path'])]
            if ovsdb_process['protocol'] == 'ssl':
                if not pki_done:
                    pki_done = True
                    self._init_ovsdb_pki()
                ovsdb_server_cmd.append('--private-key=%s' % self.private_key)
                ovsdb_server_cmd.append('--certificate=%s' % self.certificate)
                ovsdb_server_cmd.append('--ca-cert=%s' % self.ca_cert)
            ovsdb_server_cmd.append(ovsdb_process['db_path'])
            obj, _ = utils.create_process(ovsdb_server_cmd)

            conn_cmd = [spawn.find_executable(ovsdb_process['ctl_cmd']),
                        '--db=unix:%s' % ovsdb_process['remote_path'],
                        'set-connection',
                        'p%s:%s:%s' % (ovsdb_process['protocol'],
                                       ovsdb_process['remote_port'],
                                       ovsdb_process['remote_ip']),
                        '--', 'set', 'connection', '.',
                        'inactivity_probe=60000']

            @tenacity.retry(wait=tenacity.wait_exponential(multiplier=0.1),
                            stop=tenacity.stop_after_delay(3), reraise=True)
            def _set_connection():
                utils.execute(conn_cmd)

            @tenacity.retry(
                wait=tenacity.wait_exponential(multiplier=0.1),
                stop=tenacity.stop_after_delay(10),
                reraise=True)
            def get_ovsdb_remote_port_retry(pid):
                process = psutil.Process(pid)
                for connect in process.connections():
                    if connect.status == 'LISTEN':
                        return connect.laddr[1]
                raise Exception(_("Could not find LISTEN port."))

            if ovsdb_process['protocol'] != 'unix':
                _set_connection()
                ovsdb_process['remote_port'] = \
                    get_ovsdb_remote_port_retry(obj.pid)

    def stop(self):
        for ovsdb_process in self.ovsdb_server_processes:
            try:
                stop_cmd = ['ovs-appctl', '-t', ovsdb_process['unixctl_path'],
                            'exit']
                utils.execute(stop_cmd)
            except Exception:
                pass

    def get_ovsdb_connection_path(self, db_type='nb'):
        for ovsdb_process in self.ovsdb_server_processes:
            if ovsdb_process['db_type'] == db_type:
                if ovsdb_process['protocol'] == 'unix':
                    return 'unix:' + ovsdb_process['remote_path']
                else:
                    return '%s:%s:%s' % (ovsdb_process['protocol'],
                                         ovsdb_process['remote_ip'],
                                         ovsdb_process['remote_port'])




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\functional\resources\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\fakes.py
===========File Type===========
.py
===========File Content===========
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import collections
import copy

import mock
from neutron_lib.api.definitions import l3
from oslo_utils import uuidutils

from networking_ovn.common import constants as ovn_const
from networking_ovn.common import utils


class FakeOvsdbNbOvnIdl(object):

    def __init__(self, **kwargs):
        self.lswitch_table = FakeOvsdbTable.create_one_ovsdb_table()
        self.lsp_table = FakeOvsdbTable.create_one_ovsdb_table()
        self.lrouter_table = FakeOvsdbTable.create_one_ovsdb_table()
        self.lrouter_static_route_table = \
            FakeOvsdbTable.create_one_ovsdb_table()
        self.lrp_table = FakeOvsdbTable.create_one_ovsdb_table()
        self.addrset_table = FakeOvsdbTable.create_one_ovsdb_table()
        self.acl_table = FakeOvsdbTable.create_one_ovsdb_table()
        self.dhcp_options_table = FakeOvsdbTable.create_one_ovsdb_table()
        self.nat_table = FakeOvsdbTable.create_one_ovsdb_table()
        self.port_group_table = FakeOvsdbTable.create_one_ovsdb_table()
        self._tables = {}
        self._tables['Logical_Switch'] = self.lswitch_table
        self._tables['Logical_Switch_Port'] = self.lsp_table
        self._tables['Logical_Router'] = self.lrouter_table
        self._tables['Logical_Router_Port'] = self.lrp_table
        self._tables['Logical_Router_Static_Route'] = \
            self.lrouter_static_route_table
        self._tables['ACL'] = self.acl_table
        self._tables['Address_Set'] = self.addrset_table
        self._tables['DHCP_Options'] = self.dhcp_options_table
        self._tables['NAT'] = self.nat_table
        self._tables['Port_Group'] = self.port_group_table
        self.transaction = mock.MagicMock()
        self.ls_add = mock.Mock()
        self.set_lswitch_ext_ids = mock.Mock()
        self.ls_del = mock.Mock()
        self.create_lswitch_port = mock.Mock()
        self.set_lswitch_port = mock.Mock()
        self.delete_lswitch_port = mock.Mock()
        self.get_acls_for_lswitches = mock.Mock()
        self.create_lrouter = mock.Mock()
        self.lrp_del = mock.Mock()
        self.update_lrouter = mock.Mock()
        self.delete_lrouter = mock.Mock()
        self.add_lrouter_port = mock.Mock()
        self.update_lrouter_port = mock.Mock()
        self.delete_lrouter_port = mock.Mock()
        self.set_lrouter_port_in_lswitch_port = mock.Mock()
        self.add_acl = mock.Mock()
        self.delete_acl = mock.Mock()
        self.update_acls = mock.Mock()
        self.idl = mock.Mock()
        self.add_static_route = mock.Mock()
        self.delete_static_route = mock.Mock()
        self.create_address_set = mock.Mock()
        self.update_address_set_ext_ids = mock.Mock()
        self.delete_address_set = mock.Mock()
        self.update_address_set = mock.Mock()
        self.get_all_chassis_gateway_bindings = mock.Mock()
        self.get_gateway_chassis_binding = mock.Mock()
        self.get_unhosted_gateways = mock.Mock()
        self.add_dhcp_options = mock.Mock()
        self.delete_dhcp_options = mock.Mock()
        self.get_subnet_dhcp_options = mock.Mock()
        self.get_subnet_dhcp_options.return_value = {
            'subnet': None, 'ports': []}
        self.get_subnets_dhcp_options = mock.Mock()
        self.get_subnets_dhcp_options.return_value = []
        self.get_all_dhcp_options = mock.Mock()
        self.get_router_port_options = mock.MagicMock()
        self.get_router_port_options.return_value = {}
        self.add_nat_rule_in_lrouter = mock.Mock()
        self.delete_nat_rule_in_lrouter = mock.Mock()
        self.get_lrouter_nat_rules = mock.Mock()
        self.get_lrouter_nat_rules.return_value = []
        self.set_nat_rule_in_lrouter = mock.Mock()
        self.check_for_row_by_value_and_retry = mock.Mock()
        self.get_parent_port = mock.Mock()
        self.get_parent_port.return_value = []
        self.dns_add = mock.Mock()
        self.get_lswitch = mock.Mock()
        fake_ovs_row = FakeOvsdbRow.create_one_ovsdb_row()
        self.get_lswitch.return_value = fake_ovs_row
        self.get_ls_and_dns_record = mock.Mock()
        self.get_ls_and_dns_record.return_value = (fake_ovs_row, None)
        self.ls_set_dns_records = mock.Mock()
        self.get_floatingip = mock.Mock()
        self.get_floatingip.return_value = None
        self.check_revision_number = mock.Mock()
        self.lookup = mock.MagicMock()
        # TODO(lucasagomes): The get_floatingip_by_ips() method is part
        # of a backwards compatibility layer for the Pike -> Queens release,
        # remove it in the Rocky release.
        self.get_floatingip_by_ips = mock.Mock()
        self.get_floatingip_by_ips.return_value = None
        self.is_col_present = mock.Mock()
        self.is_col_present.return_value = False
        self.get_lrouter = mock.Mock()
        self.get_lrouter.return_value = None
        self.delete_lrouter_ext_gw = mock.Mock()
        self.delete_lrouter_ext_gw.return_value = None
        self.is_port_groups_supported = mock.Mock()
        # TODO(lucasagomes): Flip this return value to True at some point,
        # port groups should be the default method used by networking-ovn
        self.is_port_groups_supported.return_value = False
        self.get_address_set = mock.Mock()
        self.get_address_set.return_value = None
        self.pg_acl_add = mock.Mock()
        self.pg_acl_del = mock.Mock()
        self.pg_del = mock.Mock()
        self.pg_add = mock.Mock()
        self.get_port_group = mock.Mock()
        self.pg_add_ports = mock.Mock()
        self.pg_del_ports = mock.Mock()
        self.lsp_get_up = mock.Mock()
        self.nb_global = mock.Mock()
        self.db_list_rows = mock.Mock()
        self.lsp_list = mock.MagicMock()


class FakeOvsdbSbOvnIdl(object):

    def __init__(self, **kwargs):
        self.chassis_exists = mock.Mock()
        self.chassis_exists.return_value = True
        self.get_chassis_hostname_and_physnets = mock.Mock()
        self.get_chassis_hostname_and_physnets.return_value = {}
        self.get_all_chassis = mock.Mock()
        self.get_chassis_data_for_ml2_bind_port = mock.Mock()
        self.get_chassis_data_for_ml2_bind_port.return_value = \
            ('fake', '', ['fake-physnet'])


class FakeOvsdbTransaction(object):
    def __init__(self, **kwargs):
        self.insert = mock.Mock()


class FakePlugin(object):

    def __init__(self, **kwargs):
        self.get_ports = mock.Mock()
        self._get_port_security_group_bindings = mock.Mock()


class FakeResource(dict):

    def __init__(self, manager=None, info=None, loaded=False, methods=None):
        """Set attributes and methods for a resource.

        :param manager:
            The resource manager
        :param Dictionary info:
            A dictionary with all attributes
        :param bool loaded:
            True if the resource is loaded in memory
        :param Dictionary methods:
            A dictionary with all methods
        """
        info = info or {}
        super(FakeResource, self).__init__(info)
        methods = methods or {}

        self.__name__ = type(self).__name__
        self.manager = manager
        self._info = info
        self._add_details(info)
        self._add_methods(methods)
        self._loaded = loaded
        # Add a revision number by default
        setattr(self, 'revision_number', 1)

    @property
    def db_obj(self):
        return self

    def _add_details(self, info):
        for (k, v) in info.items():
            setattr(self, k, v)

    def _add_methods(self, methods):
        """Fake methods with MagicMock objects.

        For each <@key, @value> pairs in methods, add an callable MagicMock
        object named @key as an attribute, and set the mock's return_value to
        @value. When users access the attribute with (), @value will be
        returned, which looks like a function call.
        """
        for (name, ret) in methods.items():
            method = mock.MagicMock(return_value=ret)
            setattr(self, name, method)

    def __repr__(self):
        reprkeys = sorted(k for k in self.__dict__.keys() if k[0] != '_' and
                          k != 'manager')
        info = ", ".join("%s=%s" % (k, getattr(self, k)) for k in reprkeys)
        return "<%s %s>" % (self.__class__.__name__, info)

    def keys(self):
        return self._info.keys()

    def info(self):
        return self._info

    def update(self, info):
        super(FakeResource, self).update(info)
        self._add_details(info)


class FakeNetwork(object):
    """Fake one or more networks."""

    @staticmethod
    def create_one_network(attrs=None):
        """Create a fake network.

        :param Dictionary attrs:
            A dictionary with all attributes
        :return:
            A FakeResource object faking the network
        """
        attrs = attrs or {}

        # Set default attributes.
        fake_uuid = uuidutils.generate_uuid()
        network_attrs = {
            'id': 'network-id-' + fake_uuid,
            'name': 'network-name-' + fake_uuid,
            'status': 'ACTIVE',
            'tenant_id': 'project-id-' + fake_uuid,
            'admin_state_up': True,
            'shared': False,
            'subnets': [],
            'provider:network_type': 'geneve',
            'provider:physical_network': None,
            'provider:segmentation_id': 10,
            'router:external': False,
            'availability_zones': [],
            'availability_zone_hints': [],
            'is_default': False,
        }

        # Overwrite default attributes.
        network_attrs.update(attrs)

        return FakeResource(info=copy.deepcopy(network_attrs),
                            loaded=True)


class FakeNetworkContext(object):
    def __init__(self, network, segments):
        self.fake_network = network
        self.fake_segments = segments
        self._plugin_context = mock.MagicMock()

    @property
    def current(self):
        return self.fake_network

    @property
    def original(self):
        return None

    @property
    def network_segments(self):
        return self.fake_segments


class FakeSubnetContext(object):
    def __init__(self, subnet, original_subnet=None, network=None):
        self.fake_subnet = subnet
        self.fake_original_subnet = original_subnet
        self.fake_network = FakeNetworkContext(network, None)

    @property
    def current(self):
        return self.fake_subnet

    @property
    def original(self):
        return self.fake_original_subnet

    @property
    def network(self):
        return self.fake_network


class FakeOvsdbRow(FakeResource):
    """Fake one or more OVSDB rows."""

    @staticmethod
    def create_one_ovsdb_row(attrs=None, methods=None):
        """Create a fake OVSDB row.

        :param Dictionary attrs:
            A dictionary with all attributes
        :param Dictionary methods:
            A dictionary with all methods
        :return:
            A FakeResource object faking the OVSDB row
        """
        attrs = attrs or {}
        methods = methods or {}

        # Set default attributes.
        fake_uuid = uuidutils.generate_uuid()
        ovsdb_row_attrs = {
            'uuid': fake_uuid,
            'name': 'name-' + fake_uuid,
            'external_ids': {},
        }

        # Set default methods.
        ovsdb_row_methods = {
            'addvalue': None,
            'delete': None,
            'delvalue': None,
            'verify': None,
        }

        # Overwrite default attributes and methods.
        ovsdb_row_attrs.update(attrs)
        ovsdb_row_methods.update(methods)

        return FakeResource(info=copy.deepcopy(ovsdb_row_attrs),
                            loaded=True,
                            methods=copy.deepcopy(ovsdb_row_methods))


class FakeOvsdbTable(FakeResource):
    """Fake one or more OVSDB tables."""

    @staticmethod
    def create_one_ovsdb_table(attrs=None):
        """Create a fake OVSDB table.

        :param Dictionary attrs:
            A dictionary with all attributes
        :return:
            A FakeResource object faking the OVSDB table
        """
        attrs = attrs or {}

        # Set default attributes.
        ovsdb_table_attrs = {
            'rows': {},
            'columns': {},
        }

        # Overwrite default attributes.
        ovsdb_table_attrs.update(attrs)

        return FakeResource(info=copy.deepcopy(ovsdb_table_attrs),
                            loaded=True)


class FakePort(object):
    """Fake one or more ports."""

    @staticmethod
    def create_one_port(attrs=None):
        """Create a fake port.

        :param Dictionary attrs:
            A dictionary with all attributes
        :return:
            A FakeResource object faking the port
        """
        attrs = attrs or {}

        # Set default attributes.
        fake_uuid = uuidutils.generate_uuid()
        port_attrs = {
            'admin_state_up': True,
            'allowed_address_pairs': [{}],
            'binding:host_id': 'binding-host-id-' + fake_uuid,
            'binding:profile': {},
            'binding:vif_details': {},
            'binding:vif_type': 'ovs',
            'binding:vnic_type': 'normal',
            'device_id': 'device-id-' + fake_uuid,
            'device_owner': 'compute:nova',
            'dns_assignment': [{}],
            'dns_name': 'dns-name-' + fake_uuid,
            'extra_dhcp_opts': [{}],
            'fixed_ips': [{'subnet_id': 'subnet-id-' + fake_uuid,
                           'ip_address': '10.10.10.20'}],
            'id': 'port-id-' + fake_uuid,
            'mac_address': 'fa:16:3e:a9:4e:72',
            'name': 'port-name-' + fake_uuid,
            'network_id': 'network-id-' + fake_uuid,
            'port_security_enabled': True,
            'security_groups': [],
            'status': 'ACTIVE',
            'tenant_id': 'project-id-' + fake_uuid,
        }

        # Overwrite default attributes.
        port_attrs.update(attrs)

        return FakeResource(info=copy.deepcopy(port_attrs),
                            loaded=True)


class FakePortContext(object):
    def __init__(self, port, host, segments_to_bind):
        self.fake_port = port
        self.fake_host = host
        self.fake_segments_to_bind = segments_to_bind
        self.set_binding = mock.Mock()

    @property
    def current(self):
        return self.fake_port

    @property
    def host(self):
        return self.fake_host

    @property
    def segments_to_bind(self):
        return self.fake_segments_to_bind


class FakeSecurityGroup(object):
    """Fake one or more security groups."""

    @staticmethod
    def create_one_security_group(attrs=None):
        """Create a fake security group.

        :param Dictionary attrs:
            A dictionary with all attributes
        :return:
            A FakeResource object faking the security group
        """
        attrs = attrs or {}

        # Set default attributes.
        fake_uuid = uuidutils.generate_uuid()
        security_group_attrs = {
            'id': 'security-group-id-' + fake_uuid,
            'name': 'security-group-name-' + fake_uuid,
            'description': 'security-group-description-' + fake_uuid,
            'tenant_id': 'project-id-' + fake_uuid,
            'security_group_rules': [],
        }

        # Overwrite default attributes.
        security_group_attrs.update(attrs)

        return FakeResource(info=copy.deepcopy(security_group_attrs),
                            loaded=True)


class FakeSecurityGroupRule(object):
    """Fake one or more security group rules."""

    @staticmethod
    def create_one_security_group_rule(attrs=None):
        """Create a fake security group rule.

        :param Dictionary attrs:
            A dictionary with all attributes
        :return:
            A FakeResource object faking the security group rule
        """
        attrs = attrs or {}

        # Set default attributes.
        fake_uuid = uuidutils.generate_uuid()
        security_group_rule_attrs = {
            'direction': 'ingress',
            'ethertype': 'IPv4',
            'id': 'security-group-rule-id-' + fake_uuid,
            'port_range_max': 22,
            'port_range_min': 22,
            'protocol': 'tcp',
            'remote_group_id': None,
            'remote_ip_prefix': '0.0.0.0/0',
            'security_group_id': 'security-group-id-' + fake_uuid,
            'tenant_id': 'project-id-' + fake_uuid,
        }

        # Overwrite default attributes.
        security_group_rule_attrs.update(attrs)

        return FakeResource(info=copy.deepcopy(security_group_rule_attrs),
                            loaded=True)


class FakeSegment(object):
    """Fake one or more segments."""

    @staticmethod
    def create_one_segment(attrs=None):
        """Create a fake segment.

        :param Dictionary attrs:
            A dictionary with all attributes
        :return:
            A FakeResource object faking the segment
        """
        attrs = attrs or {}

        # Set default attributes.
        fake_uuid = uuidutils.generate_uuid()
        segment_attrs = {
            'id': 'segment-id-' + fake_uuid,
            'network_type': 'geneve',
            'physical_network': None,
            'segmentation_id': 10,
        }

        # Overwrite default attributes.
        segment_attrs.update(attrs)

        return FakeResource(info=copy.deepcopy(segment_attrs),
                            loaded=True)


class FakeSubnet(object):
    """Fake one or more subnets."""

    @staticmethod
    def create_one_subnet(attrs=None):
        """Create a fake subnet.

        :param Dictionary attrs:
            A dictionary with all attributes
        :return:
            A FakeResource object faking the subnet
        """
        attrs = attrs or {}

        # Set default attributes.
        fake_uuid = uuidutils.generate_uuid()
        subnet_attrs = {
            'id': 'subnet-id-' + fake_uuid,
            'name': 'subnet-name-' + fake_uuid,
            'network_id': 'network-id-' + fake_uuid,
            'cidr': '10.10.10.0/24',
            'tenant_id': 'project-id-' + fake_uuid,
            'enable_dhcp': True,
            'dns_nameservers': [],
            'allocation_pools': [],
            'host_routes': [],
            'ip_version': 4,
            'gateway_ip': '10.10.10.1',
            'ipv6_address_mode': 'None',
            'ipv6_ra_mode': 'None',
            'subnetpool_id': None,
        }

        # Overwrite default attributes.
        subnet_attrs.update(attrs)

        return FakeResource(info=copy.deepcopy(subnet_attrs),
                            loaded=True)


class FakeFloatingIp(object):
    """Fake one or more floating ips."""

    @staticmethod
    def create_one_fip(attrs=None):
        """Create a fake floating ip.

        :param Dictionary attrs:
            A dictionary with all attributes
        :return:
            A FakeResource object faking the floating ip
        """
        attrs = attrs or {}

        # Set default attributes.
        fake_uuid = uuidutils.generate_uuid()
        fip_attrs = {
            'id': 'fip-id-' + fake_uuid,
            'tenant_id': '',
            'fixed_ip_address': '10.0.0.10',
            'floating_ip_address': '172.21.0.100',
            'router_id': 'router-id',
            'port_id': 'port_id',
            'fixed_port_id': 'port_id',
            'floating_port_id': 'fip-port-id',
            'status': 'Active',
            'floating_network_id': 'fip-net-id',
            'dns': '',
            'dns_domain': '',
            'dns_name': '',
            'project_id': '',
        }

        # Overwrite default attributes.
        fip_attrs.update(attrs)

        return FakeResource(info=copy.deepcopy(fip_attrs),
                            loaded=True)


class FakeOVNPort(object):
    """Fake one or more ports."""

    @staticmethod
    def create_one_port(attrs=None):
        """Create a fake ovn port.

        :param Dictionary attrs:
            A dictionary with all attributes
        :return:
            A FakeResource object faking the port
        """
        attrs = attrs or {}

        # Set default attributes.
        fake_uuid = uuidutils.generate_uuid()
        port_attrs = {
            'addresses': [],
            'dhcpv4_options': '',
            'dhcpv6_options': [],
            'enabled': True,
            'external_ids': {},
            'name': fake_uuid,
            'options': {},
            'parent_name': [],
            'port_security': [],
            'tag': [],
            'tag_request': [],
            'type': '',
            'up': False,
        }

        # Overwrite default attributes.
        port_attrs.update(attrs)
        return type('Logical_Switch_Port', (object, ), port_attrs)

    @staticmethod
    def from_neutron_port(port):
        """Create a fake ovn port based on a neutron port."""
        external_ids = {
            ovn_const.OVN_NETWORK_NAME_EXT_ID_KEY:
                utils.ovn_name(port['network_id']),
            ovn_const.OVN_SG_IDS_EXT_ID_KEY:
                ' '.join(port['security_groups']),
            ovn_const.OVN_DEVICE_OWNER_EXT_ID_KEY:
                port.get('device_owner', '')}
        addresses = [port['mac_address'], ]
        addresses += [x['ip_address'] for x in port.get('fixed_ips', [])]
        port_security = (
            addresses + [x['ip_address'] for x in
                         port.get('allowed_address_pairs', [])])
        return FakeOVNPort.create_one_port(
            {'external_ids': external_ids, 'addresses': addresses,
             'port_security': port_security})


FakeStaticRoute = collections.namedtuple(
    'Static_Routes', ['ip_prefix', 'nexthop', 'external_ids'])


class FakeOVNRouter(object):

    @staticmethod
    def create_one_router(attrs=None):
        router_attrs = {
            'enabled': False,
            'external_ids': {},
            'load_balancer': [],
            'name': '',
            'nat': [],
            'options': {},
            'ports': [],
            'static_routes': [],
        }

        # Overwrite default attributes.
        router_attrs.update(attrs)
        return type('Logical_Router', (object, ), router_attrs)

    @staticmethod
    def from_neutron_router(router):

        def _get_subnet_id(gw_info):
            subnet_id = ''
            ext_ips = gw_info.get('external_fixed_ips', [])
            if ext_ips:
                subnet_id = ext_ips[0]['subnet_id']
            return subnet_id

        external_ids = {
            ovn_const.OVN_GW_PORT_EXT_ID_KEY: router.get('gw_port_id') or '',
            ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY:
                router.get('name', 'no_router_name')}

        # Get the routes
        routes = []
        for r in router.get('routes', []):
            routes.append(FakeStaticRoute(ip_prefix=r['destination'],
                                          nexthop=r['nexthop'],
                                          external_ids={}))

        gw_info = router.get(l3.EXTERNAL_GW_INFO)
        if gw_info:
            external_ids = {
                ovn_const.OVN_ROUTER_IS_EXT_GW: 'true',
                ovn_const.OVN_SUBNET_EXT_ID_KEY: _get_subnet_id(gw_info)}
            routes.append(FakeStaticRoute(
                ip_prefix='0.0.0.0/0', nexthop='',
                external_ids=external_ids))

        return FakeOVNRouter.create_one_router(
            {'external_ids': external_ids,
             'enabled': router.get('admin_state_up') or False,
             'name': utils.ovn_name(router['id']),
             'static_routes': routes})




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\test_ovn_db_sync.py
===========File Type===========
.py
===========File Content===========
# Copyright 2016 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import collections

import mock

from networking_ovn.common import constants as ovn_const
from networking_ovn.common import ovn_client
from networking_ovn import ovn_db_sync
from networking_ovn.tests.unit.ml2 import test_mech_driver


OvnPortInfo = collections.namedtuple('OvnPortInfo', ['name'])


@mock.patch('networking_ovn.l3.l3_ovn.OVNL3RouterPlugin._sb_ovn', mock.Mock())
class TestOvnNbSyncML2(test_mech_driver.OVNMechanismDriverTestCase):

    l3_plugin = 'networking_ovn.l3.l3_ovn.OVNL3RouterPlugin'

    def setUp(self):
        super(TestOvnNbSyncML2, self).setUp()

        self.subnet = {'cidr': '10.0.0.0/24',
                       'id': 'subnet1',
                       'subnetpool_id': None,
                       'name': 'private-subnet',
                       'enable_dhcp': True,
                       'network_id': 'n1',
                       'tenant_id': 'tenant1',
                       'gateway_ip': '10.0.0.1',
                       'ip_version': 4,
                       'shared': False}
        self.matches = ["", "", "", ""]

        self.networks = [{'id': 'n1',
                          'mtu': 1450,
                          'provider:physical_network': 'physnet1',
                          'provider:segmentation_id': 1000},
                         {'id': 'n2',
                          'mtu': 1450},
                         {'id': 'n4',
                          'mtu': 1450,
                          'provider:physical_network': 'physnet2'}]

        self.subnets = [{'id': 'n1-s1',
                         'network_id': 'n1',
                         'enable_dhcp': True,
                         'cidr': '10.0.0.0/24',
                         'tenant_id': 'tenant1',
                         'gateway_ip': '10.0.0.1',
                         'dns_nameservers': [],
                         'host_routes': [],
                         'ip_version': 4},
                        {'id': 'n1-s2',
                         'network_id': 'n1',
                         'enable_dhcp': True,
                         'cidr': 'fd79:e1c:a55::/64',
                         'tenant_id': 'tenant1',
                         'gateway_ip': 'fd79:e1c:a55::1',
                         'dns_nameservers': [],
                         'host_routes': [],
                         'ip_version': 6},
                        {'id': 'n2',
                         'network_id': 'n2',
                         'enable_dhcp': True,
                         'cidr': '20.0.0.0/24',
                         'tenant_id': 'tenant1',
                         'gateway_ip': '20.0.0.1',
                         'dns_nameservers': [],
                         'host_routes': [],
                         'ip_version': 4}]

        self.security_groups = [
            {'id': 'sg1', 'tenant_id': 'tenant1',
             'security_group_rules': [{'remote_group_id': None,
                                       'direction': 'ingress',
                                       'remote_ip_prefix': '0.0.0.0/0',
                                       'protocol': 'tcp',
                                       'ethertype': 'IPv4',
                                       'tenant_id': 'tenant1',
                                       'port_range_max': 65535,
                                       'port_range_min': 1,
                                       'id': 'ruleid1',
                                       'security_group_id': 'sg1'}],
             'name': 'all-tcp'},
            {'id': 'sg2', 'tenant_id': 'tenant1',
             'security_group_rules': [{'remote_group_id': 'sg2',
                                       'direction': 'egress',
                                       'remote_ip_prefix': '0.0.0.0/0',
                                       'protocol': 'tcp',
                                       'ethertype': 'IPv4',
                                       'tenant_id': 'tenant1',
                                       'port_range_max': 65535,
                                       'port_range_min': 1,
                                       'id': 'ruleid1',
                                       'security_group_id': 'sg2'}],
             'name': 'all-tcpe'}]

        self.port_groups_ovn = [mock.Mock(), mock.Mock(), mock.Mock()]
        self.port_groups_ovn[0].configure_mock(
            name='pg_sg1',
            external_ids={ovn_const.OVN_SG_EXT_ID_KEY: 'sg1'},
            ports=[],
            acls=[])
        self.port_groups_ovn[1].configure_mock(
            name='pg_unknown_del',
            external_ids={ovn_const.OVN_SG_EXT_ID_KEY: 'sg2'},
            ports=[],
            acls=[])
        self.port_groups_ovn[2].configure_mock(
            name='neutron_pg_drop',
            external_ids=[],
            ports=[],
            acls=[])

        self.ports = [
            {'id': 'p1n1',
             'device_owner': 'compute:None',
             'fixed_ips':
                 [{'subnet_id': 'b142f5e3-d434-4740-8e88-75e8e5322a40',
                   'ip_address': '10.0.0.4'},
                  {'subnet_id': 'subnet1',
                   'ip_address': 'fd79:e1c:a55::816:eff:eff:ff2'}],
             'security_groups': ['sg1'],
             'network_id': 'n1'},
            {'id': 'p2n1',
             'device_owner': 'compute:None',
             'fixed_ips':
                 [{'subnet_id': 'b142f5e3-d434-4740-8e88-75e8e5322a40',
                   'ip_address': '10.0.0.4'},
                  {'subnet_id': 'subnet1',
                   'ip_address': 'fd79:e1c:a55::816:eff:eff:ff2'}],
             'security_groups': ['sg2'],
             'network_id': 'n1',
             'extra_dhcp_opts': [{'ip_version': 6,
                                  'opt_name': 'domain-search',
                                  'opt_value': 'foo-domain'}]},
            {'id': 'p1n2',
             'device_owner': 'compute:None',
             'fixed_ips':
                 [{'subnet_id': 'b142f5e3-d434-4740-8e88-75e8e5322a40',
                   'ip_address': '10.0.0.4'},
                  {'subnet_id': 'subnet1',
                   'ip_address': 'fd79:e1c:a55::816:eff:eff:ff2'}],
             'security_groups': ['sg1'],
             'network_id': 'n2',
             'extra_dhcp_opts': [{'ip_version': 4,
                                  'opt_name': 'tftp-server',
                                  'opt_value': '20.0.0.20'},
                                 {'ip_version': 4,
                                  'opt_name': 'dns-server',
                                  'opt_value': '8.8.8.8'},
                                 {'ip_version': 6,
                                  'opt_name': 'domain-search',
                                  'opt_value': 'foo-domain'}]},
            {'id': 'p2n2',
             'device_owner': 'compute:None',
             'fixed_ips':
                 [{'subnet_id': 'b142f5e3-d434-4740-8e88-75e8e5322a40',
                   'ip_address': '10.0.0.4'},
                  {'subnet_id': 'subnet1',
                   'ip_address': 'fd79:e1c:a55::816:eff:eff:ff2'}],
             'security_groups': ['sg2'],
             'network_id': 'n2'},
            {'id': 'fp1',
             'device_owner': 'network:floatingip',
             'fixed_ips':
                 [{'subnet_id': 'ext-subnet',
                   'ip_address': '90.0.0.10'}],
             'network_id': 'ext-net'}]

        self.ports_ovn = [OvnPortInfo('p1n1'), OvnPortInfo('p1n2'),
                          OvnPortInfo('p2n1'), OvnPortInfo('p2n2'),
                          OvnPortInfo('p3n1'), OvnPortInfo('p3n3')]

        self.acls_ovn = {
            'lport1':
            # ACLs need to be removed by the sync tool
            [{'id': 'acl1', 'priority': 00, 'policy': 'allow',
              'lswitch': 'lswitch1', 'lport': 'lport1'}],
            'lport2':
            [{'id': 'acl2', 'priority': 00, 'policy': 'drop',
             'lswitch': 'lswitch2', 'lport': 'lport2'}],
            # ACLs need to be kept as-is by the sync tool
            'p2n2':
            [{'lport': 'p2n2', 'direction': 'to-lport',
              'log': False, 'lswitch': 'neutron-n2',
              'priority': 1001, 'action': 'drop',
             'external_ids': {'neutron:lport': 'p2n2'},
              'match': 'outport == "p2n2" && ip'},
             {'lport': 'p2n2', 'direction': 'to-lport',
              'log': False, 'lswitch': 'neutron-n2',
              'priority': 1002, 'action': 'allow',
              'external_ids': {'neutron:lport': 'p2n2'},
              'match': 'outport == "p2n2" && ip4 && '
              'ip4.src == 10.0.0.0/24 && udp && '
              'udp.src == 67 && udp.dst == 68'}]}
        self.address_sets_ovn = {
            'as_ip4_sg1': {'external_ids': {ovn_const.OVN_SG_EXT_ID_KEY:
                                            'all-tcp'},
                           'name': 'as_ip4_sg1',
                           'addresses': ['10.0.0.4']},
            'as_ip4_sg2': {'external_ids': {ovn_const.OVN_SG_EXT_ID_KEY:
                                            'all-tcpe'},
                           'name': 'as_ip4_sg2',
                           'addresses': []},
            'as_ip6_sg2': {'external_ids': {ovn_const.OVN_SG_EXT_ID_KEY:
                                            'all-tcpe'},
                           'name': 'as_ip6_sg2',
                           'addresses': ['fd79:e1c:a55::816:eff:eff:ff2',
                                         'fd79:e1c:a55::816:eff:eff:ff3']},
            'as_ip4_del': {'external_ids': {ovn_const.OVN_SG_EXT_ID_KEY:
                                            'all-delete'},
                           'name': 'as_ip4_delete',
                           'addresses': ['10.0.0.4']},
            }

        self.routers = [{'id': 'r1', 'routes': [{'nexthop': '20.0.0.100',
                         'destination': '11.0.0.0/24'}, {
                         'nexthop': '20.0.0.101',
                         'destination': '12.0.0.0/24'}],
                         'gw_port_id': 'gpr1',
                         'external_gateway_info': {
                             'network_id': "ext-net", 'enable_snat': True,
                             'external_fixed_ips': [
                                 {'subnet_id': 'ext-subnet',
                                  'ip_address': '90.0.0.2'}]}},
                        {'id': 'r2', 'routes': [{'nexthop': '40.0.0.100',
                         'destination': '30.0.0.0/24'}],
                         'gw_port_id': 'gpr2',
                         'external_gateway_info': {
                             'network_id': "ext-net", 'enable_snat': True,
                             'external_fixed_ips': [
                                 {'subnet_id': 'ext-subnet',
                                  'ip_address': '100.0.0.2'}]}},
                        {'id': 'r4', 'routes': []}]

        self.get_sync_router_ports = [
            {'fixed_ips': [{'subnet_id': 'subnet1',
                            'ip_address': '192.168.1.1'}],
             'id': 'p1r1',
             'device_id': 'r1',
             'mac_address': 'fa:16:3e:d7:fd:5f'},
            {'fixed_ips': [{'subnet_id': 'subnet2',
                            'ip_address': '192.168.2.1'}],
             'id': 'p1r2',
             'device_id': 'r2',
             'mac_address': 'fa:16:3e:d6:8b:ce'},
            {'fixed_ips': [{'subnet_id': 'subnet4',
                            'ip_address': '192.168.4.1'}],
             'id': 'p1r4',
             'device_id': 'r4',
             'mac_address': 'fa:16:3e:12:34:56'}]

        self.floating_ips = [{'id': 'fip1', 'router_id': 'r1',
                              'floating_ip_address': '90.0.0.10',
                              'fixed_ip_address': '172.16.0.10'},
                             {'id': 'fip2', 'router_id': 'r1',
                              'floating_ip_address': '90.0.0.12',
                              'fixed_ip_address': '172.16.2.12'},
                             {'id': 'fip3', 'router_id': 'r2',
                              'floating_ip_address': '100.0.0.10',
                              'fixed_ip_address': '192.168.2.10'},
                             {'id': 'fip4', 'router_id': 'r2',
                              'floating_ip_address': '100.0.0.11',
                              'fixed_ip_address': '192.168.2.11'}]

        self.lrouters_with_rports = [{'name': 'r3',
                                      'ports': {'p1r3': ['fake']},
                                      'static_routes': [],
                                      'snats': [],
                                      'dnat_and_snats': []},
                                     {'name': 'r4',
                                      'ports': {'p1r4':
                                                ['fdad:123:456::1/64',
                                                 'fdad:789:abc::1/64']},
                                      'static_routes': [],
                                      'snats': [],
                                      'dnat_and_snats': []},
                                     {'name': 'r1',
                                      'ports': {'p3r1': ['fake']},
                                      'static_routes':
                                      [{'nexthop': '20.0.0.100',
                                        'destination': '11.0.0.0/24'},
                                       {'nexthop': '20.0.0.100',
                                        'destination': '10.0.0.0/24'}],
                                      'snats':
                                      [{'logical_ip': '172.16.0.0/24',
                                        'external_ip': '90.0.0.2',
                                        'type': 'snat'},
                                       {'logical_ip': '172.16.1.0/24',
                                        'external_ip': '90.0.0.2',
                                        'type': 'snat'}],
                                      'dnat_and_snats':
                                      [{'logical_ip': '172.16.0.10',
                                        'external_ip': '90.0.0.10',
                                        'type': 'dnat_and_snat'},
                                       {'logical_ip': '172.16.1.11',
                                        'external_ip': '90.0.0.11',
                                        'type': 'dnat_and_snat'},
                                       {'logical_ip': '192.168.2.11',
                                        'external_ip': '100.0.0.11',
                                        'type': 'dnat_and_snat',
                                        'external_mac': '01:02:03:04:05:06',
                                        'logical_port': 'vm1'}]}]

        self.lswitches_with_ports = [{'name': 'neutron-n1',
                                      'ports': ['p1n1', 'p3n1'],
                                      'provnet_port': None},
                                     {'name': 'neutron-n3',
                                      'ports': ['p1n3', 'p2n3'],
                                      'provnet_port': None},
                                     {'name': 'neutron-n4',
                                      'ports': [],
                                      'provnet_port': 'provnet-n4'}]

        self.lrport_networks = ['fdad:123:456::1/64', 'fdad:cafe:a1b2::1/64']

    def _fake_get_ovn_dhcp_options(self, subnet, network, server_mac=None):
        if subnet['id'] == 'n1-s1':
            return {'cidr': '10.0.0.0/24',
                    'options': {'server_id': '10.0.0.1',
                                'server_mac': '01:02:03:04:05:06',
                                'lease_time': str(12 * 60 * 60),
                                'mtu': '1450',
                                'router': '10.0.0.1'},
                    'external_ids': {'subnet_id': 'n1-s1'}}
        return {'cidr': '', 'options': '', 'external_ids': {}}

    def _fake_get_gw_info(self, ctx, router):
        return {
            'r1': ovn_client.GW_INFO(router_ip='90.0.0.2',
                                     gateway_ip='90.0.0.1',
                                     network_id='', subnet_id=''),
            'r2': ovn_client.GW_INFO(router_ip='100.0.0.2',
                                     gateway_ip='100.0.0.1',
                                     network_id='', subnet_id='')
        }.get(router['id'], ovn_client.GW_INFO('', '', '', ''))

    def _fake_get_v4_network_of_all_router_ports(self, ctx, router_id):
        return {'r1': ['172.16.0.0/24', '172.16.2.0/24'],
                'r2': ['192.168.2.0/24']}.get(router_id, [])

    def _test_mocks_helper(self, ovn_nb_synchronizer):
        core_plugin = ovn_nb_synchronizer.core_plugin
        ovn_api = ovn_nb_synchronizer.ovn_api
        ovn_driver = ovn_nb_synchronizer.ovn_driver
        l3_plugin = ovn_nb_synchronizer.l3_plugin

        core_plugin.get_networks = mock.Mock()
        core_plugin.get_networks.return_value = self.networks
        core_plugin.get_subnets = mock.Mock()
        core_plugin.get_subnets.return_value = self.subnets
        # following block is used for acl syncing unit-test

        # With the given set of values in the unit testing,
        # 19 neutron acls should have been there,
        # 4 acls are returned as current ovn acls,
        # two of which will match with neutron.
        # So, in this example 17 will be added, 2 removed
        core_plugin.get_ports = mock.Mock()
        core_plugin.get_ports.return_value = self.ports
        mock.patch(
            "networking_ovn.common.acl._get_subnet_from_cache",
            return_value=self.subnet
        ).start()
        mock.patch(
            "networking_ovn.common.acl.acl_remote_group_id",
            side_effect=self.matches
        ).start()
        core_plugin.get_security_group = mock.MagicMock(
            side_effect=self.security_groups)
        ovn_nb_synchronizer.get_acls = mock.Mock()
        ovn_nb_synchronizer.get_acls.return_value = self.acls_ovn
        core_plugin.get_security_groups = mock.MagicMock(
            return_value=self.security_groups)
        ovn_nb_synchronizer.get_address_sets = mock.Mock()
        ovn_nb_synchronizer.get_address_sets.return_value =\
            self.address_sets_ovn
        get_port_groups = mock.MagicMock()
        get_port_groups.execute.return_value = self.port_groups_ovn
        ovn_api.db_list_rows.return_value = get_port_groups
        ovn_api.lsp_list.execute.return_value = self.ports_ovn
        # end of acl-sync block

        # The following block is used for router and router port syncing tests
        # With the give set of values in the unit test,
        # The Neutron db has Routers r1 and r2 present.
        # The OVN db has Routers r1 and r3 present.
        # During the sync r2 will need to be created and r3 will need
        # to be deleted from the OVN db. When Router r3 is deleted, all LRouter
        # ports associated with r3 is deleted too.
        #
        # Neutron db has Router ports p1r1 in Router r1 and p1r2 in Router r2
        # OVN db has p1r3 in Router 3 and p3r1 in Router 1.
        # During the sync p1r1 and p1r2 will be added and p1r3 and p3r1
        # will be deleted from the OVN db
        l3_plugin.get_routers = mock.Mock()
        l3_plugin.get_routers.return_value = self.routers
        l3_plugin._get_sync_interfaces = mock.Mock()
        l3_plugin._get_sync_interfaces.return_value = (
            self.get_sync_router_ports)
        ovn_nb_synchronizer._ovn_client = mock.Mock()
        ovn_nb_synchronizer._ovn_client.\
            _get_nets_and_ipv6_ra_confs_for_router_port.return_value = (
                self.lrport_networks, {})
        ovn_nb_synchronizer._ovn_client._get_v4_network_of_all_router_ports. \
            side_effect = self._fake_get_v4_network_of_all_router_ports
        ovn_nb_synchronizer._ovn_client._get_gw_info = mock.Mock()
        ovn_nb_synchronizer._ovn_client._get_gw_info.side_effect = (
            self._fake_get_gw_info)
        # end of router-sync block
        l3_plugin.get_floatingips = mock.Mock()
        l3_plugin.get_floatingips.return_value = self.floating_ips
        ovn_api.get_all_logical_switches_with_ports = mock.Mock()
        ovn_api.get_all_logical_switches_with_ports.return_value = (
            self.lswitches_with_ports)

        ovn_api.get_all_logical_routers_with_rports = mock.Mock()
        ovn_api.get_all_logical_routers_with_rports.return_value = (
            self.lrouters_with_rports)

        ovn_api.transaction = mock.MagicMock()

        ovn_nb_synchronizer._ovn_client.create_network = mock.Mock()
        ovn_nb_synchronizer._ovn_client.create_port = mock.Mock()
        ovn_driver.validate_and_get_data_from_binding_profile = mock.Mock()
        ovn_nb_synchronizer._ovn_client.create_port = mock.Mock()
        ovn_nb_synchronizer._ovn_client.create_port.return_value = mock.ANY
        ovn_nb_synchronizer._ovn_client._create_provnet_port = mock.Mock()
        ovn_api.ls_del = mock.Mock()
        ovn_api.delete_lswitch_port = mock.Mock()

        ovn_api.delete_lrouter = mock.Mock()
        ovn_api.delete_lrouter_port = mock.Mock()
        ovn_api.add_static_route = mock.Mock()
        ovn_api.delete_static_route = mock.Mock()
        ovn_api.get_all_dhcp_options.return_value = {
            'subnets': {'n1-s1': {'cidr': '10.0.0.0/24',
                                  'options':
                                  {'server_id': '10.0.0.1',
                                   'server_mac': '01:02:03:04:05:06',
                                   'lease_time': str(12 * 60 * 60),
                                   'mtu': '1450',
                                   'router': '10.0.0.1'},
                                  'external_ids': {'subnet_id': 'n1-s1'},
                                  'uuid': 'UUID1'},
                        'n1-s3': {'cidr': '30.0.0.0/24',
                                  'options':
                                  {'server_id': '30.0.0.1',
                                   'server_mac': '01:02:03:04:05:06',
                                   'lease_time': str(12 * 60 * 60),
                                   'mtu': '1450',
                                   'router': '30.0.0.1'},
                                  'external_ids': {'subnet_id': 'n1-s3'},
                                  'uuid': 'UUID2'}},
            'ports_v4': {'p1n2': {'cidr': '10.0.0.0/24',
                                  'options': {'server_id': '10.0.0.1',
                                              'server_mac':
                                                  '01:02:03:04:05:06',
                                              'lease_time': '1000',
                                              'mtu': '1400',
                                              'router': '10.0.0.1'},
                                  'external_ids': {'subnet_id': 'n1-s1',
                                                   'port_id': 'p1n2'},
                                  'uuid': 'UUID3'},
                         'p5n2': {'cidr': '10.0.0.0/24',
                                  'options': {'server_id': '10.0.0.1',
                                              'server_mac':
                                                  '01:02:03:04:05:06',
                                              'lease_time': '1000',
                                              'mtu': '1400',
                                              'router': '10.0.0.1'},
                                  'external_ids': {'subnet_id': 'n1-s1',
                                                   'port_id': 'p5n2'},
                                  'uuid': 'UUID4'}},
            'ports_v6': {'p1n1': {'cidr': 'fd79:e1c:a55::/64',
                                  'options': {'server_id': '01:02:03:04:05:06',
                                              'mtu': '1450'},
                                  'external_ids': {'subnet_id': 'fake',
                                                   'port_id': 'p1n1'},
                                  'uuid': 'UUID5'},
                         'p1n2': {'cidr': 'fd79:e1c:a55::/64',
                                  'options': {'server_id': '01:02:03:04:05:06',
                                              'mtu': '1450'},
                                  'external_ids': {'subnet_id': 'fake',
                                                   'port_id': 'p1n2'},
                                  'uuid': 'UUID6'}}}

        ovn_api.create_address_set = mock.Mock()
        ovn_api.delete_address_set = mock.Mock()
        ovn_api.update_address_set = mock.Mock()
        ovn_nb_synchronizer._ovn_client._add_subnet_dhcp_options = mock.Mock()
        ovn_nb_synchronizer._ovn_client._get_ovn_dhcp_options = mock.Mock()
        ovn_nb_synchronizer._ovn_client._get_ovn_dhcp_options.side_effect = (
            self._fake_get_ovn_dhcp_options)
        ovn_api.delete_dhcp_options = mock.Mock()
        ovn_nb_synchronizer._ovn_client.get_port_dns_records = mock.Mock()
        ovn_nb_synchronizer._ovn_client.get_port_dns_records.return_value = {}

    def _test_ovn_nb_sync_helper(self, ovn_nb_synchronizer,
                                 networks, ports,
                                 routers, router_ports,
                                 create_router_list, create_router_port_list,
                                 update_router_port_list,
                                 del_router_list, del_router_port_list,
                                 create_network_list, create_port_list,
                                 create_provnet_port_list,
                                 del_network_list, del_port_list,
                                 add_static_route_list, del_static_route_list,
                                 add_snat_list, del_snat_list,
                                 add_floating_ip_list, del_floating_ip_list,
                                 add_address_set_list, del_address_set_list,
                                 update_address_set_list,
                                 add_subnet_dhcp_options_list,
                                 delete_dhcp_options_list,
                                 add_port_groups_list,
                                 del_port_groups_list,
                                 port_groups_supported=False):
        self._test_mocks_helper(ovn_nb_synchronizer)

        core_plugin = ovn_nb_synchronizer.core_plugin
        ovn_api = ovn_nb_synchronizer.ovn_api
        ovn_api.is_port_groups_supported.return_value = port_groups_supported
        mock.patch("networking_ovn.ovsdb.impl_idl_ovn.get_connection").start()

        ovn_nb_synchronizer.do_sync()

        if not ovn_api.is_port_groups_supported():
            get_security_group_calls = [mock.call(mock.ANY, sg['id'])
                                        for sg in self.security_groups]
            self.assertEqual(len(self.security_groups),
                             core_plugin.get_security_group.call_count)
            core_plugin.get_security_group.assert_has_calls(
                get_security_group_calls, any_order=True)

        create_address_set_calls = [mock.call(**a)
                                    for a in add_address_set_list]
        self.assertEqual(
            len(add_address_set_list),
            ovn_api.create_address_set.call_count)
        ovn_api.create_address_set.assert_has_calls(
            create_address_set_calls, any_order=True)

        del_address_set_calls = [mock.call(**d)
                                 for d in del_address_set_list]
        self.assertEqual(
            len(del_address_set_list),
            ovn_api.delete_address_set.call_count)
        ovn_api.delete_address_set.assert_has_calls(
            del_address_set_calls, any_order=True)

        update_address_set_calls = [mock.call(**u)
                                    for u in update_address_set_list]
        self.assertEqual(
            len(update_address_set_list),
            ovn_api.update_address_set.call_count)
        ovn_api.update_address_set.assert_has_calls(
            update_address_set_calls, any_order=True)

        create_port_groups_calls = [mock.call(**a)
                                    for a in add_port_groups_list]
        self.assertEqual(
            len(add_port_groups_list),
            ovn_api.pg_add.call_count)
        ovn_api.pg_add.assert_has_calls(
            create_port_groups_calls, any_order=True)

        del_port_groups_calls = [mock.call(d)
                                 for d in del_port_groups_list]
        self.assertEqual(
            len(del_port_groups_list),
            ovn_api.pg_del.call_count)
        ovn_api.pg_del.assert_has_calls(
            del_port_groups_calls, any_order=True)

        self.assertEqual(
            len(create_network_list),
            ovn_nb_synchronizer._ovn_client.create_network.call_count)
        create_network_calls = [mock.call(net['net'])
                                for net in create_network_list]
        ovn_nb_synchronizer._ovn_client.create_network.assert_has_calls(
            create_network_calls, any_order=True)

        self.assertEqual(
            len(create_port_list),
            ovn_nb_synchronizer._ovn_client.create_port.call_count)
        create_port_calls = [mock.call(port) for port in create_port_list]
        ovn_nb_synchronizer._ovn_client.create_port.assert_has_calls(
            create_port_calls, any_order=True)

        create_provnet_port_calls = [
            mock.call(mock.ANY, mock.ANY,
                      network['provider:physical_network'],
                      network['provider:segmentation_id'])
            for network in create_provnet_port_list]
        self.assertEqual(
            len(create_provnet_port_list),
            ovn_nb_synchronizer._ovn_client._create_provnet_port.call_count)
        ovn_nb_synchronizer._ovn_client._create_provnet_port.assert_has_calls(
            create_provnet_port_calls, any_order=True)

        self.assertEqual(len(del_network_list),
                         ovn_api.ls_del.call_count)
        ls_del_calls = [mock.call(net_name)
                        for net_name in del_network_list]
        ovn_api.ls_del.assert_has_calls(
            ls_del_calls, any_order=True)

        self.assertEqual(len(del_port_list),
                         ovn_api.delete_lswitch_port.call_count)
        delete_lswitch_port_calls = [mock.call(lport_name=port['id'],
                                               lswitch_name=port['lswitch'])
                                     for port in del_port_list]
        ovn_api.delete_lswitch_port.assert_has_calls(
            delete_lswitch_port_calls, any_order=True)

        add_route_calls = [mock.call(mock.ANY, ip_prefix=route['destination'],
                                     nexthop=route['nexthop'])
                           for route in add_static_route_list]
        ovn_api.add_static_route.assert_has_calls(add_route_calls,
                                                  any_order=True)
        self.assertEqual(len(add_static_route_list),
                         ovn_api.add_static_route.call_count)
        del_route_calls = [mock.call(mock.ANY, ip_prefix=route['destination'],
                                     nexthop=route['nexthop'])
                           for route in del_static_route_list]
        ovn_api.delete_static_route.assert_has_calls(del_route_calls,
                                                     any_order=True)
        self.assertEqual(len(del_static_route_list),
                         ovn_api.delete_static_route.call_count)

        add_nat_calls = [mock.call(mock.ANY, **nat) for nat in add_snat_list]
        ovn_api.add_nat_rule_in_lrouter.assert_has_calls(add_nat_calls,
                                                         any_order=True)
        self.assertEqual(len(add_snat_list),
                         ovn_api.add_nat_rule_in_lrouter.call_count)

        add_fip_calls = [mock.call(nat, txn=mock.ANY)
                         for nat in add_floating_ip_list]
        (ovn_nb_synchronizer._ovn_client._create_or_update_floatingip.
            assert_has_calls(add_fip_calls))
        self.assertEqual(
            len(add_floating_ip_list),
            ovn_nb_synchronizer._ovn_client._create_or_update_floatingip.
            call_count)

        del_nat_calls = [mock.call(mock.ANY, **nat) for nat in del_snat_list]
        ovn_api.delete_nat_rule_in_lrouter.assert_has_calls(del_nat_calls,
                                                            any_order=True)
        self.assertEqual(len(del_snat_list),
                         ovn_api.delete_nat_rule_in_lrouter.call_count)

        del_fip_calls = [mock.call(nat, mock.ANY, txn=mock.ANY) for nat in
                         del_floating_ip_list]
        ovn_nb_synchronizer._ovn_client._delete_floatingip.assert_has_calls(
            del_fip_calls, any_order=True)
        self.assertEqual(
            len(del_floating_ip_list),
            ovn_nb_synchronizer._ovn_client._delete_floatingip.call_count)

        create_router_calls = [mock.call(r, add_external_gateway=False)
                               for r in create_router_list]
        self.assertEqual(
            len(create_router_list),
            ovn_nb_synchronizer._ovn_client.create_router.call_count)
        ovn_nb_synchronizer._ovn_client.create_router.assert_has_calls(
            create_router_calls, any_order=True)

        create_router_port_calls = [mock.call(p['device_id'],
                                              mock.ANY)
                                    for p in create_router_port_list]
        self.assertEqual(
            len(create_router_port_list),
            ovn_nb_synchronizer._ovn_client._create_lrouter_port.call_count)
        ovn_nb_synchronizer._ovn_client._create_lrouter_port.assert_has_calls(
            create_router_port_calls,
            any_order=True)

        self.assertEqual(len(del_router_list),
                         ovn_api.delete_lrouter.call_count)
        update_router_port_calls = [mock.call(p)
                                    for p in update_router_port_list]
        self.assertEqual(
            len(update_router_port_list),
            ovn_nb_synchronizer._ovn_client.update_router_port.call_count)
        ovn_nb_synchronizer._ovn_client.update_router_port.assert_has_calls(
            update_router_port_calls,
            any_order=True)

        delete_lrouter_calls = [mock.call(r['router'])
                                for r in del_router_list]
        ovn_api.delete_lrouter.assert_has_calls(
            delete_lrouter_calls, any_order=True)

        self.assertEqual(
            len(del_router_port_list),
            ovn_api.delete_lrouter_port.call_count)
        delete_lrouter_port_calls = [mock.call(port['id'],
                                               port['router'], if_exists=False)
                                     for port in del_router_port_list]
        ovn_api.delete_lrouter_port.assert_has_calls(
            delete_lrouter_port_calls, any_order=True)

        self.assertEqual(
            len(add_subnet_dhcp_options_list),
            ovn_nb_synchronizer._ovn_client._add_subnet_dhcp_options.
            call_count)
        add_subnet_dhcp_options_calls = [
            mock.call(subnet, net, mock.ANY)
            for (subnet, net) in add_subnet_dhcp_options_list]
        ovn_nb_synchronizer._ovn_client._add_subnet_dhcp_options. \
            assert_has_calls(add_subnet_dhcp_options_calls, any_order=True)

        self.assertEqual(ovn_api.delete_dhcp_options.call_count,
                         len(delete_dhcp_options_list))
        delete_dhcp_options_calls = [
            mock.call(dhcp_opt_uuid)
            for dhcp_opt_uuid in delete_dhcp_options_list]
        ovn_api.delete_dhcp_options.assert_has_calls(
            delete_dhcp_options_calls, any_order=True)

    def _test_ovn_nb_sync_mode_repair_helper(self, port_groups_supported=True):

        create_network_list = [{'net': {'id': 'n2', 'mtu': 1450},
                                'ext_ids': {}}]
        del_network_list = ['neutron-n3']
        del_port_list = [{'id': 'p3n1', 'lswitch': 'neutron-n1'},
                         {'id': 'p1n1', 'lswitch': 'neutron-n1'}]
        create_port_list = self.ports
        for port in create_port_list:
            if port['id'] in ['p1n1', 'fp1']:
                # this will be skipped by the logic,
                # because p1n1 is already in lswitch-port list
                # and fp1 is a floating IP port
                create_port_list.remove(port)
        create_provnet_port_list = [{'id': 'n1', 'mtu': 1450,
                                     'provider:physical_network': 'physnet1',
                                     'provider:segmentation_id': 1000}]
        create_router_list = [{
            'id': 'r2', 'routes': [
                {'nexthop': '40.0.0.100', 'destination': '30.0.0.0/24'}],
            'gw_port_id': 'gpr2',
            'external_gateway_info': {
                'network_id': "ext-net", 'enable_snat': True,
                'external_fixed_ips': [{
                    'subnet_id': 'ext-subnet',
                    'ip_address': '100.0.0.2'}]}}]

        # Test adding and deleting routes snats fips behaviors for router r1
        # existing in both neutron DB and OVN DB.
        # Test adding behaviors for router r2 only existing in neutron DB.
        # Static routes with destination 0.0.0.0/0 are default gateway routes
        add_static_route_list = [{'nexthop': '20.0.0.101',
                                  'destination': '12.0.0.0/24'},
                                 {'nexthop': '90.0.0.1',
                                  'destination': '0.0.0.0/0'},
                                 {'nexthop': '40.0.0.100',
                                  'destination': '30.0.0.0/24'},
                                 {'nexthop': '100.0.0.1',
                                  'destination': '0.0.0.0/0'}]
        del_static_route_list = [{'nexthop': '20.0.0.100',
                                  'destination': '10.0.0.0/24'}]
        add_snat_list = [{'logical_ip': '172.16.2.0/24',
                          'external_ip': '90.0.0.2',
                          'type': 'snat'},
                         {'logical_ip': '192.168.2.0/24',
                          'external_ip': '100.0.0.2',
                          'type': 'snat'}]
        del_snat_list = [{'logical_ip': '172.16.1.0/24',
                          'external_ip': '90.0.0.2',
                          'type': 'snat'}]
        # fip 100.0.0.11 exists in OVN with distributed type and in Neutron
        # with centralized type. This fip is used to test
        # enable_distributed_floating_ip switch and migration
        add_floating_ip_list = [{'id': 'fip2', 'router_id': 'r1',
                                 'floating_ip_address': '90.0.0.12',
                                 'fixed_ip_address': '172.16.2.12'},
                                {'id': 'fip3', 'router_id': 'r2',
                                 'floating_ip_address': '100.0.0.10',
                                 'fixed_ip_address': '192.168.2.10'},
                                {'id': 'fip4', 'router_id': 'r2',
                                 'floating_ip_address': '100.0.0.11',
                                 'fixed_ip_address': '192.168.2.11'}]
        del_floating_ip_list = [{'logical_ip': '172.16.1.11',
                                 'external_ip': '90.0.0.11',
                                 'type': 'dnat_and_snat'},
                                {'logical_ip': '192.168.2.11',
                                 'external_ip': '100.0.0.11',
                                 'type': 'dnat_and_snat',
                                 'external_mac': '01:02:03:04:05:06',
                                 'logical_port': 'vm1'}]

        del_router_list = [{'router': 'neutron-r3'}]
        del_router_port_list = [{'id': 'lrp-p3r1', 'router': 'neutron-r1'}]
        create_router_port_list = self.get_sync_router_ports[:2]
        update_router_port_list = [self.get_sync_router_ports[2]]
        update_router_port_list[0].update(
            {'networks': self.lrport_networks})

        if not port_groups_supported:
            add_address_set_list = [
                {'external_ids': {ovn_const.OVN_SG_EXT_ID_KEY: 'sg1'},
                 'name': 'as_ip6_sg1',
                 'addresses': ['fd79:e1c:a55::816:eff:eff:ff2']}]
            del_address_set_list = [{'name': 'as_ip4_del'}]
            update_address_set_list = [
                {'addrs_remove': [],
                 'addrs_add': ['10.0.0.4'],
                 'name': 'as_ip4_sg2'},
                {'addrs_remove': ['fd79:e1c:a55::816:eff:eff:ff3'],
                 'addrs_add': [],
                 'name': 'as_ip6_sg2'}]
            # If Port Groups are not supported, we don't expect any of those
            # to be created/deleted.
            add_port_groups_list = []
            del_port_groups_list = []
        else:
            add_port_groups_list = [
                {'external_ids': {ovn_const.OVN_SG_EXT_ID_KEY: 'sg2'},
                 'name': 'pg_sg2',
                 'acls': []}]
            del_port_groups_list = ['pg_unknown_del']
            # If using Port Groups, no Address Set shall be created/updated
            # and all the existing ones have to be removed.
            add_address_set_list = []
            update_address_set_list = []
            del_address_set_list = [{'name': 'as_ip4_sg1'},
                                    {'name': 'as_ip4_sg2'},
                                    {'name': 'as_ip6_sg2'},
                                    {'name': 'as_ip4_del'}]

        add_subnet_dhcp_options_list = [(self.subnets[2], self.networks[1]),
                                        (self.subnets[1], self.networks[0])]
        delete_dhcp_options_list = ['UUID2', 'UUID4', 'UUID5']

        ovn_nb_synchronizer = ovn_db_sync.OvnNbSynchronizer(
            self.plugin, self.mech_driver._nb_ovn, self.mech_driver._sb_ovn,
            'repair', self.mech_driver)
        self._test_ovn_nb_sync_helper(ovn_nb_synchronizer,
                                      self.networks,
                                      self.ports,
                                      self.routers,
                                      self.get_sync_router_ports,
                                      create_router_list,
                                      create_router_port_list,
                                      update_router_port_list,
                                      del_router_list, del_router_port_list,
                                      create_network_list, create_port_list,
                                      create_provnet_port_list,
                                      del_network_list, del_port_list,
                                      add_static_route_list,
                                      del_static_route_list,
                                      add_snat_list,
                                      del_snat_list,
                                      add_floating_ip_list,
                                      del_floating_ip_list,
                                      add_address_set_list,
                                      del_address_set_list,
                                      update_address_set_list,
                                      add_subnet_dhcp_options_list,
                                      delete_dhcp_options_list,
                                      add_port_groups_list,
                                      del_port_groups_list,
                                      port_groups_supported)

    def test_ovn_nb_sync_mode_repair_no_pgs(self):
        self._test_ovn_nb_sync_mode_repair_helper(port_groups_supported=False)

    def test_ovn_nb_sync_mode_repair_pgs(self):
        self._test_ovn_nb_sync_mode_repair_helper(port_groups_supported=True)

    def _test_ovn_nb_sync_mode_log_helper(self, port_groups_supported=True):
        create_network_list = []
        create_port_list = []
        create_provnet_port_list = []
        del_network_list = []
        del_port_list = []
        create_router_list = []
        create_router_port_list = []
        update_router_port_list = []
        del_router_list = []
        del_router_port_list = []
        add_static_route_list = []
        del_static_route_list = []
        add_snat_list = []
        del_snat_list = []
        add_floating_ip_list = []
        del_floating_ip_list = []
        add_address_set_list = []
        del_address_set_list = []
        update_address_set_list = []
        add_subnet_dhcp_options_list = []
        delete_dhcp_options_list = []
        add_port_groups_list = []
        del_port_groups_list = []

        ovn_nb_synchronizer = ovn_db_sync.OvnNbSynchronizer(
            self.plugin, self.mech_driver._nb_ovn, self.mech_driver._sb_ovn,
            'log', self.mech_driver)
        self._test_ovn_nb_sync_helper(ovn_nb_synchronizer,
                                      self.networks,
                                      self.ports,
                                      self.routers,
                                      self.get_sync_router_ports,
                                      create_router_list,
                                      create_router_port_list,
                                      update_router_port_list,
                                      del_router_list, del_router_port_list,
                                      create_network_list, create_port_list,
                                      create_provnet_port_list,
                                      del_network_list, del_port_list,
                                      add_static_route_list,
                                      del_static_route_list,
                                      add_snat_list,
                                      del_snat_list,
                                      add_floating_ip_list,
                                      del_floating_ip_list,
                                      add_address_set_list,
                                      del_address_set_list,
                                      update_address_set_list,
                                      add_subnet_dhcp_options_list,
                                      delete_dhcp_options_list,
                                      add_port_groups_list,
                                      del_port_groups_list,
                                      port_groups_supported)

    def test_ovn_nb_sync_mode_log_pgs(self):
        self._test_ovn_nb_sync_mode_log_helper(port_groups_supported=True)

    def test_ovn_nb_sync_mode_log_no_pgs(self):
        self._test_ovn_nb_sync_mode_log_helper(port_groups_supported=False)


class TestOvnSbSyncML2(test_mech_driver.OVNMechanismDriverTestCase):

    def test_ovn_sb_sync(self):
        ovn_sb_synchronizer = ovn_db_sync.OvnSbSynchronizer(
            self.plugin,
            self.mech_driver._sb_ovn,
            self.mech_driver)
        ovn_api = ovn_sb_synchronizer.ovn_api
        hostname_with_physnets = {'hostname1': ['physnet1', 'physnet2'],
                                  'hostname2': ['physnet1']}
        ovn_api.get_chassis_hostname_and_physnets.return_value = (
            hostname_with_physnets)
        ovn_driver = ovn_sb_synchronizer.ovn_driver
        ovn_driver.update_segment_host_mapping = mock.Mock()
        hosts_in_neutron = {'hostname2', 'hostname3'}

        with mock.patch.object(ovn_db_sync.segments_db,
                               'get_hosts_mapped_with_segments',
                               return_value=hosts_in_neutron):
            ovn_sb_synchronizer.sync_hostname_and_physical_networks(mock.ANY)
            all_hosts = set(hostname_with_physnets.keys()) | hosts_in_neutron
            self.assertEqual(
                len(all_hosts),
                ovn_driver.update_segment_host_mapping.call_count)
            update_segment_host_mapping_calls = [mock.call(
                host, hostname_with_physnets[host])
                for host in hostname_with_physnets]
            update_segment_host_mapping_calls += [
                mock.call(host, []) for host in
                hosts_in_neutron - set(hostname_with_physnets.keys())]
            ovn_driver.update_segment_host_mapping.assert_has_calls(
                update_segment_host_mapping_calls, any_order=True)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\test_ovn_parent_tag.py
===========File Type===========
.py
===========File Content===========
# Copyright (c) 2015 OpenStack Foundation.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import mock

from networking_ovn.common import constants as ovn_const
from networking_ovn.tests.unit.ml2 import test_mech_driver

OVN_PROFILE = ovn_const.OVN_PORT_BINDING_PROFILE


class TestOVNParentTagPortBinding(test_mech_driver.OVNMechanismDriverTestCase):

    def test_create_port_with_invalid_parent(self):
        binding = {OVN_PROFILE: {"parent_name": 'invalid', 'tag': 1}}
        with self.network() as n:
            with self.subnet(n):
                self._create_port(
                    self.fmt, n['network']['id'],
                    expected_res_status=404,
                    arg_list=(OVN_PROFILE,),
                    **binding)

    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    def test_create_port_with_parent_and_tag(self, mock_get_port):
        binding = {OVN_PROFILE: {"parent_name": '', 'tag': 1}}
        with self.network() as n:
            with self.subnet(n) as s:
                with self.port(s) as p:
                    binding[OVN_PROFILE]['parent_name'] = p['port']['id']
                    res = self._create_port(self.fmt, n['network']['id'],
                                            arg_list=(OVN_PROFILE,),
                                            **binding)
                    port = self.deserialize(self.fmt, res)
                    self.assertEqual(port['port'][OVN_PROFILE],
                                     binding[OVN_PROFILE])
                    mock_get_port.assert_called_with(mock.ANY, p['port']['id'])

    def test_create_port_with_invalid_tag(self):
        binding = {OVN_PROFILE: {"parent_name": '', 'tag': 'a'}}
        with self.network() as n:
            with self.subnet(n) as s:
                with self.port(s) as p:
                    binding[OVN_PROFILE]['parent_name'] = p['port']['id']
                    self._create_port(self.fmt, n['network']['id'],
                                      arg_list=(OVN_PROFILE,),
                                      expected_res_status=400,
                                      **binding)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\test_ovn_vtep.py
===========File Type===========
.py
===========File Content===========
# Copyright (c) 2015 OpenStack Foundation.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from networking_ovn.common import constants as ovn_const
from networking_ovn.tests.unit.ml2 import test_mech_driver

OVN_PROFILE = ovn_const.OVN_PORT_BINDING_PROFILE


class TestOVNVtepPortBinding(test_mech_driver.OVNMechanismDriverTestCase):

    def test_create_port_with_vtep_options(self):
        binding = {OVN_PROFILE: {"vtep-physical-switch": 'psw1',
                   "vtep-logical-switch": 'lsw1'}}
        with self.network() as n:
            with self.subnet(n):
                res = self._create_port(self.fmt, n['network']['id'],
                                        arg_list=(OVN_PROFILE,),
                                        **binding)
                port = self.deserialize(self.fmt, res)
                self.assertEqual(binding[OVN_PROFILE],
                                 port['port'][OVN_PROFILE])

    def test_create_port_with_only_vtep_physical_switch(self):
        binding = {OVN_PROFILE: {"vtep-physical-switch": 'psw'}}
        with self.network() as n:
            with self.subnet(n):
                self._create_port(self.fmt, n['network']['id'],
                                  arg_list=(OVN_PROFILE,),
                                  expected_res_status=400,
                                  **binding)

    def test_create_port_with_only_vtep_logical_switch(self):
        binding = {OVN_PROFILE: {"vtep-logical-switch": 'lsw1'}}
        with self.network() as n:
            with self.subnet(n):
                self._create_port(self.fmt, n['network']['id'],
                                  arg_list=(OVN_PROFILE,),
                                  expected_res_status=400,
                                  **binding)

    def test_create_port_with_invalid_vtep_logical_switch(self):
        binding = {OVN_PROFILE: {"vtep-logical-switch": 1234,
                                 "vtep-physical-switch": "psw1"}}
        with self.network() as n:
            with self.subnet(n):
                self._create_port(self.fmt, n['network']['id'],
                                  arg_list=(OVN_PROFILE,),
                                  expected_res_status=400,
                                  **binding)

    def test_create_port_with_vtep_options_and_parent_name_tag(self):
        binding = {OVN_PROFILE: {"vtep-logical-switch": "lsw1",
                                 "vtep-physical-switch": "psw1",
                                 "parent_name": "pname", "tag": 22}}
        with self.network() as n:
            with self.subnet(n):
                self._create_port(self.fmt, n['network']['id'],
                                  arg_list=(OVN_PROFILE,),
                                  expected_res_status=400,
                                  **binding)

    def test_create_port_with_vtep_options_and_check_vtep_keys(self):
        port = {
            'id': 'foo-port',
            'device_owner': 'compute:None',
            'fixed_ips': [{'subnet_id': 'foo-subnet',
                           'ip_address': '10.0.0.11'}],
            OVN_PROFILE: {"vtep-logical-switch": "lsw1",
                          "vtep-physical-switch": "psw1"}
        }
        ovn_port_info = (
            self.mech_driver._ovn_client._get_port_options(port))
        self.assertEqual(port[OVN_PROFILE]["vtep-physical-switch"],
                         ovn_port_info.options["vtep-physical-switch"])
        self.assertEqual(port[OVN_PROFILE]["vtep-logical-switch"],
                         ovn_port_info.options["vtep-logical-switch"])




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\agent\test_stats.py
===========File Type===========
.py
===========File Content===========
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import uuid

from networking_ovn.agent import stats
from networking_ovn.tests import base


class AgentStatsTest(base.TestCase):
    def test_add_get_del(self):
        uid = uuid.uuid4()
        nb_cfg = 7
        stats.AgentStats.add_stat(uid, nb_cfg)
        got = stats.AgentStats.get_stat(uid)
        self.assertEqual(nb_cfg, got.nb_cfg)
        self.assertTrue(got.updated_at)
        stats.AgentStats.del_agent(uid)
        self.assertNotIn(uid, stats.AgentStats._agents)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\agent\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\agent\metadata\test_agent.py
===========File Type===========
.py
===========File Content===========
# Copyright 2017 Red Hat, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import collections

import mock
from neutron.agent.linux import ip_lib
from neutron.agent.linux.ip_lib import IpAddrCommand as ip_addr
from neutron.agent.linux.ip_lib import IpLinkCommand as ip_link
from neutron.agent.linux.ip_lib import IpNetnsCommand as ip_netns
from neutron.agent.linux.ip_lib import IPWrapper as ip_wrap
from neutron.tests import base
from oslo_config import cfg
from oslo_config import fixture as config_fixture

from networking_ovn.agent.metadata import agent
from networking_ovn.agent.metadata import driver
from networking_ovn.conf.agent.metadata import config as meta_conf


OvnPortInfo = collections.namedtuple(
    'OvnPortInfo', ['datapath', 'type', 'mac', 'external_ids', 'logical_port'])
DatapathInfo = collections.namedtuple('DatapathInfo', 'uuid')


def makePort(datapath=None, type='', mac=None, external_ids=None,
             logical_port=None):
    return OvnPortInfo(datapath, type, mac, external_ids, logical_port)


class ConfFixture(config_fixture.Config):
    def setUp(self):
        super(ConfFixture, self).setUp()
        meta_conf.register_meta_conf_opts(meta_conf.SHARED_OPTS, self.conf)
        meta_conf.register_meta_conf_opts(
            meta_conf.UNIX_DOMAIN_METADATA_PROXY_OPTS, self.conf)
        meta_conf.register_meta_conf_opts(
            meta_conf.METADATA_PROXY_HANDLER_OPTS, self.conf)
        meta_conf.register_meta_conf_opts(meta_conf.OVS_OPTS, self.conf,
                                          group='ovs')


class TestMetadataAgent(base.BaseTestCase):
    fake_conf = cfg.CONF
    fake_conf_fixture = ConfFixture(fake_conf)

    def setUp(self):
        super(TestMetadataAgent, self).setUp()
        self.useFixture(self.fake_conf_fixture)
        self.log_p = mock.patch.object(agent, 'LOG')
        self.log = self.log_p.start()
        self.agent = agent.MetadataAgent(self.fake_conf)
        self.agent.sb_idl = mock.Mock()
        self.agent.ovs_idl = mock.Mock()
        self.agent.chassis = 'chassis'

    def test_sync(self):
        with mock.patch.object(
                self.agent, 'ensure_all_networks_provisioned') as enp,\
                mock.patch.object(
                    ip_lib, 'list_network_namespaces') as lnn,\
                mock.patch.object(
                    self.agent, 'teardown_datapath') as tdp:
            enp.return_value = ['ovnmeta-1', 'ovnmeta-2']
            lnn.return_value = ['ovnmeta-1', 'ovnmeta-2']

            self.agent.sync()

            enp.assert_called_once()
            lnn.assert_called_once()
            tdp.assert_not_called()

    def test_sync_teardown_namespace(self):
        """Test that sync tears down unneeded metadata namespaces."""
        with mock.patch.object(
                self.agent, 'ensure_all_networks_provisioned') as enp,\
                mock.patch.object(
                    ip_lib, 'list_network_namespaces') as lnn,\
                mock.patch.object(
                    self.agent, 'teardown_datapath') as tdp:
            enp.return_value = ['ovnmeta-1', 'ovnmeta-2']
            lnn.return_value = ['ovnmeta-1', 'ovnmeta-2', 'ovnmeta-3',
                                'ns1', 'ns2']

            self.agent.sync()

            enp.assert_called_once()
            lnn.assert_called_once()
            tdp.assert_called_once_with('3')

    def test_ensure_all_networks_provisioned(self):
        """Test networks are provisioned.

        This test simulates that this chassis has the following ports:
            * datapath '0': 1 port
            * datapath '1': 2 ports
            * datapath '2': 1 port
            * datapath '5': 1 port with type 'unk'

        It is expected that only datapaths '0', '1' and '2' are provisioned
        once.
        """

        ports = []
        for i in range(0, 3):
            ports.append(makePort(datapath=DatapathInfo(uuid=str(i))))
        ports.append(makePort(datapath=DatapathInfo(uuid='1')))
        ports.append(makePort(datapath=DatapathInfo(uuid='5'), type='unknown'))

        with mock.patch.object(self.agent, 'provision_datapath',
                               return_value=None) as pdp,\
                mock.patch.object(self.agent.sb_idl, 'get_ports_on_chassis',
                                  return_value=ports):
            self.agent.ensure_all_networks_provisioned()

            expected_calls = [mock.call(str(i)) for i in range(0, 3)]
            self.assertEqual(sorted(expected_calls),
                             sorted(pdp.call_args_list))

    def test_update_datapath_provision(self):
        ports = []
        for i in range(0, 3):
            ports.append(makePort(datapath=DatapathInfo(uuid=str(i))))

        with mock.patch.object(self.agent, 'provision_datapath',
                               return_value=None) as pdp,\
                mock.patch.object(self.agent, 'teardown_datapath') as tdp,\
                mock.patch.object(self.agent.sb_idl, 'get_ports_on_chassis',
                                  return_value=ports):
            self.agent.update_datapath('1')
            pdp.assert_called_once_with('1')
            tdp.assert_not_called()

    def test_update_datapath_teardown(self):
        ports = []
        for i in range(0, 3):
            ports.append(makePort(datapath=DatapathInfo(uuid=str(i))))

        with mock.patch.object(self.agent, 'provision_datapath',
                               return_value=None) as pdp,\
                mock.patch.object(self.agent, 'teardown_datapath') as tdp,\
                mock.patch.object(self.agent.sb_idl, 'get_ports_on_chassis',
                                  return_value=ports):
            self.agent.update_datapath('5')
            tdp.assert_called_once_with('5')
            pdp.assert_not_called()

    def test_teardown_datapath(self):
        """Test teardown datapath.

        Check that the VETH pair, OVS port and namespace associated to this
        namespace are deleted and the metadata proxy is destroyed.
        """
        with mock.patch.object(self.agent,
                               'update_chassis_metadata_networks'),\
                mock.patch.object(
                    ip_netns, 'exists', return_value=True),\
                mock.patch.object(
                    ip_lib, 'device_exists', return_value=True),\
                mock.patch.object(
                    ip_wrap, 'garbage_collect_namespace') as garbage_collect,\
                mock.patch.object(
                    ip_wrap, 'del_veth') as del_veth,\
                mock.patch.object(agent.MetadataAgent, '_get_veth_name',
                                  return_value=['veth_0', 'veth_1']),\
                mock.patch.object(
                    driver.MetadataDriver,
                    'destroy_monitored_metadata_proxy') as destroy_mdp:

            self.agent.teardown_datapath('1')

            destroy_mdp.assert_called_once()
            self.agent.ovs_idl.del_port.assert_called_once_with(
                'veth_0', bridge='br-int')
            del_veth.assert_called_once_with('veth_0')
            garbage_collect.assert_called_once()

    def test_provision_datapath(self):
        """Test datapath provisioning.

        Check that the VETH pair, OVS port and namespace associated to this
        namespace are created, that the interface is properly configured with
        the right IP addresses and that the metadata proxy is spawned.
        """

        metadata_port = makePort(mac=['aa:bb:cc:dd:ee:ff'],
                                 external_ids={
                                     'neutron:cidrs': '10.0.0.1/23 '
                                     '2001:470:9:1224:5595:dd51:6ba2:e788/64'},
                                 logical_port='port')

        with mock.patch.object(self.agent.sb_idl,
                               'get_metadata_port_network',
                               return_value=metadata_port),\
                mock.patch.object(
                    ip_lib, 'device_exists', return_value=False),\
                mock.patch.object(
                    ip_lib.IPDevice, 'exists', return_value=False),\
                mock.patch.object(agent.MetadataAgent, '_get_veth_name',
                                  return_value=['veth_0', 'veth_1']),\
                mock.patch.object(agent.MetadataAgent, '_get_namespace_name',
                                  return_value='namespace'),\
                mock.patch.object(ip_link, 'set_up') as link_set_up,\
                mock.patch.object(ip_link, 'set_address') as link_set_addr,\
                mock.patch.object(ip_addr, 'list', return_value=[]),\
                mock.patch.object(ip_addr, 'add') as ip_addr_add,\
                mock.patch.object(
                    ip_wrap, 'add_veth',
                    return_value=[ip_lib.IPDevice('ip1'),
                                  ip_lib.IPDevice('ip2')]) as add_veth,\
                mock.patch.object(
                    self.agent,
                    'update_chassis_metadata_networks') as update_chassis,\
                mock.patch.object(
                    driver.MetadataDriver,
                    'spawn_monitored_metadata_proxy') as spawn_mdp:

            self.agent.provision_datapath('1')

            # Check that the VETH pair is created
            add_veth.assert_called_once_with('veth_0', 'veth_1', 'namespace')
            # Make sure that the two ends of the VETH pair have been set as up.
            self.assertEqual(2, link_set_up.call_count)
            link_set_addr.assert_called_once_with('aa:bb:cc:dd:ee:ff')
            # Make sure that the port has been added to OVS.
            self.agent.ovs_idl.add_port.assert_called_once_with(
                'br-int', 'veth_0')
            self.agent.ovs_idl.db_set.assert_called_once_with(
                'Interface', 'veth_0', ('external_ids', {'iface-id': 'port'}))
            # Check that the metadata port has the IP addresses properly
            # configured and that IPv6 address has been skipped.
            expected_calls = [mock.call('10.0.0.1/23'),
                              mock.call('169.254.169.254/16')]
            self.assertEqual(sorted(expected_calls),
                             sorted(ip_addr_add.call_args_list))
            # Check that metadata proxy has been spawned
            spawn_mdp.assert_called_once()
            # Check that the chassis has been updated with the datapath.
            update_chassis.assert_called_once_with('1')




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\agent\metadata\test_driver.py
===========File Type===========
.py
===========File Content===========
# Copyright 2017 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os

import mock
from oslo_config import cfg
from oslo_utils import uuidutils

from neutron.tests import base
from neutron.tests import tools
from neutron.tests.unit.agent.linux import test_utils

from networking_ovn.agent.metadata import agent as metadata_agent
from networking_ovn.agent.metadata import driver as metadata_driver
from networking_ovn.conf.agent.metadata import config as meta_conf

_uuid = uuidutils.generate_uuid


class TestMetadataDriverProcess(base.BaseTestCase):

    EUNAME = 'neutron'
    EGNAME = 'neutron'
    METADATA_PORT = 8080
    METADATA_SOCKET = '/socket/path'
    PIDFILE = 'pidfile'

    def setUp(self):
        super(TestMetadataDriverProcess, self).setUp()
        mock.patch('eventlet.spawn').start()

        meta_conf.register_meta_conf_opts(meta_conf.SHARED_OPTS, cfg.CONF)

    def test_spawn_metadata_proxy(self):
        datapath_id = _uuid()
        metadata_ns = metadata_agent.NS_PREFIX + datapath_id
        ip_class_path = 'neutron.agent.linux.ip_lib.IPWrapper'

        cfg.CONF.set_override('metadata_proxy_user', self.EUNAME)
        cfg.CONF.set_override('metadata_proxy_group', self.EGNAME)
        cfg.CONF.set_override('metadata_proxy_socket', self.METADATA_SOCKET)
        cfg.CONF.set_override('debug', True)

        agent = metadata_agent.MetadataAgent(cfg.CONF)
        with mock.patch(ip_class_path) as ip_mock,\
                mock.patch(
                    'neutron.agent.linux.external_process.'
                    'ProcessManager.get_pid_file_name',
                    return_value=self.PIDFILE),\
                mock.patch('pwd.getpwnam',
                           return_value=test_utils.FakeUser(self.EUNAME)),\
                mock.patch('grp.getgrnam',
                           return_value=test_utils.FakeGroup(self.EGNAME)),\
                mock.patch('os.makedirs'):
            cfg_file = os.path.join(
                metadata_driver.HaproxyConfigurator.get_config_path(
                    cfg.CONF.state_path),
                "%s.conf" % datapath_id)
            mock_open = self.useFixture(
                tools.OpenFixture(cfg_file)).mock_open
            metadata_driver.MetadataDriver.spawn_monitored_metadata_proxy(
                agent._process_monitor,
                metadata_ns,
                self.METADATA_PORT,
                cfg.CONF,
                network_id=datapath_id)

            netns_execute_args = [
                'haproxy',
                '-f', cfg_file]

            cfg_contents = metadata_driver._HAPROXY_CONFIG_TEMPLATE % {
                'user': self.EUNAME,
                'group': self.EGNAME,
                'port': self.METADATA_PORT,
                'unix_socket_path': self.METADATA_SOCKET,
                'res_type': 'Network',
                'res_id': datapath_id,
                'pidfile': self.PIDFILE,
                'log_level': 'debug'}

            mock_open.assert_has_calls([
                mock.call(cfg_file, 'w'),
                mock.call().write(cfg_contents)],
                any_order=True)

            ip_mock.assert_has_calls([
                mock.call(namespace=metadata_ns),
                mock.call().netns.execute(netns_execute_args, addl_env=None,
                                          run_as_root=True)
            ])

    def test_create_config_file_wrong_user(self):
        with mock.patch('pwd.getpwnam', side_effect=KeyError):
            config = metadata_driver.HaproxyConfigurator(mock.ANY, mock.ANY,
                                                         mock.ANY, mock.ANY,
                                                         self.EUNAME,
                                                         self.EGNAME,
                                                         mock.ANY, mock.ANY)
            self.assertRaises(metadata_driver.InvalidUserOrGroupException,
                              config.create_config_file)

    def test_create_config_file_wrong_group(self):
        with mock.patch('grp.getgrnam', side_effect=KeyError),\
                mock.patch('pwd.getpwnam',
                           return_value=test_utils.FakeUser(self.EUNAME)):
            config = metadata_driver.HaproxyConfigurator(mock.ANY, mock.ANY,
                                                         mock.ANY, mock.ANY,
                                                         self.EUNAME,
                                                         self.EGNAME,
                                                         mock.ANY, mock.ANY)
            self.assertRaises(metadata_driver.InvalidUserOrGroupException,
                              config.create_config_file)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\agent\metadata\test_server.py
===========File Type===========
.py
===========File Content===========
# Copyright 2017 Red Hat, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import collections

import mock
from neutron.agent.linux import utils as agent_utils
from neutron.tests import base
from oslo_config import cfg
from oslo_config import fixture as config_fixture
from oslo_utils import fileutils
import testtools
import webob

from networking_ovn.agent.metadata import server as agent
from networking_ovn.conf.agent.metadata import config as meta_conf

OvnPortInfo = collections.namedtuple('OvnPortInfo', 'external_ids')


class ConfFixture(config_fixture.Config):
    def setUp(self):
        super(ConfFixture, self).setUp()
        meta_conf.register_meta_conf_opts(
            meta_conf.METADATA_PROXY_HANDLER_OPTS, self.conf)
        self.config(auth_ca_cert=None,
                    nova_metadata_host='9.9.9.9',
                    nova_metadata_port=8775,
                    metadata_proxy_shared_secret='secret',
                    nova_metadata_protocol='http',
                    nova_metadata_insecure=True,
                    nova_client_cert='nova_cert',
                    nova_client_priv_key='nova_priv_key')


class TestMetadataProxyHandler(base.BaseTestCase):
    fake_conf = cfg.CONF
    fake_conf_fixture = ConfFixture(fake_conf)

    def setUp(self):
        super(TestMetadataProxyHandler, self).setUp()
        self.useFixture(self.fake_conf_fixture)
        self.log_p = mock.patch.object(agent, 'LOG')
        self.log = self.log_p.start()
        self.handler = agent.MetadataProxyHandler(self.fake_conf)
        self.handler.sb_idl = mock.Mock()

    def test_call(self):
        req = mock.Mock()
        with mock.patch.object(self.handler,
                               '_get_instance_and_project_id') as get_ids:
            get_ids.return_value = ('instance_id', 'project_id')
            with mock.patch.object(self.handler, '_proxy_request') as proxy:
                proxy.return_value = 'value'

                retval = self.handler(req)
                self.assertEqual(retval, 'value')

    def test_call_no_instance_match(self):
        req = mock.Mock()
        with mock.patch.object(self.handler,
                               '_get_instance_and_project_id') as get_ids:
            get_ids.return_value = None, None
            retval = self.handler(req)
            self.assertIsInstance(retval, webob.exc.HTTPNotFound)

    def test_call_internal_server_error(self):
        req = mock.Mock()
        with mock.patch.object(self.handler,
                               '_get_instance_and_project_id') as get_ids:
            get_ids.side_effect = Exception
            retval = self.handler(req)
            self.assertIsInstance(retval, webob.exc.HTTPInternalServerError)
            self.assertEqual(len(self.log.mock_calls), 2)

    def _get_instance_and_project_id_helper(self, headers, list_ports_retval,
                                            network=None):
        remote_address = '192.168.1.1'
        headers['X-Forwarded-For'] = remote_address
        req = mock.Mock(headers=headers)

        def mock_get_network_port_bindings_by_ip(*args, **kwargs):
            return list_ports_retval.pop(0)

        self.handler.sb_idl.get_network_port_bindings_by_ip.side_effect = (
            mock_get_network_port_bindings_by_ip)

        instance_id, project_id = (
            self.handler._get_instance_and_project_id(req))

        expected = [mock.call(network, '192.168.1.1')]
        self.handler.sb_idl.get_network_port_bindings_by_ip.assert_has_calls(
            expected)
        return (instance_id, project_id)

    def test_get_instance_id_network_id(self):
        network_id = 'the_id'
        headers = {
            'X-OVN-Network-ID': network_id
        }

        ovn_port = OvnPortInfo(
            external_ids={'neutron:device_id': 'device_id',
                          'neutron:project_id': 'project_id'})
        ports = [[ovn_port]]

        self.assertEqual(
            self._get_instance_and_project_id_helper(headers, ports,
                                                     network='the_id'),
            ('device_id', 'project_id')
        )

    def test_get_instance_id_network_id_no_match(self):
        network_id = 'the_id'
        headers = {
            'X-OVN-Network-ID': network_id
        }

        ports = [[]]

        expected = (None, None)
        observed = self._get_instance_and_project_id_helper(headers, ports,
                                                            network='the_id')
        self.assertEqual(expected, observed)

    def _proxy_request_test_helper(self, response_code=200, method='GET'):
        hdrs = {'X-Forwarded-For': '8.8.8.8'}
        body = 'body'

        req = mock.Mock(path_info='/the_path', query_string='', headers=hdrs,
                        method=method, body=body)
        resp = mock.MagicMock(status=response_code)
        req.response = resp
        with mock.patch.object(self.handler, '_sign_instance_id') as sign:
            sign.return_value = 'signed'
            with mock.patch('httplib2.Http') as mock_http:
                resp.__getitem__.return_value = "text/plain"
                mock_http.return_value.request.return_value = (resp, 'content')

                retval = self.handler._proxy_request('the_id', 'tenant_id',
                                                     req)
                mock_http.assert_called_once_with(
                    ca_certs=None, disable_ssl_certificate_validation=True)
                mock_http.assert_has_calls([
                    mock.call().add_certificate(
                        self.fake_conf.nova_client_priv_key,
                        self.fake_conf.nova_client_cert,
                        "%s:%s" % (self.fake_conf.nova_metadata_host,
                                   self.fake_conf.nova_metadata_port)
                    ),
                    mock.call().request(
                        'http://9.9.9.9:8775/the_path',
                        method=method,
                        headers={
                            'X-Forwarded-For': '8.8.8.8',
                            'X-Instance-ID-Signature': 'signed',
                            'X-Instance-ID': 'the_id',
                            'X-Tenant-ID': 'tenant_id'
                        },
                        body=body
                    )]
                )

                return retval

    def test_proxy_request_post(self):
        response = self._proxy_request_test_helper(method='POST')
        self.assertEqual(response.content_type, "text/plain")
        self.assertEqual(response.body, 'content')

    def test_proxy_request_200(self):
        response = self._proxy_request_test_helper(200)
        self.assertEqual(response.content_type, "text/plain")
        self.assertEqual(response.body, 'content')

    def test_proxy_request_400(self):
        self.assertIsInstance(self._proxy_request_test_helper(400),
                              webob.exc.HTTPBadRequest)

    def test_proxy_request_403(self):
        self.assertIsInstance(self._proxy_request_test_helper(403),
                              webob.exc.HTTPForbidden)

    def test_proxy_request_404(self):
        self.assertIsInstance(self._proxy_request_test_helper(404),
                              webob.exc.HTTPNotFound)

    def test_proxy_request_409(self):
        self.assertIsInstance(self._proxy_request_test_helper(409),
                              webob.exc.HTTPConflict)

    def test_proxy_request_500(self):
        self.assertIsInstance(self._proxy_request_test_helper(500),
                              webob.exc.HTTPInternalServerError)

    def test_proxy_request_other_code(self):
        with testtools.ExpectedException(Exception):
            self._proxy_request_test_helper(302)

    def test_sign_instance_id(self):
        self.assertEqual(
            self.handler._sign_instance_id('foo'),
            '773ba44693c7553d6ee20f61ea5d2757a9a4f4a44d2841ae4e95b52e4cd62db4'
        )


class TestUnixDomainMetadataProxy(base.BaseTestCase):
    def setUp(self):
        super(TestUnixDomainMetadataProxy, self).setUp()
        self.cfg_p = mock.patch.object(agent, 'cfg')
        self.cfg = self.cfg_p.start()
        self.cfg.CONF.metadata_proxy_socket = '/the/path'
        self.cfg.CONF.metadata_workers = 0
        self.cfg.CONF.metadata_backlog = 128
        self.cfg.CONF.metadata_proxy_socket_mode = meta_conf.USER_MODE

    @mock.patch.object(fileutils, 'ensure_tree')
    def test_init_doesnot_exists(self, ensure_dir):
        agent.UnixDomainMetadataProxy(mock.Mock())
        ensure_dir.assert_called_once_with('/the', mode=0o755)

    def test_init_exists(self):
        with mock.patch('os.path.isdir') as isdir:
            with mock.patch('os.unlink') as unlink:
                isdir.return_value = True
                agent.UnixDomainMetadataProxy(mock.Mock())
                unlink.assert_called_once_with('/the/path')

    def test_init_exists_unlink_no_file(self):
        with mock.patch('os.path.isdir') as isdir:
            with mock.patch('os.unlink') as unlink:
                with mock.patch('os.path.exists') as exists:
                    isdir.return_value = True
                    exists.return_value = False
                    unlink.side_effect = OSError

                    agent.UnixDomainMetadataProxy(mock.Mock())
                    unlink.assert_called_once_with('/the/path')

    def test_init_exists_unlink_fails_file_still_exists(self):
        with mock.patch('os.path.isdir') as isdir:
            with mock.patch('os.unlink') as unlink:
                with mock.patch('os.path.exists') as exists:
                    isdir.return_value = True
                    exists.return_value = True
                    unlink.side_effect = OSError

                    with testtools.ExpectedException(OSError):
                        agent.UnixDomainMetadataProxy(mock.Mock())
                    unlink.assert_called_once_with('/the/path')

    @mock.patch.object(agent, 'MetadataProxyHandler')
    @mock.patch.object(agent_utils, 'UnixDomainWSGIServer')
    @mock.patch.object(fileutils, 'ensure_tree')
    def test_run(self, ensure_dir, server, handler):
        p = agent.UnixDomainMetadataProxy(self.cfg.CONF)
        p.run()

        ensure_dir.assert_called_once_with('/the', mode=0o755)
        server.assert_has_calls([
            mock.call('networking-ovn-metadata-agent'),
            mock.call().start(handler.return_value,
                              '/the/path', workers=0,
                              backlog=128, mode=0o644)]
        )




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\agent\metadata\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\cmd\test_neutron_ovn_db_sync_util.py
===========File Type===========
.py
===========File Content===========
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import mock

from networking_ovn.cmd import neutron_ovn_db_sync_util as cmd
from networking_ovn.tests import base


class TestNeutronOVNDBSyncUtil(base.TestCase):

    def setUp(self):
        super(TestNeutronOVNDBSyncUtil, self).setUp()
        self.cmd_log = mock.Mock()
        cmd.LOG = self.cmd_log
        self.cmd_sync = mock.Mock()
        self.cmd_sync.do_sync = mock.Mock()
        self.cmd_sb_sync = mock.Mock()
        self.cmd_sb_sync.do_sync = mock.Mock()

    def _setup_default_mock_cfg(self, mock_cfg):
        mock_cfg.ovn.neutron_sync_mode = 'log'
        mock_cfg.core_plugin = 'neutron.plugins.ml2.plugin.Ml2Plugin'
        mock_cfg.ml2.mechanism_drivers = ['ovn']

    # Test that the configuration can be loaded successfully.
    def test_setup_conf(self):
        cmd.setup_conf()

    def test_main_invalid_conf(self):
        with mock.patch(
                'networking_ovn.cmd.neutron_ovn_db_sync_util.setup_conf',
                return_value=None):
            cmd.main()
        self.cmd_log.error.assert_called_once_with(
            'Error parsing the configuration values. Please verify.')

    @mock.patch('oslo_log.log.setup')
    @mock.patch('networking_ovn.cmd.neutron_ovn_db_sync_util.setup_conf')
    @mock.patch('networking_ovn.ovsdb.impl_idl_ovn.get_connection')
    def test_main_invalid_nb_idl(self, mock_con, mock_conf, mock_log_setup):
        with mock.patch('oslo_config.cfg.CONF') as mock_cfg, \
            mock.patch('networking_ovn.ovsdb.impl_idl_ovn.OvsdbNbOvnIdl',
                       side_effect=RuntimeError):
            self._setup_default_mock_cfg(mock_cfg)
            cmd.main()
        self.cmd_log.error.assert_called_once_with(
            'Invalid --ovn-ovn_nb_connection parameter provided.')

    @mock.patch('oslo_log.log.setup')
    @mock.patch('networking_ovn.cmd.neutron_ovn_db_sync_util.setup_conf')
    @mock.patch('networking_ovn.ovsdb.impl_idl_ovn.get_connection')
    def test_main_invalid_sb_idl(self, mock_con, mock_conf, mock_log_setup):
        with mock.patch('oslo_config.cfg.CONF') as mock_cfg, \
                mock.patch('networking_ovn.ovsdb.impl_idl_ovn.OvsdbSbOvnIdl',
                           side_effect=RuntimeError):
            self._setup_default_mock_cfg(mock_cfg)
            cmd.main()
        self.cmd_log.error.assert_called_once_with(
            'Invalid --ovn-ovn_sb_connection parameter provided.')

    @mock.patch('neutron.manager.init')
    @mock.patch('neutron_lib.plugins.directory.get_plugin')
    @mock.patch('networking_ovn.ovsdb.impl_idl_ovn.OvsdbNbOvnIdl')
    @mock.patch('oslo_log.log.setup')
    @mock.patch('networking_ovn.cmd.neutron_ovn_db_sync_util.setup_conf')
    @mock.patch('networking_ovn.ovsdb.impl_idl_ovn.get_connection')
    def _test_main(self, mock_con, mock_conf, mock_log_setup, mock_nb_idl,
                   mock_plugin, mock_manager_init):
        cmd.main()

    def test_main_invalid_sync_mode(self):
        with mock.patch('oslo_config.cfg.CONF') as mock_cfg:
            self._setup_default_mock_cfg(mock_cfg)
            mock_cfg.ovn.neutron_sync_mode = 'off'
            self._test_main()
        self.cmd_log.error.assert_called_once_with(
            'Invalid sync mode : ["%s"]. Should be "log" or "repair"', 'off')

    def test_main_invalid_core_plugin(self):
        with mock.patch('oslo_config.cfg.CONF') as mock_cfg:
            self._setup_default_mock_cfg(mock_cfg)
            mock_cfg.core_plugin = 'foo'
            self._test_main()
        self.cmd_log.error.assert_called_once_with(
            'Invalid core plugin : ["%s"].', 'foo')

    def test_main_no_mechanism_driver(self):
        with mock.patch('oslo_config.cfg.CONF') as mock_cfg:
            mock_cfg.ovn.neutron_sync_mode = 'repair'
            mock_cfg.core_plugin = 'ml2'
            mock_cfg.ml2.mechanism_drivers = []
            self._test_main()
        self.cmd_log.error.assert_called_once_with(
            'please use --config-file to specify '
            'neutron and ml2 configuration file.')

    def test_main_invalid_mechanism_driver(self):
        with mock.patch('oslo_config.cfg.CONF') as mock_cfg:
            self._setup_default_mock_cfg(mock_cfg)
            mock_cfg.ml2.mechanism_drivers = ['foo']
            self._test_main()
        self.cmd_log.error.assert_called_once_with(
            'No "ovn" mechanism driver found : "%s".', ['foo'])

    def _test_main_sync(self):
        with mock.patch('networking_ovn.ovn_db_sync.OvnNbSynchronizer',
                        return_value=self.cmd_sync), \
                mock.patch('networking_ovn.ovn_db_sync.OvnSbSynchronizer',
                           return_value=self.cmd_sb_sync), \
                mock.patch('oslo_config.cfg.CONF') as mock_cfg:
            self._setup_default_mock_cfg(mock_cfg)
            self._test_main()

    def test_main_sync_success(self):
        self._test_main_sync()
        self.cmd_sync.do_sync.assert_called_once_with()
        self.cmd_sb_sync.do_sync.assert_called_once_with()




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\cmd\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\common\test_acl.py
===========File Type===========
.py
===========File Content===========
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import copy

import mock
from neutron_lib import constants as const

from networking_ovn.common import acl as ovn_acl
from networking_ovn.common import constants as ovn_const
from networking_ovn.common import utils as ovn_utils
from networking_ovn.ovsdb import commands as cmd
from networking_ovn.tests import base
from networking_ovn.tests.unit import fakes


class TestACLs(base.TestCase):

    def setUp(self):
        super(TestACLs, self).setUp()
        self.driver = mock.Mock()
        self.driver._nb_ovn = fakes.FakeOvsdbNbOvnIdl()
        self.plugin = fakes.FakePlugin()
        self.admin_context = mock.Mock()
        self.fake_port = fakes.FakePort.create_one_port({
            'id': 'fake_port_id1',
            'network_id': 'network_id1',
            'fixed_ips': [{'subnet_id': 'subnet_id1',
                           'ip_address': '1.1.1.1'}],
        }).info()
        self.fake_subnet = fakes.FakeSubnet.create_one_subnet({
            'id': 'subnet_id1',
            'ip_version': 4,
            'cidr': '1.1.1.0/24',
        }).info()
        patcher = mock.patch(
            'ovsdbapp.backend.ovs_idl.idlutils.row_by_value',
            lambda *args, **kwargs: mock.MagicMock())
        patcher.start()
        mock.patch(
            "networking_ovn.common.acl._acl_columns_name_severity_supported",
            return_value=True
        ).start()

    def test_drop_all_ip_traffic_for_port(self):
        acls = ovn_acl.drop_all_ip_traffic_for_port(self.fake_port)
        acl_to_lport = {'action': 'drop', 'direction': 'to-lport',
                        'external_ids': {'neutron:lport':
                                         self.fake_port['id']},
                        'log': False, 'name': [], 'severity': [],
                        'lport': self.fake_port['id'],
                        'lswitch': 'neutron-network_id1',
                        'match': 'outport == "fake_port_id1" && ip',
                        'priority': 1001}
        acl_from_lport = {'action': 'drop', 'direction': 'from-lport',
                          'external_ids': {'neutron:lport':
                                           self.fake_port['id']},
                          'log': False, 'name': [], 'severity': [],
                          'lport': self.fake_port['id'],
                          'lswitch': 'neutron-network_id1',
                          'match': 'inport == "fake_port_id1" && ip',
                          'priority': 1001}
        for acl in acls:
            if 'to-lport' in acl.values():
                self.assertEqual(acl_to_lport, acl)
            if 'from-lport' in acl.values():
                self.assertEqual(acl_from_lport, acl)

    def test_add_acl_dhcp(self):
        ovn_dhcp_acls = ovn_acl.add_acl_dhcp(self.fake_port, self.fake_subnet)
        other_dhcp_acls = ovn_acl.add_acl_dhcp(self.fake_port,
                                               self.fake_subnet,
                                               ovn_dhcp=False)

        expected_match_to_lport = (
            'outport == "%s" && ip4 && ip4.src == %s && udp && udp.src == 67 '
            '&& udp.dst == 68') % (self.fake_port['id'],
                                   self.fake_subnet['cidr'])
        acl_to_lport = {'action': 'allow', 'direction': 'to-lport',
                        'external_ids': {'neutron:lport': 'fake_port_id1'},
                        'log': False, 'name': [], 'severity': [],
                        'lport': 'fake_port_id1',
                        'lswitch': 'neutron-network_id1',
                        'match': expected_match_to_lport, 'priority': 1002}
        expected_match_from_lport = (
            'inport == "%s" && ip4 && '
            'ip4.dst == {255.255.255.255, %s} && '
            'udp && udp.src == 68 && udp.dst == 67'
        ) % (self.fake_port['id'], self.fake_subnet['cidr'])
        acl_from_lport = {'action': 'allow', 'direction': 'from-lport',
                          'external_ids': {'neutron:lport': 'fake_port_id1'},
                          'log': False, 'name': [], 'severity': [],
                          'lport': 'fake_port_id1',
                          'lswitch': 'neutron-network_id1',
                          'match': expected_match_from_lport, 'priority': 1002}
        self.assertEqual(1, len(ovn_dhcp_acls))
        self.assertEqual(acl_from_lport, ovn_dhcp_acls[0])
        self.assertEqual(2, len(other_dhcp_acls))
        for acl in other_dhcp_acls:
            if 'to-lport' in acl.values():
                self.assertEqual(acl_to_lport, acl)
            if 'from-lport' in acl.values():
                self.assertEqual(acl_from_lport, acl)

    def _test_add_sg_rule_acl_for_port(self, sg_rule, direction, match):
        port = {'id': 'port-id',
                'network_id': 'network-id'}
        acl = ovn_acl.add_sg_rule_acl_for_port(port, sg_rule, match)
        self.assertEqual({'lswitch': 'neutron-network-id',
                          'lport': 'port-id',
                          'priority': ovn_const.ACL_PRIORITY_ALLOW,
                          'action': ovn_const.ACL_ACTION_ALLOW_RELATED,
                          'log': False, 'name': [], 'severity': [],
                          'direction': direction,
                          'match': match,
                          'external_ids': {
                              'neutron:lport': 'port-id',
                              'neutron:security_group_rule_id': 'sgr_id'}},
                         acl)

    def test_add_sg_rule_acl_for_port_remote_ip_prefix(self):
        sg_rule = {'id': 'sgr_id',
                   'direction': 'ingress',
                   'ethertype': 'IPv4',
                   'remote_group_id': None,
                   'remote_ip_prefix': '1.1.1.0/24',
                   'protocol': None}
        match = 'outport == "port-id" && ip4 && ip4.src == 1.1.1.0/24'
        self._test_add_sg_rule_acl_for_port(sg_rule,
                                            'to-lport',
                                            match)
        sg_rule['direction'] = 'egress'
        match = 'inport == "port-id" && ip4 && ip4.dst == 1.1.1.0/24'
        self._test_add_sg_rule_acl_for_port(sg_rule,
                                            'from-lport',
                                            match)

    def test_add_sg_rule_acl_for_port_remote_group(self):
        sg_rule = {'id': 'sgr_id',
                   'direction': 'ingress',
                   'ethertype': 'IPv4',
                   'remote_group_id': 'sg1',
                   'remote_ip_prefix': None,
                   'protocol': None}
        match = 'outport == "port-id" && ip4 && (ip4.src == 1.1.1.100' \
                ' || ip4.src == 1.1.1.101' \
                ' || ip4.src == 1.1.1.102)'

        self._test_add_sg_rule_acl_for_port(sg_rule,
                                            'to-lport',
                                            match)
        sg_rule['direction'] = 'egress'
        match = 'inport == "port-id" && ip4 && (ip4.dst == 1.1.1.100' \
                ' || ip4.dst == 1.1.1.101' \
                ' || ip4.dst == 1.1.1.102)'
        self._test_add_sg_rule_acl_for_port(sg_rule,
                                            'from-lport',
                                            match)

    def test__update_acls_compute_difference(self):
        lswitch_name = 'lswitch-1'
        port1 = {'id': 'port-id1',
                 'network_id': lswitch_name,
                 'fixed_ips': [{'subnet_id': 'subnet-id',
                                'ip_address': '1.1.1.101'},
                               {'subnet_id': 'subnet-id-v6',
                                'ip_address': '2001:0db8::1:0:0:1'}]}
        port2 = {'id': 'port-id2',
                 'network_id': lswitch_name,
                 'fixed_ips': [{'subnet_id': 'subnet-id',
                                'ip_address': '1.1.1.102'},
                               {'subnet_id': 'subnet-id-v6',
                                'ip_address': '2001:0db8::1:0:0:2'}]}
        ports = [port1, port2]
        # OLD ACLs, allow IPv4 communication
        aclport1_old1 = {'priority': 1002, 'direction': 'from-lport',
                         'lport': port1['id'], 'lswitch': lswitch_name,
                         'match': 'inport == %s && ip4 && (ip.src == %s)' %
                         (port1['id'], port1['fixed_ips'][0]['ip_address'])}
        aclport1_old2 = {'priority': 1002, 'direction': 'from-lport',
                         'lport': port1['id'], 'lswitch': lswitch_name,
                         'match': 'inport == %s && ip6 && (ip.src == %s)' %
                         (port1['id'], port1['fixed_ips'][1]['ip_address'])}
        aclport1_old3 = {'priority': 1002, 'direction': 'to-lport',
                         'lport': port1['id'], 'lswitch': lswitch_name,
                         'match': 'ip4 && (ip.src == %s)' %
                         (port2['fixed_ips'][0]['ip_address'])}
        port1_acls_old = [aclport1_old1, aclport1_old2, aclport1_old3]
        aclport2_old1 = {'priority': 1002, 'direction': 'from-lport',
                         'lport': port2['id'], 'lswitch': lswitch_name,
                         'match': 'inport == %s && ip4 && (ip.src == %s)' %
                         (port2['id'], port2['fixed_ips'][0]['ip_address'])}
        aclport2_old2 = {'priority': 1002, 'direction': 'from-lport',
                         'lport': port2['id'], 'lswitch': lswitch_name,
                         'match': 'inport == %s && ip6 && (ip.src == %s)' %
                         (port2['id'], port2['fixed_ips'][1]['ip_address'])}
        aclport2_old3 = {'priority': 1002, 'direction': 'to-lport',
                         'lport': port2['id'], 'lswitch': lswitch_name,
                         'match': 'ip4 && (ip.src == %s)' %
                         (port1['fixed_ips'][0]['ip_address'])}
        port2_acls_old = [aclport2_old1, aclport2_old2, aclport2_old3]
        acls_old_dict = {'%s' % (port1['id']): port1_acls_old,
                         '%s' % (port2['id']): port2_acls_old}
        acl_obj_dict = {str(aclport1_old1): 'row1',
                        str(aclport1_old2): 'row2',
                        str(aclport1_old3): 'row3',
                        str(aclport2_old1): 'row4',
                        str(aclport2_old2): 'row5',
                        str(aclport2_old3): 'row6'}
        # NEW ACLs, allow IPv6 communication
        aclport1_new1 = {'priority': 1002, 'direction': 'from-lport',
                         'lport': port1['id'], 'lswitch': lswitch_name,
                         'match': 'inport == %s && ip4 && (ip.src == %s)' %
                         (port1['id'], port1['fixed_ips'][0]['ip_address'])}
        aclport1_new2 = {'priority': 1002, 'direction': 'from-lport',
                         'lport': port1['id'], 'lswitch': lswitch_name,
                         'match': 'inport == %s && ip6 && (ip.src == %s)' %
                         (port1['id'], port1['fixed_ips'][1]['ip_address'])}
        aclport1_new3 = {'priority': 1002, 'direction': 'to-lport',
                         'lport': port1['id'], 'lswitch': lswitch_name,
                         'match': 'ip6 && (ip.src == %s)' %
                         (port2['fixed_ips'][1]['ip_address'])}
        port1_acls_new = [aclport1_new1, aclport1_new2, aclport1_new3]
        aclport2_new1 = {'priority': 1002, 'direction': 'from-lport',
                         'lport': port2['id'], 'lswitch': lswitch_name,
                         'match': 'inport == %s && ip4 && (ip.src == %s)' %
                         (port2['id'], port2['fixed_ips'][0]['ip_address'])}
        aclport2_new2 = {'priority': 1002, 'direction': 'from-lport',
                         'lport': port2['id'], 'lswitch': lswitch_name,
                         'match': 'inport == %s && ip6 && (ip.src == %s)' %
                         (port2['id'], port2['fixed_ips'][1]['ip_address'])}
        aclport2_new3 = {'priority': 1002, 'direction': 'to-lport',
                         'lport': port2['id'], 'lswitch': lswitch_name,
                         'match': 'ip6 && (ip.src == %s)' %
                         (port1['fixed_ips'][1]['ip_address'])}
        port2_acls_new = [aclport2_new1, aclport2_new2, aclport2_new3]
        acls_new_dict = {'%s' % (port1['id']): port1_acls_new,
                         '%s' % (port2['id']): port2_acls_new}

        acls_new_dict_copy = copy.deepcopy(acls_new_dict)

        # Invoke _compute_acl_differences
        update_cmd = cmd.UpdateACLsCommand(self.driver._nb_ovn,
                                           [lswitch_name],
                                           iter(ports),
                                           acls_new_dict
                                           )
        acl_dels, acl_adds =\
            update_cmd._compute_acl_differences(iter(ports),
                                                acls_old_dict,
                                                acls_new_dict,
                                                acl_obj_dict)
        # Expected Difference (Sorted)
        acl_del_exp = {lswitch_name: ['row3', 'row6']}
        acl_adds_exp = {lswitch_name:
                        [{'priority': 1002, 'direction': 'to-lport',
                          'match': 'ip6 && (ip.src == %s)' %
                          (port2['fixed_ips'][1]['ip_address'])},
                         {'priority': 1002, 'direction': 'to-lport',
                          'match': 'ip6 && (ip.src == %s)' %
                          (port1['fixed_ips'][1]['ip_address'])}]}
        self.assertEqual(acl_del_exp, acl_dels)
        self.assertEqual(acl_adds_exp, acl_adds)

        # make sure argument add_acl=False will take no affect in
        # need_compare=True scenario
        update_cmd_with_acl = cmd.UpdateACLsCommand(self.driver._nb_ovn,
                                                    [lswitch_name],
                                                    iter(ports),
                                                    acls_new_dict_copy,
                                                    need_compare=True,
                                                    is_add_acl=False)
        new_acl_dels, new_acl_adds =\
            update_cmd_with_acl._compute_acl_differences(iter(ports),
                                                         acls_old_dict,
                                                         acls_new_dict_copy,
                                                         acl_obj_dict)
        self.assertEqual(acl_dels, new_acl_dels)
        self.assertEqual(acl_adds, new_acl_adds)

    def test__get_update_data_without_compare(self):
        lswitch_name = 'lswitch-1'
        port1 = {'id': 'port-id1',
                 'network_id': lswitch_name,
                 'fixed_ips': mock.Mock()}
        port2 = {'id': 'port-id2',
                 'network_id': lswitch_name,
                 'fixed_ips': mock.Mock()}
        ports = [port1, port2]
        aclport1_new = {'priority': 1002, 'direction': 'to-lport',
                        'match': 'outport == %s && ip4 && icmp4' %
                        (port1['id']), 'external_ids': {}}
        aclport2_new = {'priority': 1002, 'direction': 'to-lport',
                        'match': 'outport == %s && ip4 && icmp4' %
                        (port2['id']), 'external_ids': {}}
        acls_new_dict = {'%s' % (port1['id']): aclport1_new,
                         '%s' % (port2['id']): aclport2_new}

        # test for creating new acls
        update_cmd_add_acl = cmd.UpdateACLsCommand(self.driver._nb_ovn,
                                                   [lswitch_name],
                                                   iter(ports),
                                                   acls_new_dict,
                                                   need_compare=False,
                                                   is_add_acl=True)
        lswitch_dict, acl_del_dict, acl_add_dict = \
            update_cmd_add_acl._get_update_data_without_compare()
        self.assertIn('neutron-lswitch-1', lswitch_dict)
        self.assertEqual({}, acl_del_dict)
        expected_acls = {'neutron-lswitch-1': [aclport1_new, aclport2_new]}
        self.assertEqual(expected_acls, acl_add_dict)

        # test for deleting existing acls
        acl1 = mock.Mock(
            match='outport == port-id1 && ip4 && icmp4', external_ids={})
        acl2 = mock.Mock(
            match='outport == port-id2 && ip4 && icmp4', external_ids={})
        acl3 = mock.Mock(
            match='outport == port-id1 && ip4 && (ip4.src == fake_ip)',
            external_ids={})
        lswitch_obj = mock.Mock(
            name='neutron-lswitch-1', acls=[acl1, acl2, acl3])
        with mock.patch('ovsdbapp.backend.ovs_idl.idlutils.row_by_value',
                        return_value=lswitch_obj):
            update_cmd_del_acl = cmd.UpdateACLsCommand(self.driver._nb_ovn,
                                                       [lswitch_name],
                                                       iter(ports),
                                                       acls_new_dict,
                                                       need_compare=False,
                                                       is_add_acl=False)
            lswitch_dict, acl_del_dict, acl_add_dict = \
                update_cmd_del_acl._get_update_data_without_compare()
            self.assertIn('neutron-lswitch-1', lswitch_dict)
            expected_acls = {'neutron-lswitch-1': [acl1, acl2]}
            self.assertEqual(expected_acls, acl_del_dict)
            self.assertEqual({}, acl_add_dict)

    def test_acl_protocol_and_ports_for_tcp_udp_and_sctp_number(self):
        sg_rule = {'port_range_min': None,
                   'port_range_max': None}

        sg_rule['protocol'] = str(const.PROTO_NUM_TCP)
        match = ovn_acl.acl_protocol_and_ports(sg_rule, None)
        self.assertEqual(' && tcp', match)

        sg_rule['protocol'] = str(const.PROTO_NUM_UDP)
        match = ovn_acl.acl_protocol_and_ports(sg_rule, None)
        self.assertEqual(' && udp', match)

        sg_rule['protocol'] = str(const.PROTO_NUM_SCTP)
        match = ovn_acl.acl_protocol_and_ports(sg_rule, None)
        self.assertEqual(' && sctp', match)

    def test_acl_protocol_and_ports_for_tcp_udp_and_sctp_number_one(self):
        sg_rule = {'port_range_min': 22,
                   'port_range_max': 22}

        sg_rule['protocol'] = str(const.PROTO_NUM_TCP)
        match = ovn_acl.acl_protocol_and_ports(sg_rule, None)
        self.assertEqual(' && tcp && tcp.dst == 22', match)

        sg_rule['protocol'] = str(const.PROTO_NUM_UDP)
        match = ovn_acl.acl_protocol_and_ports(sg_rule, None)
        self.assertEqual(' && udp && udp.dst == 22', match)

        sg_rule['protocol'] = str(const.PROTO_NUM_SCTP)
        match = ovn_acl.acl_protocol_and_ports(sg_rule, None)
        self.assertEqual(' && sctp && sctp.dst == 22', match)

    def test_acl_protocol_and_ports_for_tcp_udp_and_sctp_number_range(self):
        sg_rule = {'port_range_min': 21,
                   'port_range_max': 23}

        sg_rule['protocol'] = str(const.PROTO_NUM_TCP)
        match = ovn_acl.acl_protocol_and_ports(sg_rule, None)
        self.assertEqual(' && tcp && tcp.dst >= 21 && tcp.dst <= 23', match)

        sg_rule['protocol'] = str(const.PROTO_NUM_UDP)
        match = ovn_acl.acl_protocol_and_ports(sg_rule, None)
        self.assertEqual(' && udp && udp.dst >= 21 && udp.dst <= 23', match)

        sg_rule['protocol'] = str(const.PROTO_NUM_SCTP)
        match = ovn_acl.acl_protocol_and_ports(sg_rule, None)
        self.assertEqual(' && sctp && sctp.dst >= 21 && sctp.dst <= 23', match)

    def test_acl_protocol_and_ports_for_ipv6_icmp_protocol(self):
        sg_rule = {'port_range_min': None,
                   'port_range_max': None}
        icmp = 'icmp6'
        expected_match = ' && icmp6'

        sg_rule['protocol'] = const.PROTO_NAME_ICMP
        match = ovn_acl.acl_protocol_and_ports(sg_rule, icmp)
        self.assertEqual(expected_match, match)

        sg_rule['protocol'] = str(const.PROTO_NUM_ICMP)
        match = ovn_acl.acl_protocol_and_ports(sg_rule, icmp)
        self.assertEqual(expected_match, match)

        sg_rule['protocol'] = const.PROTO_NAME_IPV6_ICMP
        match = ovn_acl.acl_protocol_and_ports(sg_rule, icmp)
        self.assertEqual(expected_match, match)

        sg_rule['protocol'] = const.PROTO_NAME_IPV6_ICMP_LEGACY
        match = ovn_acl.acl_protocol_and_ports(sg_rule, icmp)
        self.assertEqual(expected_match, match)

        sg_rule['protocol'] = str(const.PROTO_NUM_IPV6_ICMP)
        match = ovn_acl.acl_protocol_and_ports(sg_rule, icmp)
        self.assertEqual(expected_match, match)

    def test_acl_protocol_and_ports_for_icmp4_and_icmp6_port_range(self):
        match_list = [
            (None, None, ' && icmp4'),
            (0, None, ' && icmp4 && icmp4.type == 0'),
            (0, 0, ' && icmp4 && icmp4.type == 0 && icmp4.code == 0'),
            (0, 5, ' && icmp4 && icmp4.type == 0 && icmp4.code == 5')]
        v6_match_list = [
            (None, None, ' && icmp6'),
            (133, None, ' && icmp6 && icmp6.type == 133'),
            (1, 1, ' && icmp6 && icmp6.type == 1 && icmp6.code == 1'),
            (138, 1, ' && icmp6 && icmp6.type == 138 && icmp6.code == 1')]

        sg_rule = {'protocol': const.PROTO_NAME_ICMP}
        icmp = 'icmp4'
        for pmin, pmax, expected_match in match_list:
            sg_rule['port_range_min'] = pmin
            sg_rule['port_range_max'] = pmax
            match = ovn_acl.acl_protocol_and_ports(sg_rule, icmp)
            self.assertEqual(expected_match, match)

        sg_rule = {'protocol': const.PROTO_NAME_IPV6_ICMP}
        icmp = 'icmp6'
        for pmin, pmax, expected_match in v6_match_list:
            sg_rule['port_range_min'] = pmin
            sg_rule['port_range_max'] = pmax
            match = ovn_acl.acl_protocol_and_ports(sg_rule, icmp)
            self.assertEqual(expected_match, match)

    def test_acl_protocol_and_ports_protocol_not_supported(self):
        sg_rule = {'port_range_min': None,
                   'port_range_max': None}
        sg_rule['protocol'] = '1234567'
        self.assertRaises(ovn_acl.ProtocolNotSupported,
                          ovn_acl.acl_protocol_and_ports, sg_rule, None)

    def test_acl_protocol_and_ports_protocol_range(self):
        sg_rule = {'port_range_min': None,
                   'port_range_max': None}

        # For more common protocols such as TCP, UDP and ICMP, we
        # prefer to use the protocol name in the match string instead of
        # the protocol number (e.g: the word "tcp" instead of "ip.proto
        # == 6"). This improves the readability/debbugability when
        # troubleshooting the ACLs
        skip_protos = (const.PROTO_NUM_TCP, const.PROTO_NUM_UDP,
                       const.PROTO_NUM_SCTP, const.PROTO_NUM_ICMP,
                       const.PROTO_NUM_IPV6_ICMP)

        for proto in range(256):
            if proto in skip_protos:
                continue
            sg_rule['protocol'] = str(proto)
            match = ovn_acl.acl_protocol_and_ports(sg_rule, None)
            self.assertEqual(' && ip.proto == %s' % proto, match)

    def test_acl_protocol_and_ports_name_to_number(self):
        sg_rule = {'port_range_min': None,
                   'port_range_max': None}

        sg_rule['protocol'] = str(const.PROTO_NAME_OSPF)
        match = ovn_acl.acl_protocol_and_ports(sg_rule, None)
        self.assertEqual(' && ip.proto == 89', match)

    def test_acl_direction(self):
        sg_rule = fakes.FakeSecurityGroupRule.create_one_security_group_rule({
            'direction': 'ingress'
        }).info()

        match = ovn_acl.acl_direction(sg_rule, self.fake_port)
        self.assertEqual('outport == "' + self.fake_port['id'] + '"', match)

        sg_rule['direction'] = 'egress'
        match = ovn_acl.acl_direction(sg_rule, self.fake_port)
        self.assertEqual('inport == "' + self.fake_port['id'] + '"', match)

    def test_acl_ethertype(self):
        sg_rule = fakes.FakeSecurityGroupRule.create_one_security_group_rule({
            'ethertype': 'IPv4'
        }).info()

        match, ip_version, icmp = ovn_acl.acl_ethertype(sg_rule)
        self.assertEqual(' && ip4', match)
        self.assertEqual('ip4', ip_version)
        self.assertEqual('icmp4', icmp)

        sg_rule['ethertype'] = 'IPv6'
        match, ip_version, icmp = ovn_acl.acl_ethertype(sg_rule)
        self.assertEqual(' && ip6', match)
        self.assertEqual('ip6', ip_version)
        self.assertEqual('icmp6', icmp)

        sg_rule['ethertype'] = 'IPv10'
        match, ip_version, icmp = ovn_acl.acl_ethertype(sg_rule)
        self.assertEqual('', match)
        self.assertIsNone(ip_version)
        self.assertIsNone(icmp)

    def test_acl_remote_ip_prefix(self):
        sg_rule = fakes.FakeSecurityGroupRule.create_one_security_group_rule({
            'direction': 'ingress',
            'remote_ip_prefix': None
        }).info()
        ip_version = 'ip4'
        remote_ip_prefix = '10.10.0.0/24'

        match = ovn_acl.acl_remote_ip_prefix(sg_rule, ip_version)
        self.assertEqual('', match)

        sg_rule['remote_ip_prefix'] = remote_ip_prefix
        match = ovn_acl.acl_remote_ip_prefix(sg_rule, ip_version)
        expected_match = ' && %s.src == %s' % (ip_version, remote_ip_prefix)
        self.assertEqual(expected_match, match)

        sg_rule['direction'] = 'egress'
        match = ovn_acl.acl_remote_ip_prefix(sg_rule, ip_version)
        expected_match = ' && %s.dst == %s' % (ip_version, remote_ip_prefix)
        self.assertEqual(expected_match, match)

    def test_acl_remote_group_id(self):
        sg_rule = fakes.FakeSecurityGroupRule.create_one_security_group_rule({
            'direction': 'ingress',
            'remote_group_id': None
        }).info()
        ip_version = 'ip4'
        sg_id = sg_rule['security_group_id']

        addrset_name = ovn_utils.ovn_addrset_name(sg_id, ip_version)

        match = ovn_acl.acl_remote_group_id(sg_rule, ip_version)
        self.assertEqual('', match)

        sg_rule['remote_group_id'] = sg_id
        match = ovn_acl.acl_remote_group_id(sg_rule, ip_version)
        self.assertEqual(' && ip4.src == $' + addrset_name, match)

        sg_rule['direction'] = 'egress'
        match = ovn_acl.acl_remote_group_id(sg_rule, ip_version)
        self.assertEqual(' && ip4.dst == $' + addrset_name, match)

    def _test_update_acls_for_security_group(self, use_cache=True):
        sg = fakes.FakeSecurityGroup.create_one_security_group().info()
        remote_sg = fakes.FakeSecurityGroup.create_one_security_group().info()
        sg_rule = fakes.FakeSecurityGroupRule.create_one_security_group_rule({
            'security_group_id': sg['id'],
            'remote_group_id': remote_sg['id']
        }).info()
        port = fakes.FakePort.create_one_port({
            'security_groups': [sg['id']]
        }).info()
        self.plugin.get_ports.return_value = [port]
        if use_cache:
            sg_ports_cache = {sg['id']: [{'port_id': port['id']}],
                              remote_sg['id']: []}
        else:
            sg_ports_cache = None
            self.plugin._get_port_security_group_bindings.return_value = \
                [{'port_id': port['id']}]

        # Build ACL for validation.
        expected_acl = ovn_acl._add_sg_rule_acl_for_port(port, sg_rule)
        expected_acl.pop('lport')
        expected_acl.pop('lswitch')

        # Validate ACLs when port has security groups.
        ovn_acl.update_acls_for_security_group(self.plugin,
                                               self.admin_context,
                                               self.driver._nb_ovn,
                                               sg['id'],
                                               sg_rule,
                                               sg_ports_cache=sg_ports_cache)
        self.driver._nb_ovn.update_acls.assert_called_once_with(
            [port['network_id']],
            mock.ANY,
            {port['id']: expected_acl},
            need_compare=False,
            is_add_acl=True
        )

    def test_update_acls_for_security_group_cache(self):
        self._test_update_acls_for_security_group(use_cache=True)

    def test_update_acls_for_security_group_no_cache(self):
        self._test_update_acls_for_security_group(use_cache=False)

    def test_acl_port_ips(self):
        port4 = fakes.FakePort.create_one_port({
            'fixed_ips': [{'subnet_id': 'subnet-ipv4',
                           'ip_address': '10.0.0.1'}],
        }).info()
        port46 = fakes.FakePort.create_one_port({
            'fixed_ips': [{'subnet_id': 'subnet-ipv4',
                           'ip_address': '10.0.0.2'},
                          {'subnet_id': 'subnet-ipv6',
                           'ip_address': 'fde3:d45:df72::1'}],
        }).info()
        port6 = fakes.FakePort.create_one_port({
            'fixed_ips': [{'subnet_id': 'subnet-ipv6',
                           'ip_address': '2001:db8::8'}],
        }).info()

        addresses = ovn_acl.acl_port_ips(port4)
        self.assertEqual({'ip4': [port4['fixed_ips'][0]['ip_address']],
                          'ip6': []},
                         addresses)

        addresses = ovn_acl.acl_port_ips(port46)
        self.assertEqual({'ip4': [port46['fixed_ips'][0]['ip_address']],
                          'ip6': [port46['fixed_ips'][1]['ip_address']]},
                         addresses)

        addresses = ovn_acl.acl_port_ips(port6)
        self.assertEqual({'ip4': [],
                          'ip6': [port6['fixed_ips'][0]['ip_address']]},
                         addresses)

    def test_sg_disabled(self):
        sg = fakes.FakeSecurityGroup.create_one_security_group().info()
        port = fakes.FakePort.create_one_port({
            'security_groups': [sg['id']]
        }).info()

        with mock.patch('networking_ovn.common.acl.is_sg_enabled',
                        return_value=False):
            acl_list = ovn_acl.add_acls(self.plugin,
                                        self.admin_context,
                                        port, {}, {}, self.driver._ovn)
            self.assertEqual([], acl_list)

            ovn_acl.update_acls_for_security_group(self.plugin,
                                                   self.admin_context,
                                                   self.driver._ovn,
                                                   sg['id'],
                                                   None)
            self.driver._ovn.update_acls.assert_not_called()

            addresses = ovn_acl.acl_port_ips(port)
            self.assertEqual({'ip4': [], 'ip6': []}, addresses)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\common\test_maintenance.py
===========File Type===========
.py
===========File Content===========
# Copyright 2017 Red Hat, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mock

from futurist import periodics
from neutron.tests.unit.plugins.ml2 import test_security_group as test_sg
from neutron_lib.db import api as db_api

from networking_ovn.common import constants
from networking_ovn.common import maintenance
from networking_ovn.common import utils
from networking_ovn.db import maintenance as db_maint
from networking_ovn.db import revision as db_rev
from networking_ovn import ovn_db_sync
from networking_ovn.tests.unit.db import base as db_base


@mock.patch.object(maintenance.DBInconsistenciesPeriodics,
                   'has_lock', mock.PropertyMock(return_value=True))
class TestDBInconsistenciesPeriodics(db_base.DBTestCase,
                                     test_sg.Ml2SecurityGroupsTestCase):

    def setUp(self):
        super(TestDBInconsistenciesPeriodics, self).setUp()
        self.net = self._make_network(
            self.fmt, name='net1', admin_state_up=True)['network']
        self.port = self._make_port(
            self.fmt, self.net['id'], name='port1')['port']
        self.fake_ovn_client = mock.Mock()
        self.periodic = maintenance.DBInconsistenciesPeriodics(
            self.fake_ovn_client)
        self.session = db_api.get_writer_session()

    @mock.patch.object(maintenance.DBInconsistenciesPeriodics,
                       '_fix_create_update')
    @mock.patch.object(db_maint, 'get_inconsistent_resources')
    def test_check_for_inconsistencies(self, mock_get_incon_res, mock_fix_net):
        fake_row = mock.Mock(resource_type=constants.TYPE_NETWORKS)
        mock_get_incon_res.return_value = [fake_row, ]
        self.periodic.check_for_inconsistencies()
        mock_fix_net.assert_called_once_with(fake_row)

    def _test_migrate_to_port_groups_helper(self, pg_supported, a_sets,
                                            migration_expected, never_again):
        self.fake_ovn_client._nb_idl.is_port_groups_supported.return_value = (
            pg_supported)
        self.fake_ovn_client._nb_idl.get_address_sets.return_value = a_sets
        with mock.patch.object(ovn_db_sync.OvnNbSynchronizer,
                               'migrate_to_port_groups') as mtpg:
            if never_again:
                self.assertRaises(periodics.NeverAgain,
                                  self.periodic.migrate_to_port_groups)
            else:
                self.periodic.migrate_to_port_groups()

            if migration_expected:
                mtpg.assert_called_once()
            else:
                mtpg.assert_not_called()

    def test_migrate_to_port_groups_port_groups_not_supported(self):
        self._test_migrate_to_port_groups_helper(pg_supported=False,
                                                 a_sets=None,
                                                 migration_expected=False,
                                                 never_again=True)

    def test_migrate_to_port_groups_not_needed(self):
        self._test_migrate_to_port_groups_helper(pg_supported=True,
                                                 a_sets=None,
                                                 migration_expected=False,
                                                 never_again=True)

    def test_migrate_to_port_groups(self):
        # Check normal migration path: if port groups are supported by the
        # schema and the migration has to be done, it will take place and
        # won't be attempted in the future.
        self._test_migrate_to_port_groups_helper(pg_supported=True,
                                                 a_sets=['as1', 'as2'],
                                                 migration_expected=True,
                                                 never_again=True)

    def test_migrate_to_port_groups_no_lock(self):
        with mock.patch.object(maintenance.DBInconsistenciesPeriodics,
                               'has_lock', mock.PropertyMock(
                                   return_value=False)):
            # Check that if this worker doesn't have the lock, it won't
            # perform the migration and it will try again later.
            self._test_migrate_to_port_groups_helper(pg_supported=True,
                                                     a_sets=['as1', 'as2'],
                                                     migration_expected=False,
                                                     never_again=False)

    def _test_fix_create_update_network(self, ovn_rev, neutron_rev):
        self.net['revision_number'] = neutron_rev

        # Create an entry to the revision_numbers table and assert the
        # initial revision_number for our test object is the expected
        db_rev.create_initial_revision(
            self.net['id'], constants.TYPE_NETWORKS, self.session,
            revision_number=ovn_rev)
        row = db_rev.get_revision_row(self.net['id'])
        self.assertEqual(ovn_rev, row.revision_number)

        if ovn_rev < 0:
            self.fake_ovn_client._nb_idl.get_lswitch.return_value = None
        else:
            fake_ls = mock.Mock(external_ids={
                constants.OVN_REV_NUM_EXT_ID_KEY: ovn_rev})
            self.fake_ovn_client._nb_idl.get_lswitch.return_value = fake_ls

        self.fake_ovn_client._plugin.get_network.return_value = self.net
        self.periodic._fix_create_update(row)

        # Since the revision number was < 0, make sure create_network()
        # is invoked with the latest version of the object in the neutron
        # database
        if ovn_rev < 0:
            self.fake_ovn_client.create_network.assert_called_once_with(
                self.net)
        # If the revision number is > 0 it means that the object already
        # exist and we just need to update to match the latest in the
        # neutron database so, update_network() should be called.
        else:
            self.fake_ovn_client.update_network.assert_called_once_with(
                self.net)

    def test_fix_network_create(self):
        self._test_fix_create_update_network(ovn_rev=-1, neutron_rev=2)

    def test_fix_network_update(self):
        self._test_fix_create_update_network(ovn_rev=5, neutron_rev=7)

    def _test_fix_create_update_port(self, ovn_rev, neutron_rev):
        self.port['revision_number'] = neutron_rev

        # Create an entry to the revision_numbers table and assert the
        # initial revision_number for our test object is the expected
        db_rev.create_initial_revision(
            self.port['id'], constants.TYPE_PORTS, self.session,
            revision_number=ovn_rev)
        row = db_rev.get_revision_row(self.port['id'])
        self.assertEqual(ovn_rev, row.revision_number)

        if ovn_rev < 0:
            self.fake_ovn_client._nb_idl.get_lswitch_port.return_value = None
        else:
            fake_lsp = mock.Mock(external_ids={
                constants.OVN_REV_NUM_EXT_ID_KEY: ovn_rev})
            self.fake_ovn_client._nb_idl.get_lswitch_port.return_value = (
                fake_lsp)

        self.fake_ovn_client._plugin.get_port.return_value = self.port
        self.periodic._fix_create_update(row)

        # Since the revision number was < 0, make sure create_port()
        # is invoked with the latest version of the object in the neutron
        # database
        if ovn_rev < 0:
            self.fake_ovn_client.create_port.assert_called_once_with(
                self.port)
        # If the revision number is > 0 it means that the object already
        # exist and we just need to update to match the latest in the
        # neutron database so, update_port() should be called.
        else:
            self.fake_ovn_client.update_port.assert_called_once_with(
                self.port)

    def test_fix_port_create(self):
        self._test_fix_create_update_port(ovn_rev=-1, neutron_rev=2)

    def test_fix_port_update(self):
        self._test_fix_create_update_port(ovn_rev=5, neutron_rev=7)

    @mock.patch.object(db_rev, 'bump_revision')
    def _test_fix_security_group_create(self, mock_bump, revision_number):
        sg_name = utils.ovn_addrset_name('fake_id', 'ip4')
        sg = self._make_security_group(self.fmt, sg_name, '')['security_group']

        db_rev.create_initial_revision(
            sg['id'], constants.TYPE_SECURITY_GROUPS, self.session,
            revision_number=revision_number)
        row = db_rev.get_revision_row(sg['id'])
        self.assertEqual(revision_number, row.revision_number)

        if revision_number < 0:
            self.fake_ovn_client._nb_idl.get_address_set.return_value = None
            self.fake_ovn_client._nb_idl.get_port_group.return_value = None
        else:
            self.fake_ovn_client._nb_idl.get_address_set.return_value = (
                mock.sentinel.AddressSet)

        self.fake_ovn_client._plugin.get_security_group.return_value = sg
        self.periodic._fix_create_update(row)

        if revision_number < 0:
            self.fake_ovn_client.create_security_group.assert_called_once_with(
                sg)
        else:
            # If the object already exist let's make sure we just bump
            # the revision number in the ovn_revision_numbers table
            self.assertFalse(self.fake_ovn_client.create_security_group.called)
            mock_bump.assert_called_once_with(
                sg, constants.TYPE_SECURITY_GROUPS)

    def test_fix_security_group_create_doesnt_exist(self):
        self._test_fix_security_group_create(revision_number=-1)

    def test_fix_security_group_create_version_mismatch(self):
        self._test_fix_security_group_create(revision_number=2)

    def test__create_lrouter_port(self):
        port = {'id': 'port-id',
                'device_id': 'router-id'}
        self.periodic._create_lrouter_port(port)
        l3_mock = self.periodic._ovn_client._l3_plugin
        l3_mock.add_router_interface.assert_called_once_with(
            mock.ANY, port['device_id'], {'port_id': port['id']},
            may_exist=True)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\common\test_utils.py
===========File Type===========
.py
===========File Content===========
# Copyright 2018 Red Hat, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import fixtures

from networking_ovn.common import utils
from networking_ovn.tests import base

RESOLV_CONF_TEMPLATE = """# TEST TEST TEST
# Geneated by OVN test
nameserver 10.0.0.1
#nameserver 10.0.0.2
nameserver 10.0.0.3
nameserver foo 10.0.0.4
nameserver aef0::4
foo 10.0.0.5
"""


class TestUtils(base.TestCase):

    def test_get_system_dns_resolvers(self):
        tempdir = self.useFixture(fixtures.TempDir()).path
        resolver_file_name = tempdir + '/resolv.conf'
        tmp_resolv_file = open(resolver_file_name, 'w')
        tmp_resolv_file.writelines(RESOLV_CONF_TEMPLATE)
        tmp_resolv_file.close()
        expected_dns_resolvers = ['10.0.0.1', '10.0.0.3']
        observed_dns_resolvers = utils.get_system_dns_resolvers(
            resolver_file=resolver_file_name)
        self.assertEqual(expected_dns_resolvers, observed_dns_resolvers)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\common\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\db\base.py
===========File Type===========
.py
===========File Content===========
# Copyright 2017 Red Hat, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from neutron.tests.unit.testlib_api import SqlTestCaseLight
from neutron_lib import context

from networking_ovn.db import models


class DBTestCase(SqlTestCaseLight):

    def setUp(self):
        super(DBTestCase, self).setUp()
        self.session = context.get_admin_context().session

    def tearDown(self):
        super(DBTestCase, self).tearDown()
        self.session.query(models.OVNRevisionNumbers).delete()




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\db\test_maintenance.py
===========File Type===========
.py
===========File Content===========
# Copyright 2017 Red Hat, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from neutron.api import extensions
from neutron.common import config
import neutron.extensions
from neutron.services.revisions import revision_plugin
from neutron.tests.unit.extensions import test_l3
from neutron.tests.unit.extensions import test_securitygroup
from neutron_lib import constants as n_const
from neutron_lib.db import api as db_api

from networking_ovn.common import constants
from networking_ovn.db import maintenance as db_maint
from networking_ovn.db import revision as db_rev


EXTENSIONS_PATH = ':'.join(neutron.extensions.__path__)
PLUGIN_CLASS = (
    'networking_ovn.tests.unit.db.test_maintenance.TestMaintenancePlugin')


class TestMaintenancePlugin(test_securitygroup.SecurityGroupTestPlugin,
                            test_l3.TestL3NatBasePlugin):

    __native_pagination_support = True
    __native_sorting_support = True

    supported_extension_aliases = ['external-net', 'security-group']


class TestMaintenance(test_securitygroup.SecurityGroupsTestCase,
                      test_l3.L3NatTestCaseMixin):

    def setUp(self):
        service_plugins = {
            'router':
            'neutron.tests.unit.extensions.test_l3.TestL3NatServicePlugin'}
        l3_plugin = test_l3.TestL3NatServicePlugin()
        sec_plugin = test_securitygroup.SecurityGroupTestPlugin()
        ext_mgr = extensions.PluginAwareExtensionManager(
            EXTENSIONS_PATH, {'router': l3_plugin, 'sec': sec_plugin}
        )
        super(TestMaintenance, self).setUp(plugin=PLUGIN_CLASS,
                                           service_plugins=service_plugins)
        app = config.load_paste_app('extensions_test_app')
        self.ext_api = extensions.ExtensionMiddleware(app, ext_mgr=ext_mgr)
        self.session = db_api.get_writer_session()
        revision_plugin.RevisionPlugin()
        self.net = self._make_network(self.fmt, 'net1', True)['network']

    def test_get_inconsistent_resources(self):
        # Set the intial revision to -1 to force it to be incosistent
        db_rev.create_initial_revision(
            self.net['id'], constants.TYPE_NETWORKS, self.session,
            revision_number=-1)
        res = db_maint.get_inconsistent_resources()
        self.assertEqual(1, len(res))
        self.assertEqual(self.net['id'], res[0].resource_uuid)

    def test_get_inconsistent_resources_consistent(self):
        # Set the initial revision to 0 which is the initial revision_number
        # for recently created resources
        db_rev.create_initial_revision(
            self.net['id'], constants.TYPE_NETWORKS, self.session,
            revision_number=0)
        res = db_maint.get_inconsistent_resources()
        # Assert nothing is inconsistent
        self.assertEqual([], res)

    def test_get_deleted_resources(self):
        db_rev.create_initial_revision(
            self.net['id'], constants.TYPE_NETWORKS, self.session,
            revision_number=0)
        self._delete('networks', self.net['id'])
        res = db_maint.get_deleted_resources()

        self.assertEqual(1, len(res))
        self.assertEqual(self.net['id'], res[0].resource_uuid)
        self.assertIsNone(res[0].standard_attr_id)

    def _prepare_resources_for_ordering_test(self, delete=False):
        subnet = self._make_subnet(self.fmt, {'network': self.net}, '10.0.0.1',
                                   '10.0.0.0/24')['subnet']
        self._set_net_external(self.net['id'])
        info = {'network_id': self.net['id']}
        router = self._make_router(self.fmt, None,
                                   external_gateway_info=info)['router']
        fip = self._make_floatingip(self.fmt, self.net['id'])['floatingip']
        port = self._make_port(self.fmt, self.net['id'])['port']
        sg = self._make_security_group(self.fmt, 'sg1', '')['security_group']
        rule = self._build_security_group_rule(
            sg['id'], 'ingress', n_const.PROTO_NUM_TCP)
        sg_rule = self._make_security_group_rule(
            self.fmt, rule)['security_group_rule']

        db_rev.create_initial_revision(
            router['id'], constants.TYPE_ROUTERS, self.session)
        db_rev.create_initial_revision(
            subnet['id'], constants.TYPE_SUBNETS, self.session)
        db_rev.create_initial_revision(
            fip['id'], constants.TYPE_FLOATINGIPS, self.session)
        db_rev.create_initial_revision(
            port['id'], constants.TYPE_PORTS, self.session)
        db_rev.create_initial_revision(
            port['id'], constants.TYPE_ROUTER_PORTS, self.session)
        db_rev.create_initial_revision(
            sg['id'], constants.TYPE_SECURITY_GROUPS, self.session)
        db_rev.create_initial_revision(
            sg_rule['id'], constants.TYPE_SECURITY_GROUP_RULES, self.session)
        db_rev.create_initial_revision(
            self.net['id'], constants.TYPE_NETWORKS, self.session)

        if delete:
            self._delete('security-group-rules', sg_rule['id'])
            self._delete('floatingips', fip['id'])
            self._delete('ports', port['id'])
            self._delete('security-groups', sg['id'])
            self._delete('routers', router['id'])
            self._delete('subnets', subnet['id'])
            self._delete('networks', self.net['id'])

    def test_get_inconsistent_resources_order(self):
        self._prepare_resources_for_ordering_test()
        res = db_maint.get_inconsistent_resources()
        actual_order = tuple(r.resource_type for r in res)
        self.assertEqual(constants._TYPES_PRIORITY_ORDER, actual_order)

    def test_get_deleted_resources_order(self):
        self._prepare_resources_for_ordering_test(delete=True)
        res = db_maint.get_deleted_resources()
        actual_order = tuple(r.resource_type for r in res)
        self.assertEqual(tuple(reversed(constants._TYPES_PRIORITY_ORDER)),
                         actual_order)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\db\test_revision.py
===========File Type===========
.py
===========File Content===========
# Copyright 2017 Red Hat, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mock

from neutron.tests.unit.plugins.ml2 import test_plugin
from neutron_lib.db import api as db_api

from networking_ovn.common import constants
from networking_ovn.db import revision as db_rev
from networking_ovn.tests.unit.db import base as db_base


class TestRevisionNumber(db_base.DBTestCase, test_plugin.Ml2PluginV2TestCase):

    def setUp(self):
        super(TestRevisionNumber, self).setUp()
        res = self._create_network(fmt=self.fmt, name='net',
                                   admin_state_up=True)
        self.net = self.deserialize(self.fmt, res)['network']
        self.session = db_api.get_writer_session()

    def test_bump_revision(self):
        db_rev.create_initial_revision(self.net['id'], constants.TYPE_NETWORKS,
                                       self.session)
        self.net['revision_number'] = 123
        db_rev.bump_revision(self.net, constants.TYPE_NETWORKS)
        row = db_rev.get_revision_row(self.net['id'])
        self.assertEqual(123, row.revision_number)

    def test_bump_older_revision(self):
        db_rev.create_initial_revision(self.net['id'], constants.TYPE_NETWORKS,
                                       self.session, revision_number=123)
        self.net['revision_number'] = 1
        db_rev.bump_revision(self.net, constants.TYPE_NETWORKS)
        # Assert the revision number wasn't bumped
        row = db_rev.get_revision_row(self.net['id'])
        self.assertEqual(123, row.revision_number)

    @mock.patch.object(db_rev.LOG, 'warning')
    def test_bump_revision_row_not_found(self, mock_log):
        self.net['revision_number'] = 123
        db_rev.bump_revision(self.net, constants.TYPE_NETWORKS)
        # Assert the revision number wasn't bumped
        row = db_rev.get_revision_row(self.net['id'])
        self.assertEqual(123, row.revision_number)
        self.assertIn('No revision row found for', mock_log.call_args[0][0])

    def test_delete_revision(self):
        db_rev.create_initial_revision(self.net['id'], constants.TYPE_NETWORKS,
                                       self.session)
        db_rev.delete_revision(self.net['id'], constants.TYPE_NETWORKS)
        row = db_rev.get_revision_row(self.net['id'])
        self.assertIsNone(row)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\db\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\l3\test_l3_ovn.py
===========File Type===========
.py
===========File Content===========
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import copy

import mock
from neutron.services.revisions import revision_plugin
from neutron.tests.unit.api import test_extensions
from neutron.tests.unit.extensions import test_extraroute
from neutron.tests.unit.extensions import test_l3
from neutron.tests.unit.extensions import test_l3_ext_gw_mode as test_l3_gw
from neutron_lib.api.definitions import portbindings
from neutron_lib.callbacks import events
from neutron_lib.callbacks import resources
from neutron_lib import constants
from neutron_lib import exceptions as n_exc
from neutron_lib.plugins import constants as plugin_constants
from neutron_lib.plugins import directory
from oslo_config import cfg

from networking_ovn.common import config
from networking_ovn.common import constants as ovn_const
from networking_ovn.common import utils
from networking_ovn.tests.unit import fakes
from networking_ovn.tests.unit.ml2 import test_mech_driver


class OVNL3RouterPlugin(test_mech_driver.OVNMechanismDriverTestCase):

    l3_plugin = 'networking_ovn.l3.l3_ovn.OVNL3RouterPlugin'

    def _start_mock(self, path, return_value, new_callable=None):
        patcher = mock.patch(path, return_value=return_value,
                             new_callable=new_callable)
        patch = patcher.start()
        self.addCleanup(patcher.stop)
        return patch

    def setUp(self):
        super(OVNL3RouterPlugin, self).setUp()
        revision_plugin.RevisionPlugin()
        network_attrs = {'router:external': True}
        self.fake_network = \
            fakes.FakeNetwork.create_one_network(attrs=network_attrs).info()
        self.fake_router_port = {'device_id': '',
                                 'device_owner': 'network:router_interface',
                                 'mac_address': 'aa:aa:aa:aa:aa:aa',
                                 'fixed_ips': [{'ip_address': '10.0.0.100',
                                                'subnet_id': 'subnet-id'}],
                                 'id': 'router-port-id'}
        self.fake_router_port_assert = {
            'lrouter': 'neutron-router-id',
            'mac': 'aa:aa:aa:aa:aa:aa',
            'name': 'lrp-router-port-id',
            'may_exist': True,
            'networks': ['10.0.0.100/24'],
            'external_ids': {
                ovn_const.OVN_SUBNET_EXT_IDS_KEY: 'subnet-id',
                ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'}}
        self.fake_router_ports = [self.fake_router_port]
        self.fake_subnet = {'id': 'subnet-id',
                            'ip_version': 4,
                            'cidr': '10.0.0.0/24'}
        self.fake_router = {'id': 'router-id',
                            'name': 'router',
                            'admin_state_up': False,
                            'routes': [{'destination': '1.1.1.0/24',
                                        'nexthop': '10.0.0.2'}]}
        self.fake_router_interface_info = {
            'port_id': 'router-port-id',
            'device_id': '',
            'mac_address': 'aa:aa:aa:aa:aa:aa',
            'subnet_id': 'subnet-id',
            'subnet_ids': ['subnet-id'],
            'fixed_ips': [{'ip_address': '10.0.0.100',
                           'subnet_id': 'subnet-id'}],
            'id': 'router-port-id'}
        self.fake_external_fixed_ips = {
            'network_id': 'ext-network-id',
            'external_fixed_ips': [{'ip_address': '192.168.1.1',
                                    'subnet_id': 'ext-subnet-id'}]}
        self.fake_router_with_ext_gw = {
            'id': 'router-id',
            'name': 'router',
            'admin_state_up': True,
            'external_gateway_info': self.fake_external_fixed_ips,
            'gw_port_id': 'gw-port-id'
        }
        self.fake_router_without_ext_gw = {
            'id': 'router-id',
            'name': 'router',
            'admin_state_up': True,
        }
        self.fake_ext_subnet = {'id': 'ext-subnet-id',
                                'ip_version': 4,
                                'cidr': '192.168.1.0/24',
                                'gateway_ip': '192.168.1.254'}
        self.fake_ext_gw_port = {'device_id': '',
                                 'device_owner': 'network:router_gateway',
                                 'fixed_ips': [{'ip_address': '192.168.1.1',
                                                'subnet_id': 'ext-subnet-id'}],
                                 'mac_address': '00:00:00:02:04:06',
                                 'network_id': self.fake_network['id'],
                                 'id': 'gw-port-id'}
        self.fake_ext_gw_port_assert = {
            'lrouter': 'neutron-router-id',
            'mac': '00:00:00:02:04:06',
            'name': 'lrp-gw-port-id',
            'networks': ['192.168.1.1/24'],
            'may_exist': True,
            'external_ids': {
                ovn_const.OVN_SUBNET_EXT_IDS_KEY: 'ext-subnet-id',
                ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'},
            'gateway_chassis': ['hv1']}
        self.fake_floating_ip_attrs = {'floating_ip_address': '192.168.0.10',
                                       'fixed_ip_address': '10.0.0.10'}
        self.fake_floating_ip = fakes.FakeFloatingIp.create_one_fip(
            attrs=self.fake_floating_ip_attrs)
        self.fake_floating_ip_new_attrs = {
            'router_id': 'new-router-id',
            'floating_ip_address': '192.168.0.10',
            'fixed_ip_address': '10.10.10.10',
            'port_id': 'new-port_id'}
        self.fake_floating_ip_new = fakes.FakeFloatingIp.create_one_fip(
            attrs=self.fake_floating_ip_new_attrs)
        self.fake_ovn_nat_rule = {
            'logical_ip': self.fake_floating_ip['fixed_ip_address'],
            'external_ip': self.fake_floating_ip['floating_ip_address'],
            'type': 'dnat_and_snat',
            'external_ids': {
                ovn_const.OVN_FIP_EXT_ID_KEY: self.fake_floating_ip['id'],
                ovn_const.OVN_FIP_PORT_EXT_ID_KEY:
                    self.fake_floating_ip['port_id'],
                ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY: utils.ovn_name(
                    self.fake_floating_ip['router_id'])}}
        self.l3_inst = directory.get_plugin(plugin_constants.L3)
        self._start_mock(
            'networking_ovn.l3.l3_ovn.OVNL3RouterPlugin._ovn',
            new_callable=mock.PropertyMock,
            return_value=fakes.FakeOvsdbNbOvnIdl())
        self._start_mock(
            'networking_ovn.l3.l3_ovn.OVNL3RouterPlugin._sb_ovn',
            new_callable=mock.PropertyMock,
            return_value=fakes.FakeOvsdbSbOvnIdl())
        self._start_mock(
            'neutron.plugins.ml2.plugin.Ml2Plugin.get_network',
            return_value=self.fake_network)
        self._start_mock(
            'neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port',
            return_value=self.fake_router_port)
        self._start_mock(
            'neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_subnet',
            return_value=self.fake_subnet)
        self._start_mock(
            'neutron.db.l3_db.L3_NAT_dbonly_mixin.get_router',
            return_value=self.fake_router)
        self._start_mock(
            'neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.update_router',
            return_value=self.fake_router)
        self._start_mock(
            'neutron.db.l3_db.L3_NAT_dbonly_mixin.remove_router_interface',
            return_value=self.fake_router_interface_info)
        self._start_mock(
            'neutron.db.l3_db.L3_NAT_dbonly_mixin.create_router',
            return_value=self.fake_router_with_ext_gw)
        self._start_mock(
            'neutron.db.l3_db.L3_NAT_dbonly_mixin.delete_router',
            return_value={})
        self._start_mock(
            'networking_ovn.common.ovn_client.'
            'OVNClient.get_candidates_for_scheduling',
            return_value=[])
        self._start_mock(
            'networking_ovn.l3.l3_ovn_scheduler.'
            'OVNGatewayLeastLoadedScheduler._schedule_gateway',
            return_value=['hv1'])
        # FIXME(lucasagomes): We shouldn't be mocking the creation of
        # floating IPs here, that makes the FIP to not be registered in
        # the standardattributes table and therefore we also need to mock
        # bump_revision.
        self._start_mock(
            'neutron.db.l3_db.L3_NAT_dbonly_mixin.create_floatingip',
            return_value=self.fake_floating_ip)
        self._start_mock(
            'networking_ovn.db.revision.bump_revision',
            return_value=None)
        self._start_mock(
            'neutron.db.l3_db.L3_NAT_dbonly_mixin._get_floatingip',
            return_value=self.fake_floating_ip)
        self._start_mock(
            'networking_ovn.common.ovn_client.'
            'OVNClient.update_floatingip_status',
            return_value=None)
        self.bump_rev_p = self._start_mock(
            'networking_ovn.db.revision.bump_revision', return_value=None)
        self.del_rev_p = self._start_mock(
            'networking_ovn.db.revision.delete_revision', return_value=None)

    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.add_router_interface')
    def test_add_router_interface(self, func):
        router_id = 'router-id'
        interface_info = {'port_id': 'router-port-id'}
        func.return_value = self.fake_router_interface_info
        self.l3_inst.add_router_interface(self.context, router_id,
                                          interface_info)
        self.l3_inst._ovn.add_lrouter_port.assert_called_once_with(
            **self.fake_router_port_assert)
        self.l3_inst._ovn.set_lrouter_port_in_lswitch_port.\
            assert_called_once_with(
                'router-port-id', 'lrp-router-port-id', is_gw_port=False,
                lsp_address=ovn_const.DEFAULT_ADDR_FOR_LSP_WITH_PEER)
        self.bump_rev_p.assert_called_once_with(self.fake_router_port,
                                                ovn_const.TYPE_ROUTER_PORTS)

    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.add_router_interface')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    def test_add_router_interface_update_lrouter_port(self, getp, func):
        router_id = 'router-id'
        interface_info = {'port_id': 'router-port-id'}
        func.return_value = {'id': router_id,
                             'port_id': 'router-port-id',
                             'subnet_id': 'subnet-id1',
                             'subnet_ids': ['subnet-id1'],
                             'fixed_ips': [
                                 {'ip_address': '2001:db8::1',
                                  'subnet_id': 'subnet-id1'},
                                 {'ip_address': '2001:dba::1',
                                  'subnet_id': 'subnet-id2'}],
                             'mac_address': 'aa:aa:aa:aa:aa:aa'
                             }
        getp.return_value = {
            'id': 'router-port-id',
            'fixed_ips': [
                {'ip_address': '2001:db8::1', 'subnet_id': 'subnet-id1'},
                {'ip_address': '2001:dba::1', 'subnet_id': 'subnet-id2'}],
            'mac_address': 'aa:aa:aa:aa:aa:aa'
            }
        fake_rtr_intf_networks = ['2001:db8::1/24', '2001:dba::1/24']
        self.l3_inst.add_router_interface(self.context, router_id,
                                          interface_info)
        called_args_dict = (
            self.l3_inst._ovn.update_lrouter_port.call_args_list[0][1])

        self.assertEqual(1, self.l3_inst._ovn.update_lrouter_port.call_count)
        self.assertItemsEqual(fake_rtr_intf_networks,
                              called_args_dict.get('networks', []))
        self.l3_inst._ovn.set_lrouter_port_in_lswitch_port.\
            assert_called_once_with(
                'router-port-id', 'lrp-router-port-id', is_gw_port=False,
                lsp_address=ovn_const.DEFAULT_ADDR_FOR_LSP_WITH_PEER)

    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    def test_remove_router_interface(self, getp):
        router_id = 'router-id'
        interface_info = {'port_id': 'router-port-id'}
        getp.side_effect = n_exc.PortNotFound(port_id='router-port-id')

        self.l3_inst.remove_router_interface(
            self.context, router_id, interface_info)

        self.l3_inst._ovn.lrp_del.assert_called_once_with(
            'lrp-router-port-id', 'neutron-router-id', if_exists=True)
        self.del_rev_p.assert_called_once_with('router-port-id',
                                               ovn_const.TYPE_ROUTER_PORTS)

    def test_remove_router_interface_update_lrouter_port(self):
        router_id = 'router-id'
        interface_info = {'port_id': 'router-port-id'}
        self.l3_inst.remove_router_interface(
            self.context, router_id, interface_info)

        self.l3_inst._ovn.update_lrouter_port.assert_called_once_with(
            if_exists=False, name='lrp-router-port-id',
            ipv6_ra_configs={},
            networks=['10.0.0.100/24'],
            external_ids={
                ovn_const.OVN_SUBNET_EXT_IDS_KEY: 'subnet-id',
                ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'})

    @mock.patch('neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.'
                'update_router')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.get_router')
    @mock.patch('networking_ovn.common.ovn_client.OVNClient.'
                '_get_v4_network_of_all_router_ports')
    def test_update_router_admin_state_change(self, get_rps, get_r, func):
        router_id = 'router-id'
        get_r.return_value = self.fake_router
        new_router = self.fake_router.copy()
        updated_data = {'admin_state_up': True}
        new_router.update(updated_data)
        func.return_value = new_router
        self.l3_inst.update_router(self.context, router_id,
                                   {'router': updated_data})
        self.l3_inst._ovn.update_lrouter.assert_called_once_with(
            'neutron-router-id', enabled=True, external_ids={
                ovn_const.OVN_GW_PORT_EXT_ID_KEY: '',
                ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
                ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY: 'router'})

    @mock.patch('neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.'
                'update_router')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.get_router')
    @mock.patch('networking_ovn.common.ovn_client.OVNClient.'
                '_get_v4_network_of_all_router_ports')
    def test_update_router_name_change(self, get_rps, get_r, func):
        router_id = 'router-id'
        get_r.return_value = self.fake_router
        new_router = self.fake_router.copy()
        updated_data = {'name': 'test'}
        new_router.update(updated_data)
        func.return_value = new_router
        self.l3_inst.update_router(self.context, router_id,
                                   {'router': updated_data})
        self.l3_inst._ovn.update_lrouter.assert_called_once_with(
            'neutron-router-id', enabled=False,
            external_ids={ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY: 'test',
                          ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
                          ovn_const.OVN_GW_PORT_EXT_ID_KEY: ''})

    @mock.patch.object(utils, 'get_lrouter_non_gw_routes')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.update_router')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin._get_router')
    @mock.patch('networking_ovn.common.ovn_client.OVNClient.'
                '_get_v4_network_of_all_router_ports')
    def test_update_router_static_route_no_change(self, get_rps, get_r, func,
                                                  mock_routes):
        router_id = 'router-id'
        get_rps.return_value = [{'device_id': '',
                                'device_owner': 'network:router_interface',
                                 'mac_address': 'aa:aa:aa:aa:aa:aa',
                                 'fixed_ips': [{'ip_address': '10.0.0.100',
                                                'subnet_id': 'subnet-id'}],
                                 'id': 'router-port-id'}]
        mock_routes.return_value = self.fake_router['routes']
        update_data = {'router': {'routes': [{'destination': '1.1.1.0/24',
                                              'nexthop': '10.0.0.2'}]}}
        self.l3_inst.update_router(self.context, router_id, update_data)
        self.assertFalse(self.l3_inst._ovn.add_static_route.called)
        self.assertFalse(self.l3_inst._ovn.delete_static_route.called)

    @mock.patch.object(utils, 'get_lrouter_non_gw_routes')
    @mock.patch('neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.'
                'update_router')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.get_router')
    @mock.patch('networking_ovn.common.ovn_client.OVNClient.'
                '_get_v4_network_of_all_router_ports')
    def test_update_router_static_route_change(self, get_rps, get_r, func,
                                               mock_routes):
        router_id = 'router-id'
        get_rps.return_value = [{'device_id': '',
                                'device_owner': 'network:router_interface',
                                 'mac_address': 'aa:aa:aa:aa:aa:aa',
                                 'fixed_ips': [{'ip_address': '10.0.0.100',
                                                'subnet_id': 'subnet-id'}],
                                 'id': 'router-port-id'}]

        mock_routes.return_value = self.fake_router['routes']
        get_r.return_value = self.fake_router
        new_router = self.fake_router.copy()
        updated_data = {'routes': [{'destination': '2.2.2.0/24',
                                    'nexthop': '10.0.0.3'}]}
        new_router.update(updated_data)
        func.return_value = new_router
        self.l3_inst.update_router(self.context, router_id,
                                   {'router': updated_data})
        self.l3_inst._ovn.add_static_route.assert_called_once_with(
            'neutron-router-id',
            ip_prefix='2.2.2.0/24', nexthop='10.0.0.3')
        self.l3_inst._ovn.delete_static_route.assert_called_once_with(
            'neutron-router-id',
            ip_prefix='1.1.1.0/24', nexthop='10.0.0.2')

    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_subnet')
    @mock.patch('networking_ovn.common.ovn_client.OVNClient.'
                '_get_v4_network_of_all_router_ports')
    def test_create_router_with_ext_gw(self, get_rps, get_subnet, get_port):
        self.l3_inst._ovn.is_col_present.return_value = True
        router = {'router': {'name': 'router'}}
        get_subnet.return_value = self.fake_ext_subnet
        get_port.return_value = self.fake_ext_gw_port
        get_rps.return_value = self.fake_ext_subnet['cidr']

        self.l3_inst.create_router(self.context, router)

        external_ids = {ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY: 'router',
                        ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
                        ovn_const.OVN_GW_PORT_EXT_ID_KEY: 'gw-port-id'}
        self.l3_inst._ovn.create_lrouter.assert_called_once_with(
            'neutron-router-id', external_ids=external_ids,
            enabled=True, options={})
        self.l3_inst._ovn.add_lrouter_port.assert_called_once_with(
            **self.fake_ext_gw_port_assert)
        expected_calls = [
            mock.call('neutron-router-id', ip_prefix='0.0.0.0/0',
                      nexthop='192.168.1.254',
                      external_ids={
                          ovn_const.OVN_ROUTER_IS_EXT_GW: 'true',
                          ovn_const.OVN_SUBNET_EXT_ID_KEY: 'ext-subnet-id'})]
        self.l3_inst._ovn.set_lrouter_port_in_lswitch_port.\
            assert_called_once_with(
                'gw-port-id', 'lrp-gw-port-id', is_gw_port=True,
                lsp_address=ovn_const.DEFAULT_ADDR_FOR_LSP_WITH_PEER)
        self.l3_inst._ovn.add_static_route.assert_has_calls(expected_calls)

        bump_rev_calls = [mock.call(self.fake_ext_gw_port,
                                    ovn_const.TYPE_ROUTER_PORTS),
                          mock.call(self.fake_router_with_ext_gw,
                                    ovn_const.TYPE_ROUTERS),
                          ]

        self.assertEqual(len(bump_rev_calls), self.bump_rev_p.call_count)
        self.bump_rev_p.assert_has_calls(bump_rev_calls, any_order=False)

    @mock.patch('networking_ovn.common.ovn_client.OVNClient._get_router_ports')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.get_router')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_subnet')
    def test_delete_router_with_ext_gw(self, gs, gr, gprs):
        gr.return_value = self.fake_router_with_ext_gw
        gs.return_value = self.fake_ext_subnet

        self.l3_inst.delete_router(self.context, 'router-id')

        self.l3_inst._ovn.delete_lrouter.assert_called_once_with(
            'neutron-router-id')

    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_subnet')
    @mock.patch('networking_ovn.common.ovn_client.OVNClient._get_router_ports')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.get_router')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.add_router_interface')
    def test_add_router_interface_with_gateway_set(self, ari, gr, grps,
                                                   gs, gp):
        router_id = 'router-id'
        interface_info = {'port_id': 'router-port-id'}
        ari.return_value = self.fake_router_interface_info
        gr.return_value = self.fake_router_with_ext_gw
        gs.return_value = self.fake_subnet
        gp.return_value = self.fake_router_port

        self.l3_inst.add_router_interface(self.context, router_id,
                                          interface_info)

        self.l3_inst._ovn.add_lrouter_port.assert_called_once_with(
            **self.fake_router_port_assert)
        self.l3_inst._ovn.set_lrouter_port_in_lswitch_port.\
            assert_called_once_with(
                'router-port-id', 'lrp-router-port-id', is_gw_port=False,
                lsp_address=ovn_const.DEFAULT_ADDR_FOR_LSP_WITH_PEER)
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-router-id', logical_ip='10.0.0.0/24',
            external_ip='192.168.1.1', type='snat')

        self.bump_rev_p.assert_called_with(self.fake_router_port,
                                           ovn_const.TYPE_ROUTER_PORTS)

    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_subnet')
    @mock.patch('networking_ovn.common.ovn_client.OVNClient._get_router_ports')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.get_router')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.add_router_interface')
    def test_add_router_interface_with_gateway_set_and_snat_disabled(
            self, ari, gr, grps, gs, gp):
        router_id = 'router-id'
        interface_info = {'port_id': 'router-port-id'}
        ari.return_value = self.fake_router_interface_info
        gr.return_value = self.fake_router_with_ext_gw
        gr.return_value['external_gateway_info']['enable_snat'] = False
        gs.return_value = self.fake_subnet
        gp.return_value = self.fake_router_port

        self.l3_inst.add_router_interface(self.context, router_id,
                                          interface_info)

        self.l3_inst._ovn.add_lrouter_port.assert_called_once_with(
            **self.fake_router_port_assert)
        self.l3_inst._ovn.set_lrouter_port_in_lswitch_port.\
            assert_called_once_with(
                'router-port-id', 'lrp-router-port-id', is_gw_port=False,
                lsp_address=ovn_const.DEFAULT_ADDR_FOR_LSP_WITH_PEER)
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_not_called()

    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_subnet')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.get_router')
    def test_remove_router_interface_with_gateway_set(self, gr, gs, gp):
        router_id = 'router-id'
        interface_info = {'port_id': 'router-port-id',
                          'subnet_id': 'subnet-id'}
        gr.return_value = self.fake_router_with_ext_gw
        gs.return_value = self.fake_subnet
        gp.side_effect = n_exc.PortNotFound(port_id='router-port-id')
        self.l3_inst.remove_router_interface(
            self.context, router_id, interface_info)

        self.l3_inst._ovn.lrp_del.assert_called_once_with(
            'lrp-router-port-id', 'neutron-router-id', if_exists=True)
        self.l3_inst._ovn.delete_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-router-id', logical_ip='10.0.0.0/24',
            external_ip='192.168.1.1', type='snat')

        self.del_rev_p.assert_called_with('router-port-id',
                                          ovn_const.TYPE_ROUTER_PORTS)

    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    @mock.patch('networking_ovn.common.ovn_client.OVNClient._get_router_ports')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_subnet')
    @mock.patch('neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.'
                'update_router')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.get_router')
    def test_update_router_with_ext_gw(self, gr, ur, gs, grps, gp):
        self.l3_inst._ovn.is_col_present.return_value = True
        router = {'router': {'name': 'router'}}
        gr.return_value = self.fake_router_without_ext_gw
        ur.return_value = self.fake_router_with_ext_gw
        gs.side_effect = lambda ctx, sid: {
            'ext-subnet-id': self.fake_ext_subnet}.get(sid, self.fake_subnet)
        gp.return_value = self.fake_ext_gw_port
        grps.return_value = self.fake_router_ports

        self.l3_inst.update_router(self.context, 'router-id', router)

        self.l3_inst._ovn.add_lrouter_port.assert_called_once_with(
            **self.fake_ext_gw_port_assert)
        self.l3_inst._ovn.set_lrouter_port_in_lswitch_port.\
            assert_called_once_with(
                'gw-port-id', 'lrp-gw-port-id', is_gw_port=True,
                lsp_address=ovn_const.DEFAULT_ADDR_FOR_LSP_WITH_PEER)
        self.l3_inst._ovn.add_static_route.assert_called_once_with(
            'neutron-router-id', ip_prefix='0.0.0.0/0',
            external_ids={ovn_const.OVN_ROUTER_IS_EXT_GW: 'true',
                          ovn_const.OVN_SUBNET_EXT_ID_KEY: 'ext-subnet-id'},
            nexthop='192.168.1.254')
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-router-id', type='snat',
            logical_ip='10.0.0.0/24', external_ip='192.168.1.1')
        self.bump_rev_p.assert_called_with(self.fake_ext_gw_port,
                                           ovn_const.TYPE_ROUTER_PORTS)

    @mock.patch.object(utils, 'get_lrouter_ext_gw_static_route')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    @mock.patch('networking_ovn.common.ovn_client.OVNClient._get_router_ports')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_subnet')
    @mock.patch('neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.'
                'update_router')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.get_router')
    def test_update_router_ext_gw_change_subnet(self, gr, ur, gs,
                                                grps, gp, mock_get_gw):
        self.l3_inst._ovn.is_col_present.return_value = True
        mock_get_gw.return_value = mock.sentinel.GwRoute
        router = {'router': {'name': 'router'}}
        fake_old_ext_subnet = {'id': 'old-ext-subnet-id',
                               'ip_version': 4,
                               'cidr': '192.168.2.0/24',
                               'gateway_ip': '192.168.2.254'}
        # Old gateway info with same network and different subnet
        gr.return_value = copy.copy(self.fake_router_with_ext_gw)
        gr.return_value['external_gateway_info'] = {
            'network_id': 'ext-network-id',
            'external_fixed_ips': [{'ip_address': '192.168.2.1',
                                    'subnet_id': 'old-ext-subnet-id'}]}
        gr.return_value['gw_port_id'] = 'old-gw-port-id'
        ur.return_value = self.fake_router_with_ext_gw
        gs.side_effect = lambda ctx, sid: {
            'ext-subnet-id': self.fake_ext_subnet,
            'old-ext-subnet-id': fake_old_ext_subnet}.get(sid,
                                                          self.fake_subnet)
        gp.return_value = self.fake_ext_gw_port
        grps.return_value = self.fake_router_ports

        self.l3_inst.update_router(self.context, 'router-id', router)

        # Check deleting old router gateway
        self.l3_inst._ovn.delete_lrouter_ext_gw.assert_called_once_with(
            'neutron-router-id')

        # Check adding new router gateway
        self.l3_inst._ovn.add_lrouter_port.assert_called_once_with(
            **self.fake_ext_gw_port_assert)
        self.l3_inst._ovn.set_lrouter_port_in_lswitch_port.\
            assert_called_once_with(
                'gw-port-id', 'lrp-gw-port-id', is_gw_port=True,
                lsp_address=ovn_const.DEFAULT_ADDR_FOR_LSP_WITH_PEER)
        self.l3_inst._ovn.add_static_route.assert_called_once_with(
            'neutron-router-id', ip_prefix='0.0.0.0/0',
            nexthop='192.168.1.254',
            external_ids={ovn_const.OVN_ROUTER_IS_EXT_GW: 'true',
                          ovn_const.OVN_SUBNET_EXT_ID_KEY: 'ext-subnet-id'})
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-router-id', type='snat', logical_ip='10.0.0.0/24',
            external_ip='192.168.1.1')

        self.bump_rev_p.assert_called_with(self.fake_ext_gw_port,
                                           ovn_const.TYPE_ROUTER_PORTS)
        self.del_rev_p.assert_called_once_with('old-gw-port-id',
                                               ovn_const.TYPE_ROUTER_PORTS)

    @mock.patch.object(utils, 'get_lrouter_ext_gw_static_route')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    @mock.patch('networking_ovn.common.ovn_client.OVNClient._get_router_ports')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_subnet')
    @mock.patch('neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.'
                'update_router')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.get_router')
    def test_update_router_ext_gw_change_ip_address(self, gr, ur, gs,
                                                    grps, gp, mock_get_gw):
        self.l3_inst._ovn.is_col_present.return_value = True
        mock_get_gw.return_value = mock.sentinel.GwRoute
        router = {'router': {'name': 'router'}}
        # Old gateway info with same subnet and different ip address
        gr_value = copy.deepcopy(self.fake_router_with_ext_gw)
        gr_value['external_gateway_info'][
            'external_fixed_ips'][0]['ip_address'] = '192.168.1.2'
        gr_value['gw_port_id'] = 'old-gw-port-id'
        gr.return_value = gr_value
        ur.return_value = self.fake_router_with_ext_gw
        gs.side_effect = lambda ctx, sid: {
            'ext-subnet-id': self.fake_ext_subnet}.get(sid, self.fake_subnet)
        gp.return_value = self.fake_ext_gw_port
        grps.return_value = self.fake_router_ports

        self.l3_inst.update_router(self.context, 'router-id', router)

        # Check deleting old router gateway
        self.l3_inst._ovn.delete_lrouter_ext_gw.assert_called_once_with(
            'neutron-router-id')
        # Check adding new router gateway
        self.l3_inst._ovn.add_lrouter_port.assert_called_once_with(
            **self.fake_ext_gw_port_assert)
        self.l3_inst._ovn.set_lrouter_port_in_lswitch_port.\
            assert_called_once_with(
                'gw-port-id', 'lrp-gw-port-id', is_gw_port=True,
                lsp_address=ovn_const.DEFAULT_ADDR_FOR_LSP_WITH_PEER)
        self.l3_inst._ovn.add_static_route.assert_called_once_with(
            'neutron-router-id', ip_prefix='0.0.0.0/0',
            nexthop='192.168.1.254',
            external_ids={ovn_const.OVN_ROUTER_IS_EXT_GW: 'true',
                          ovn_const.OVN_SUBNET_EXT_ID_KEY: 'ext-subnet-id'})
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-router-id', type='snat', logical_ip='10.0.0.0/24',
            external_ip='192.168.1.1')

    @mock.patch('networking_ovn.common.ovn_client.OVNClient.'
                '_get_v4_network_of_all_router_ports')
    @mock.patch('neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.'
                'update_router')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.get_router')
    def test_update_router_ext_gw_no_change(self, gr, ur, get_rps):
        router = {'router': {'name': 'router'}}
        gr.return_value = self.fake_router_with_ext_gw
        ur.return_value = self.fake_router_with_ext_gw
        self.l3_inst._ovn.get_lrouter.return_value = (
            fakes.FakeOVNRouter.from_neutron_router(
                self.fake_router_with_ext_gw))

        self.l3_inst.update_router(self.context, 'router-id', router)

        self.l3_inst._ovn.lrp_del.assert_not_called()
        self.l3_inst._ovn.delete_static_route.assert_not_called()
        self.l3_inst._ovn.delete_nat_rule_in_lrouter.assert_not_called()
        self.l3_inst._ovn.add_lrouter_port.assert_not_called()
        self.l3_inst._ovn.set_lrouter_port_in_lswitch_port.assert_not_called()
        self.l3_inst._ovn.add_static_route.assert_not_called()
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_not_called()

    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    @mock.patch('networking_ovn.common.ovn_client.OVNClient.'
                '_get_v4_network_of_all_router_ports')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_subnet')
    @mock.patch('neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.'
                'update_router')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.get_router')
    def test_update_router_with_ext_gw_and_disabled_snat(self, gr, ur,
                                                         gs, grps, gp):
        self.l3_inst._ovn.is_col_present.return_value = True
        router = {'router': {'name': 'router'}}
        gr.return_value = self.fake_router_without_ext_gw
        ur.return_value = self.fake_router_with_ext_gw
        ur.return_value['external_gateway_info']['enable_snat'] = False
        gs.side_effect = lambda ctx, sid: {
            'ext-subnet-id': self.fake_ext_subnet}.get(sid, self.fake_subnet)
        gp.return_value = self.fake_ext_gw_port
        grps.return_value = self.fake_router_ports

        self.l3_inst.update_router(self.context, 'router-id', router)

        # Need not check lsp and lrp here, it has been tested in other cases
        self.l3_inst._ovn.add_static_route.assert_called_once_with(
            'neutron-router-id', ip_prefix='0.0.0.0/0',
            external_ids={ovn_const.OVN_ROUTER_IS_EXT_GW: 'true',
                          ovn_const.OVN_SUBNET_EXT_ID_KEY: 'ext-subnet-id'},
            nexthop='192.168.1.254')
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_not_called()

    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    @mock.patch('networking_ovn.common.ovn_client.OVNClient._get_router_ports')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_subnet')
    @mock.patch('neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.'
                'update_router')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.get_router')
    def test_enable_snat(self, gr, ur, gs, grps, gp):
        router = {'router': {'name': 'router'}}
        gr.return_value = copy.deepcopy(self.fake_router_with_ext_gw)
        gr.return_value['external_gateway_info']['enable_snat'] = False
        ur.return_value = self.fake_router_with_ext_gw
        self.l3_inst._ovn.get_lrouter.return_value = (
            fakes.FakeOVNRouter.from_neutron_router(
                self.fake_router_with_ext_gw))
        gs.side_effect = lambda ctx, sid: {
            'ext-subnet-id': self.fake_ext_subnet}.get(sid, self.fake_subnet)
        gp.return_value = self.fake_ext_gw_port
        grps.return_value = self.fake_router_ports

        self.l3_inst.update_router(self.context, 'router-id', router)

        self.l3_inst._ovn.delete_static_route.assert_not_called()
        self.l3_inst._ovn.delete_nat_rule_in_lrouter.assert_not_called()
        self.l3_inst._ovn.add_static_route.assert_not_called()
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-router-id', type='snat', logical_ip='10.0.0.0/24',
            external_ip='192.168.1.1')

    @mock.patch('networking_ovn.common.ovn_client.OVNClient.'
                '_check_external_ips_changed')
    @mock.patch.object(utils, 'get_lrouter_snats')
    @mock.patch.object(utils, 'get_lrouter_ext_gw_static_route')
    @mock.patch('networking_ovn.common.utils.is_snat_enabled',
                mock.Mock(return_value=True))
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    @mock.patch('networking_ovn.common.ovn_client.OVNClient.'
                '_get_router_ports')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_subnet')
    @mock.patch('neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.'
                'update_router')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.get_router')
    def test_disable_snat(self, gr, ur, gs, grps, gp, mock_get_gw, mock_snats,
                          mock_ext_ips):
        mock_get_gw.return_value = mock.sentinel.GwRoute
        mock_snats.return_value = [mock.sentinel.NAT]
        mock_ext_ips.return_value = False
        router = {'router': {'name': 'router'}}
        gr.return_value = self.fake_router_with_ext_gw
        ur.return_value = copy.deepcopy(self.fake_router_with_ext_gw)
        ur.return_value['external_gateway_info']['enable_snat'] = False
        gs.side_effect = lambda ctx, sid: {
            'ext-subnet-id': self.fake_ext_subnet}.get(sid, self.fake_subnet)
        gp.return_value = self.fake_ext_gw_port
        grps.return_value = self.fake_router_ports

        self.l3_inst.update_router(self.context, 'router-id', router)

        self.l3_inst._ovn.delete_static_route.assert_not_called()
        self.l3_inst._ovn.delete_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-router-id', type='snat', logical_ip='10.0.0.0/24',
            external_ip='192.168.1.1')
        self.l3_inst._ovn.add_static_route.assert_not_called()
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_not_called()

    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin._get_floatingip')
    def test_create_floatingip(self, gf):
        self.l3_inst._ovn.is_col_present.return_value = True
        gf.return_value = {'floating_port_id': 'fip-port-id'}
        self.l3_inst.create_floatingip(self.context, 'floatingip')
        expected_ext_ids = {
            ovn_const.OVN_FIP_EXT_ID_KEY: self.fake_floating_ip['id'],
            ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
            ovn_const.OVN_FIP_PORT_EXT_ID_KEY:
                self.fake_floating_ip['port_id'],
            ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY: utils.ovn_name(
                self.fake_floating_ip['router_id'])}
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-router-id',
            type='dnat_and_snat',
            logical_ip='10.0.0.10',
            external_ip='192.168.0.10',
            external_ids=expected_ext_ids)
        self.l3_inst._ovn.delete_lswitch_port.assert_called_once_with(
            'fip-port-id', 'neutron-fip-net-id')

    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin._get_floatingip')
    def test_create_floatingip_distributed(self, gf, gp):
        self.l3_inst._ovn.is_col_present.return_value = True
        gp.return_value = {'mac_address': '00:01:02:03:04:05'}
        gf.return_value = {'floating_port_id': 'fip-port-id'}
        config.cfg.CONF.set_override(
            'enable_distributed_floating_ip', True, group='ovn')
        self.l3_inst.create_floatingip(self.context, 'floatingip')
        expected_ext_ids = {
            ovn_const.OVN_FIP_EXT_ID_KEY: self.fake_floating_ip['id'],
            ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
            ovn_const.OVN_FIP_PORT_EXT_ID_KEY:
                self.fake_floating_ip['port_id'],
            ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY: utils.ovn_name(
                self.fake_floating_ip['router_id']),
            ovn_const.OVN_FIP_EXT_MAC_KEY: '00:01:02:03:04:05'}
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-router-id', type='dnat_and_snat', logical_ip='10.0.0.10',
            external_ip='192.168.0.10', external_mac='00:01:02:03:04:05',
            logical_port='port_id',
            external_ids=expected_ext_ids)

    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin._get_floatingip')
    def test_create_floatingip_distributed_logical_port_down(self, gf, gp):
        # Check that when the port is down, the external_mac field is not
        # populated. This falls back to centralized routing for ports that
        # are not bound to a chassis.
        self.l3_inst._ovn.is_col_present.return_value = True
        self.l3_inst._ovn.lsp_get_up.return_value.execute.return_value = (
            False)
        gp.return_value = {'mac_address': '00:01:02:03:04:05'}
        gf.return_value = {'floating_port_id': 'fip-port-id'}
        config.cfg.CONF.set_override(
            'enable_distributed_floating_ip', True, group='ovn')
        self.l3_inst.create_floatingip(self.context, 'floatingip')
        expected_ext_ids = {
            ovn_const.OVN_FIP_EXT_ID_KEY: self.fake_floating_ip['id'],
            ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
            ovn_const.OVN_FIP_PORT_EXT_ID_KEY:
                self.fake_floating_ip['port_id'],
            ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY: utils.ovn_name(
                self.fake_floating_ip['router_id']),
            ovn_const.OVN_FIP_EXT_MAC_KEY: '00:01:02:03:04:05'}
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-router-id', type='dnat_and_snat', logical_ip='10.0.0.10',
            external_ip='192.168.0.10',
            logical_port='port_id',
            external_ids=expected_ext_ids)

    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin._get_floatingip')
    def test_create_floatingip_external_ip_present_in_nat_rule(self, gf):
        self.l3_inst._ovn.is_col_present.return_value = True
        gf.return_value = {'floating_port_id': 'fip-port-id'}
        self.l3_inst._ovn.get_lrouter_nat_rules.return_value = [
            {'external_ip': '192.168.0.10', 'logical_ip': '10.0.0.6',
             'type': 'dnat_and_snat', 'uuid': 'uuid1'}]
        self.l3_inst.create_floatingip(self.context, 'floatingip')
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_not_called()
        expected_ext_ids = {
            ovn_const.OVN_FIP_EXT_ID_KEY: self.fake_floating_ip['id'],
            ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
            ovn_const.OVN_FIP_PORT_EXT_ID_KEY:
                self.fake_floating_ip['port_id'],
            ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY: utils.ovn_name(
                self.fake_floating_ip['router_id'])}
        self.l3_inst._ovn.set_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-router-id', 'uuid1',
            type='dnat_and_snat',
            logical_ip='10.0.0.10',
            external_ip='192.168.0.10',
            external_ids=expected_ext_ids)
        self.l3_inst._ovn.delete_lswitch_port.assert_called_once_with(
            'fip-port-id', 'neutron-fip-net-id')

    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin._get_floatingip')
    def test_create_floatingip_external_ip_present_type_snat(self, gf):
        self.l3_inst._ovn.is_col_present.return_value = True
        gf.return_value = {'floating_port_id': 'fip-port-id'}
        self.l3_inst._ovn.get_lrouter_nat_rules.return_value = [
            {'external_ip': '192.168.0.10', 'logical_ip': '10.0.0.0/24',
             'type': 'snat', 'uuid': 'uuid1'}]
        self.l3_inst.create_floatingip(self.context, 'floatingip')
        self.l3_inst._ovn.set_nat_rule_in_lrouter.assert_not_called()
        expected_ext_ids = {
            ovn_const.OVN_FIP_EXT_ID_KEY: self.fake_floating_ip['id'],
            ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
            ovn_const.OVN_FIP_PORT_EXT_ID_KEY:
                self.fake_floating_ip['port_id'],
            ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY: utils.ovn_name(
                self.fake_floating_ip['router_id'])}
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-router-id',
            type='dnat_and_snat',
            logical_ip='10.0.0.10',
            external_ip='192.168.0.10',
            external_ids=expected_ext_ids)
        self.l3_inst._ovn.delete_lswitch_port.assert_called_once_with(
            'fip-port-id', 'neutron-fip-net-id')

    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.delete_floatingip')
    def test_delete_floatingip(self, df):
        self.l3_inst._ovn.get_floatingip.return_value = (
            self.fake_ovn_nat_rule)
        self.l3_inst.delete_floatingip(self.context, 'floatingip-id')
        self.l3_inst._ovn.delete_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-router-id',
            type='dnat_and_snat',
            logical_ip='10.0.0.10',
            external_ip='192.168.0.10')

    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin._get_floatingip')
    @mock.patch('neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.'
                'update_floatingip')
    def test_update_floatingip(self, uf, gf):
        self.l3_inst._ovn.is_col_present.return_value = True
        gf.return_value = self.fake_floating_ip
        uf.return_value = self.fake_floating_ip_new
        self.l3_inst._ovn.get_floatingip.return_value = (
            self.fake_ovn_nat_rule)
        self.l3_inst.update_floatingip(self.context, 'id', 'floatingip')
        self.l3_inst._ovn.delete_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-router-id',
            type='dnat_and_snat',
            logical_ip='10.0.0.10',
            external_ip='192.168.0.10')
        expected_ext_ids = {
            ovn_const.OVN_FIP_EXT_ID_KEY: self.fake_floating_ip_new['id'],
            ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
            ovn_const.OVN_FIP_PORT_EXT_ID_KEY:
                self.fake_floating_ip_new['port_id'],
            ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY: utils.ovn_name(
                self.fake_floating_ip_new['router_id'])}
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-new-router-id',
            type='dnat_and_snat',
            logical_ip='10.10.10.10',
            external_ip='192.168.0.10',
            external_ids=expected_ext_ids)

    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin._get_floatingip')
    @mock.patch('neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.'
                'update_floatingip')
    def test_update_floatingip_associate(self, uf, gf):
        self.l3_inst._ovn.is_col_present.return_value = True
        self.fake_floating_ip.update({'fixed_port_id': None})
        gf.return_value = self.fake_floating_ip
        uf.return_value = self.fake_floating_ip_new
        self.l3_inst.update_floatingip(self.context, 'id', 'floatingip')
        self.l3_inst._ovn.delete_nat_rule_in_lrouter.assert_not_called()
        expected_ext_ids = {
            ovn_const.OVN_FIP_EXT_ID_KEY: self.fake_floating_ip_new['id'],
            ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
            ovn_const.OVN_FIP_PORT_EXT_ID_KEY:
                self.fake_floating_ip_new['port_id'],
            ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY: utils.ovn_name(
                self.fake_floating_ip_new['router_id'])}
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-new-router-id',
            type='dnat_and_snat',
            logical_ip='10.10.10.10',
            external_ip='192.168.0.10',
            external_ids=expected_ext_ids)

    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_port')
    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin._get_floatingip')
    @mock.patch('neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.'
                'update_floatingip')
    def test_update_floatingip_associate_distributed(self, uf, gf, gp):
        self.l3_inst._ovn.is_col_present.return_value = True
        self.fake_floating_ip.update({'fixed_port_id': None})
        gp.return_value = {'mac_address': '00:01:02:03:04:05'}
        gf.return_value = self.fake_floating_ip
        uf.return_value = self.fake_floating_ip_new
        config.cfg.CONF.set_override(
            'enable_distributed_floating_ip', True, group='ovn')
        self.l3_inst.update_floatingip(self.context, 'id', 'floatingip')
        self.l3_inst._ovn.delete_nat_rule_in_lrouter.assert_not_called()
        expected_ext_ids = {
            ovn_const.OVN_FIP_EXT_ID_KEY: self.fake_floating_ip_new['id'],
            ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
            ovn_const.OVN_FIP_PORT_EXT_ID_KEY:
                self.fake_floating_ip_new['port_id'],
            ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY: utils.ovn_name(
                self.fake_floating_ip_new['router_id']),
            ovn_const.OVN_FIP_EXT_MAC_KEY: '00:01:02:03:04:05'}
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-new-router-id', type='dnat_and_snat',
            logical_ip='10.10.10.10', external_ip='192.168.0.10',
            external_mac='00:01:02:03:04:05', logical_port='new-port_id',
            external_ids=expected_ext_ids)

    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin._get_floatingip')
    @mock.patch('neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.'
                'update_floatingip')
    def test_update_floatingip_association_not_changed(self, uf, gf):
        self.fake_floating_ip.update({'fixed_port_id': None})
        self.fake_floating_ip_new.update({'port_id': None})
        gf.return_value = self.fake_floating_ip
        uf.return_value = self.fake_floating_ip_new
        self.l3_inst.update_floatingip(self.context, 'id', 'floatingip')
        self.l3_inst._ovn.delete_nat_rule_in_lrouter.assert_not_called()
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_not_called()

    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin._get_floatingip')
    @mock.patch('neutron.db.extraroute_db.ExtraRoute_dbonly_mixin.'
                'update_floatingip')
    def test_update_floatingip_reassociate_to_same_port_diff_fixed_ip(
            self, uf, gf):
        self.l3_inst._ovn.is_col_present.return_value = True
        self.l3_inst._ovn.get_floatingip.return_value = (
            self.fake_ovn_nat_rule)
        self.fake_floating_ip_new.update({'port_id': 'port_id',
                                          'fixed_port_id': 'port_id'})
        gf.return_value = self.fake_floating_ip
        uf.return_value = self.fake_floating_ip_new
        self.l3_inst.update_floatingip(self.context, 'id', 'floatingip')

        self.l3_inst._ovn.delete_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-router-id',
            type='dnat_and_snat',
            logical_ip='10.0.0.10',
            external_ip='192.168.0.10')
        expected_ext_ids = {
            ovn_const.OVN_FIP_EXT_ID_KEY: self.fake_floating_ip_new['id'],
            ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
            ovn_const.OVN_FIP_PORT_EXT_ID_KEY:
                self.fake_floating_ip_new['port_id'],
            ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY: utils.ovn_name(
                self.fake_floating_ip_new['router_id'])}
        self.l3_inst._ovn.add_nat_rule_in_lrouter.assert_called_once_with(
            'neutron-new-router-id',
            type='dnat_and_snat',
            logical_ip='10.10.10.10',
            external_ip='192.168.0.10',
            external_ids=expected_ext_ids)

    @mock.patch('neutron.db.l3_db.L3_NAT_dbonly_mixin.get_floatingips')
    def test_disassociate_floatingips(self, gfs):
        gfs.return_value = [{'id': 'fip-id1',
                             'floating_ip_address': '192.168.0.10',
                             'router_id': 'router-id',
                             'port_id': 'port_id',
                             'floating_port_id': 'fip-port-id1',
                             'fixed_ip_address': '10.0.0.10'},
                            {'id': 'fip-id2',
                             'floating_ip_address': '192.167.0.10',
                             'router_id': 'router-id',
                             'port_id': 'port_id',
                             'floating_port_id': 'fip-port-id2',
                             'fixed_ip_address': '10.0.0.11'}]
        self.l3_inst.disassociate_floatingips(self.context, 'port_id',
                                              do_notify=False)

        delete_nat_calls = [mock.call('neutron-router-id',
                                      type='dnat_and_snat',
                                      logical_ip=fip['fixed_ip_address'],
                                      external_ip=fip['floating_ip_address'])
                            for fip in gfs.return_value]
        self.assertEqual(
            len(delete_nat_calls),
            self.l3_inst._ovn.delete_nat_rule_in_lrouter.call_count)
        self.l3_inst._ovn.delete_nat_rule_in_lrouter.assert_has_calls(
            delete_nat_calls, any_order=True)

    @mock.patch('networking_ovn.common.ovn_client.OVNClient'
                '.update_router_port')
    def test_port_update_postcommit(self, update_rp_mock):
        kwargs = {'port': {'device_owner': 'foo'}}
        self.l3_inst._port_update(resources.PORT, events.AFTER_UPDATE, None,
                                  **kwargs)
        update_rp_mock.assert_not_called()

        kwargs = {'port': {'device_owner': constants.DEVICE_OWNER_ROUTER_INTF}}
        self.l3_inst._port_update(resources.PORT, events.AFTER_UPDATE, None,
                                  **kwargs)

        update_rp_mock.assert_called_once_with(kwargs['port'], if_exists=True)

    @mock.patch('neutron.plugins.ml2.plugin.Ml2Plugin.update_port_status')
    @mock.patch('neutron.plugins.ml2.plugin.Ml2Plugin.update_port')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_ports')
    def test_update_router_gateway_port_bindings_active(
            self, mock_get_port, mock_updt_port, mock_updt_status):
        fake_host = 'fake-host'
        fake_router = 'fake-router'
        fake_port_id = 'fake-port-id'
        mock_get_port.return_value = [{
            'id': fake_port_id,
            'status': constants.PORT_STATUS_DOWN}]
        self.l3_inst.update_router_gateway_port_bindings(
            fake_router, fake_host)

        # Assert that the port is being bound
        expected_update = {'port': {portbindings.HOST_ID: fake_host}}
        mock_updt_port.assert_called_once_with(
            mock.ANY, fake_port_id, expected_update)

        # Assert that the port status is being set to ACTIVE
        mock_updt_status.assert_called_once_with(
            mock.ANY, fake_port_id, constants.PORT_STATUS_ACTIVE)

    @mock.patch('neutron.plugins.ml2.plugin.Ml2Plugin.update_port_status')
    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_ports')
    def test_update_router_gateway_port_bindings_down(
            self, mock_get_port, mock_updt_status):
        fake_port_id = 'fake-port-id'
        mock_get_port.return_value = [{
            'id': fake_port_id,
            'status': constants.PORT_STATUS_ACTIVE}]
        self.l3_inst.update_router_gateway_port_bindings(None, None)

        # Assert that the port status is being set to DOWN
        mock_updt_status.assert_called_once_with(
            mock.ANY, fake_port_id, constants.PORT_STATUS_DOWN)


class OVNL3ExtrarouteTests(test_l3_gw.ExtGwModeIntTestCase,
                           test_l3.L3NatDBIntTestCase,
                           test_extraroute.ExtraRouteDBTestCaseBase):

    # TODO(lucasagomes): Ideally, this method should be moved to a base
    # class which all tests classes in networking-ovn inherits from but,
    # this base class doesn't seem to exist for now so we need to duplicate
    # it here
    def _start_mock(self, path, return_value, new_callable=None):
        patcher = mock.patch(path, return_value=return_value,
                             new_callable=new_callable)
        patcher.start()
        self.addCleanup(patcher.stop)

    def setUp(self):
        plugin = 'neutron.tests.unit.extensions.test_l3.TestNoL3NatPlugin'
        l3_plugin = ('networking_ovn.l3.l3_ovn.OVNL3RouterPlugin')
        service_plugins = {'l3_plugin_name': l3_plugin}
        # For these tests we need to enable overlapping ips
        cfg.CONF.set_default('allow_overlapping_ips', True)
        cfg.CONF.set_default('max_routes', 3)
        ext_mgr = test_extraroute.ExtraRouteTestExtensionManager()
        super(test_l3.L3BaseForIntTests, self).setUp(
            plugin=plugin, ext_mgr=ext_mgr,
            service_plugins=service_plugins)
        revision_plugin.RevisionPlugin()
        l3_gw_mgr = test_l3_gw.TestExtensionManager()
        test_extensions.setup_extensions_middleware(l3_gw_mgr)
        self.l3_inst = directory.get_plugin(plugin_constants.L3)
        self._start_mock(
            'networking_ovn.l3.l3_ovn.OVNL3RouterPlugin._ovn',
            new_callable=mock.PropertyMock,
            return_value=fakes.FakeOvsdbNbOvnIdl())
        self._start_mock(
            'networking_ovn.l3.l3_ovn.OVNL3RouterPlugin._sb_ovn',
            new_callable=mock.PropertyMock,
            return_value=fakes.FakeOvsdbSbOvnIdl())
        self._start_mock(
            'networking_ovn.l3.l3_ovn_scheduler.'
            'OVNGatewayScheduler._schedule_gateway',
            return_value='hv1')
        self._start_mock(
            'networking_ovn.common.ovn_client.'
            'OVNClient.get_candidates_for_scheduling',
            return_value=[])
        self._start_mock(
            'networking_ovn.common.ovn_client.OVNClient.'
            '_get_v4_network_of_all_router_ports',
            return_value=[])
        self._start_mock(
            'networking_ovn.common.ovn_client.'
            'OVNClient.update_floatingip_status',
            return_value=None)
        self._start_mock(
            'networking_ovn.common.utils.get_revision_number',
            return_value=1)
        self.setup_notification_driver()

    # Note(dongj): According to bug #1657693, status of an unassociated
    # floating IP is set to DOWN. Revise expected_status to DOWN for related
    # test cases.
    def test_floatingip_update(
            self, expected_status=constants.FLOATINGIP_STATUS_DOWN):
        super(OVNL3ExtrarouteTests, self).test_floatingip_update(
            expected_status)

    def test_floatingip_update_to_same_port_id_twice(
            self, expected_status=constants.FLOATINGIP_STATUS_DOWN):
        super(OVNL3ExtrarouteTests, self).\
            test_floatingip_update_to_same_port_id_twice(expected_status)

    def test_floatingip_update_subnet_gateway_disabled(
            self, expected_status=constants.FLOATINGIP_STATUS_DOWN):
        super(OVNL3ExtrarouteTests, self).\
            test_floatingip_update_subnet_gateway_disabled(expected_status)

    # Test function _subnet_update of L3 OVN plugin.
    def test_update_subnet_gateway_for_external_net(self):
        super(OVNL3ExtrarouteTests, self). \
            test_update_subnet_gateway_for_external_net()
        self.l3_inst._ovn.add_static_route.assert_called_once_with(
            'neutron-fake_device', ip_prefix='0.0.0.0/0', nexthop='120.0.0.2')
        self.l3_inst._ovn.delete_static_route.assert_called_once_with(
            'neutron-fake_device', ip_prefix='0.0.0.0/0', nexthop='120.0.0.1')




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\l3\test_l3_ovn_scheduler.py
===========File Type===========
.py
===========File Content===========
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import random

import mock
from neutron.tests import base

from networking_ovn.common import constants as ovn_const
from networking_ovn.l3 import l3_ovn_scheduler


class FakeOVNGatewaySchedulerNbOvnIdl(object):
    def __init__(self, chassis_gateway_mapping, gateway):
        self.get_all_chassis_gateway_bindings = mock.Mock(
            return_value=chassis_gateway_mapping['Chassis_Bindings'])
        self.get_gateway_chassis_binding = mock.Mock(
            return_value=chassis_gateway_mapping['Gateways'].get(gateway,
                                                                 None))


class FakeOVNGatewaySchedulerSbOvnIdl(object):
    def __init__(self, chassis_gateway_mapping):
        self.get_all_chassis = mock.Mock(
            return_value=chassis_gateway_mapping['Chassis'])


class TestOVNGatewayScheduler(base.BaseTestCase):

    def setUp(self):
        super(TestOVNGatewayScheduler, self).setUp()

        # Overwritten by derived classes
        self.l3_scheduler = None

        # Used for unit tests
        self.new_gateway_name = 'lrp_new'
        self.fake_chassis_gateway_mappings = {
            'None': {'Chassis': [],
                     'Gateways': {
                         'g1': [ovn_const.OVN_GATEWAY_INVALID_CHASSIS]}},
            'Multiple1': {'Chassis': ['hv1', 'hv2'],
                          'Gateways': {'g1': ['hv1'],
                                       'g2': ['hv2'],
                                       'g3': ['hv1']}},
            'Multiple2': {'Chassis': ['hv1', 'hv2', 'hv3'],
                          'Gateways': {'g1': ['hv1'],
                                       'g2': ['hv1'],
                                       'g3': ['hv1']}},
            'Multiple3': {'Chassis': ['hv1', 'hv2', 'hv3'],
                          'Gateways': {'g1': ['hv3'],
                                       'g2': ['hv2'],
                                       'g3': ['hv2']}}
            }

        # Determine the chassis to gateway list bindings
        for details in self.fake_chassis_gateway_mappings.values():
            self.assertNotIn(self.new_gateway_name, details['Gateways'])
            details.setdefault('Chassis_Bindings', {})
            for chassis in details['Chassis']:
                details['Chassis_Bindings'].setdefault(chassis, [])
            for gw, chassis_list in details['Gateways'].items():
                for chassis in chassis_list:
                    if chassis in details['Chassis_Bindings']:
                        details['Chassis_Bindings'][chassis].append((gw, 0))

    def select(self, chassis_gateway_mapping, gateway_name):
        nb_idl = FakeOVNGatewaySchedulerNbOvnIdl(chassis_gateway_mapping,
                                                 gateway_name)
        sb_idl = FakeOVNGatewaySchedulerSbOvnIdl(chassis_gateway_mapping)
        return self.l3_scheduler.select(nb_idl, sb_idl, gateway_name)


class OVNGatewayChanceScheduler(TestOVNGatewayScheduler):

    def setUp(self):
        super(OVNGatewayChanceScheduler, self).setUp()
        self.l3_scheduler = l3_ovn_scheduler.OVNGatewayChanceScheduler()

    def test_no_chassis_available_for_existing_gateway(self):
        mapping = self.fake_chassis_gateway_mappings['None']
        gateway_name = random.choice(list(mapping['Gateways'].keys()))
        chassis = self.select(mapping, gateway_name)
        self.assertEqual([ovn_const.OVN_GATEWAY_INVALID_CHASSIS], chassis)

    def test_no_chassis_available_for_new_gateway(self):
        mapping = self.fake_chassis_gateway_mappings['None']
        gateway_name = self.new_gateway_name
        chassis = self.select(mapping, gateway_name)
        self.assertEqual([ovn_const.OVN_GATEWAY_INVALID_CHASSIS], chassis)

    def test_random_chassis_available_for_new_gateway(self):
        mapping = self.fake_chassis_gateway_mappings['Multiple1']
        gateway_name = self.new_gateway_name
        chassis = self.select(mapping, gateway_name)
        self.assertItemsEqual(chassis, mapping.get('Chassis'))

    def test_existing_chassis_available_for_existing_gateway(self):
        mapping = self.fake_chassis_gateway_mappings['Multiple1']
        gateway_name = random.choice(list(mapping['Gateways'].keys()))
        chassis = self.select(mapping, gateway_name)
        self.assertEqual(mapping['Gateways'][gateway_name], chassis)


class OVNGatewayLeastLoadedScheduler(TestOVNGatewayScheduler):

    def setUp(self):
        super(OVNGatewayLeastLoadedScheduler, self).setUp()
        self.l3_scheduler = l3_ovn_scheduler.OVNGatewayLeastLoadedScheduler()

    def test_no_chassis_available_for_existing_gateway(self):
        mapping = self.fake_chassis_gateway_mappings['None']
        gateway_name = random.choice(list(mapping['Gateways'].keys()))
        chassis = self.select(mapping, gateway_name)
        self.assertEqual([ovn_const.OVN_GATEWAY_INVALID_CHASSIS], chassis)

    def test_no_chassis_available_for_new_gateway(self):
        mapping = self.fake_chassis_gateway_mappings['None']
        gateway_name = self.new_gateway_name
        chassis = self.select(mapping, gateway_name)
        self.assertEqual([ovn_const.OVN_GATEWAY_INVALID_CHASSIS], chassis)

    def test_least_loaded_chassis_available_for_new_gateway1(self):
        mapping = self.fake_chassis_gateway_mappings['Multiple1']
        gateway_name = self.new_gateway_name
        chassis = self.select(mapping, gateway_name)
        self.assertItemsEqual(chassis, mapping.get('Chassis'))
        # least loaded will be the first one in the list,
        # networking-ovn will assign highest priority to this first element
        self.assertEqual(['hv2', 'hv1'], chassis)

    def test_least_loaded_chassis_available_for_new_gateway2(self):
        mapping = self.fake_chassis_gateway_mappings['Multiple2']
        gateway_name = self.new_gateway_name
        chassis = self.select(mapping, gateway_name)
        # hv1 will have least priority
        self.assertEqual(chassis[2], 'hv1')

    def test_least_loaded_chassis_available_for_new_gateway3(self):
        mapping = self.fake_chassis_gateway_mappings['Multiple3']
        gateway_name = self.new_gateway_name
        chassis = self.select(mapping, gateway_name)
        # least loaded chassis will be in the front of the list
        self.assertEqual(['hv1', 'hv3', 'hv2'], chassis)

    def test_existing_chassis_available_for_existing_gateway(self):
        mapping = self.fake_chassis_gateway_mappings['Multiple1']
        gateway_name = random.choice(list(mapping['Gateways'].keys()))
        chassis = self.select(mapping, gateway_name)
        self.assertEqual(mapping['Gateways'][gateway_name], chassis)

    def test__get_chassis_load_by_prios_several_ports(self):
        # Adding 5 ports of prio 1 and 5 ports of prio 2
        chassis_info = []
        for i in range(1, 6):
            chassis_info.append(('lrp', 1))
            chassis_info.append(('lrp', 2))
        actual = self.l3_scheduler._get_chassis_load_by_prios(chassis_info)
        expected = {1: 5, 2: 5}
        self.assertItemsEqual(expected.items(), actual)

    def test__get_chassis_load_by_prios_no_ports(self):
        self.assertFalse(self.l3_scheduler._get_chassis_load_by_prios([]))




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\l3\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\ml2\test_mech_driver.py
===========File Type===========
.py
===========File Content===========
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import datetime
import uuid

import mock
from webob import exc

from neutron.services.revisions import revision_plugin
from neutron_lib.api.definitions import portbindings
from neutron_lib.api.definitions import provider_net as pnet
from neutron_lib.callbacks import events
from neutron_lib.callbacks import registry
from neutron_lib.callbacks import resources
from neutron_lib import constants as const
from neutron_lib import context
from neutron_lib import exceptions as n_exc
from neutron_lib.plugins import directory
from neutron_lib.utils import net as n_net
from oslo_config import cfg
from oslo_db import exception as os_db_exc
from oslo_serialization import jsonutils
from oslo_utils import timeutils

from neutron.db import api  # noqa
from neutron.db import provisioning_blocks
from neutron.plugins.ml2.drivers import type_geneve  # noqa
from neutron.tests import tools
from neutron.tests.unit.extensions import test_segment
from neutron.tests.unit.plugins.ml2 import test_ext_portsecurity
from neutron.tests.unit.plugins.ml2 import test_plugin
from neutron.tests.unit.plugins.ml2 import test_security_group

from networking_ovn.agent import stats
from networking_ovn.common import acl as ovn_acl
from networking_ovn.common import config as ovn_config
from networking_ovn.common import constants as ovn_const
from networking_ovn.common import ovn_client
from networking_ovn.common import utils as ovn_utils
from networking_ovn.db import revision as db_rev
from networking_ovn.ml2 import mech_driver
from networking_ovn.tests.unit import fakes


class TestOVNMechanismDriver(test_plugin.Ml2PluginV2TestCase):

    _mechanism_drivers = ['logger', 'ovn']
    _extension_drivers = ['port_security', 'dns']

    def setUp(self):
        cfg.CONF.set_override('extension_drivers',
                              self._extension_drivers,
                              group='ml2')
        cfg.CONF.set_override('tenant_network_types',
                              ['geneve'],
                              group='ml2')
        cfg.CONF.set_override('vni_ranges',
                              ['1:65536'],
                              group='ml2_type_geneve')
        ovn_config.cfg.CONF.set_override('ovn_metadata_enabled',
                                         False,
                                         group='ovn')
        ovn_config.cfg.CONF.set_override('dns_servers', ['8.8.8.8'],
                                         group='ovn')
        super(TestOVNMechanismDriver, self).setUp()
        mm = directory.get_plugin().mechanism_manager
        self.mech_driver = mm.mech_drivers['ovn'].obj
        self.mech_driver._nb_ovn = fakes.FakeOvsdbNbOvnIdl()
        self.mech_driver._sb_ovn = fakes.FakeOvsdbSbOvnIdl()
        self.nb_ovn = self.mech_driver._nb_ovn
        self.sb_ovn = self.mech_driver._sb_ovn

        self.fake_subnet = fakes.FakeSubnet.create_one_subnet().info()

        self.fake_sg_rule = \
            fakes.FakeSecurityGroupRule.create_one_security_group_rule().info()
        self.fake_sg = fakes.FakeSecurityGroup.create_one_security_group(
            attrs={'security_group_rules': [self.fake_sg_rule]}
        ).info()

        self.sg_cache = {self.fake_sg['id']: self.fake_sg}
        self.subnet_cache = {self.fake_subnet['id']: self.fake_subnet}
        mock.patch(
            "networking_ovn.common.acl._acl_columns_name_severity_supported",
            return_value=True
        ).start()
        revision_plugin.RevisionPlugin()
        p = mock.patch.object(ovn_utils, 'get_revision_number', return_value=1)
        p.start()
        self.addCleanup(p.stop)
        p = mock.patch.object(db_rev, 'bump_revision')
        p.start()
        self.addCleanup(p.stop)

    @mock.patch.object(db_rev, 'bump_revision')
    def test__create_security_group(self, mock_bump):
        self.mech_driver._create_security_group(
            resources.SECURITY_GROUP, events.AFTER_CREATE, {},
            security_group=self.fake_sg)
        external_ids = {ovn_const.OVN_SG_EXT_ID_KEY: self.fake_sg['id']}
        ip4_name = ovn_utils.ovn_addrset_name(self.fake_sg['id'], 'ip4')
        ip6_name = ovn_utils.ovn_addrset_name(self.fake_sg['id'], 'ip6')
        create_address_set_calls = [mock.call(name=name,
                                              external_ids=external_ids)
                                    for name in [ip4_name, ip6_name]]

        self.nb_ovn.create_address_set.assert_has_calls(
            create_address_set_calls, any_order=True)
        mock_bump.assert_called_once_with(
            self.fake_sg, ovn_const.TYPE_SECURITY_GROUPS)

    def test__delete_security_group(self):
        self.mech_driver._delete_security_group(
            resources.SECURITY_GROUP, events.AFTER_CREATE, {},
            security_group_id=self.fake_sg['id'])
        ip4_name = ovn_utils.ovn_addrset_name(self.fake_sg['id'], 'ip4')
        ip6_name = ovn_utils.ovn_addrset_name(self.fake_sg['id'], 'ip6')
        delete_address_set_calls = [mock.call(name=name)
                                    for name in [ip4_name, ip6_name]]

        self.nb_ovn.delete_address_set.assert_has_calls(
            delete_address_set_calls, any_order=True)

    @mock.patch.object(db_rev, 'bump_revision')
    def test__process_sg_rule_notifications_sgr_create(self, mock_bump):
        with mock.patch(
            'networking_ovn.common.acl.update_acls_for_security_group'
        ) as ovn_acl_up:
            rule = {'security_group_id': 'sg_id'}
            self.mech_driver._process_sg_rule_notification(
                resources.SECURITY_GROUP_RULE, events.AFTER_CREATE, {},
                security_group_rule=rule)
            ovn_acl_up.assert_called_once_with(
                mock.ANY, mock.ANY, mock.ANY,
                'sg_id', rule, is_add_acl=True)
            mock_bump.assert_called_once_with(
                rule, ovn_const.TYPE_SECURITY_GROUP_RULES)

    @mock.patch.object(db_rev, 'delete_revision')
    def test_process_sg_rule_notifications_sgr_delete(self, mock_delrev):
        rule = {'id': 'sgr_id', 'security_group_id': 'sg_id'}
        with mock.patch(
            'networking_ovn.common.acl.update_acls_for_security_group'
        ) as ovn_acl_up:
            with mock.patch(
                'neutron.db.securitygroups_db.'
                'SecurityGroupDbMixin.get_security_group_rule',
                return_value=rule
            ):
                self.mech_driver._process_sg_rule_notification(
                    resources.SECURITY_GROUP_RULE, events.BEFORE_DELETE, {},
                    security_group_rule=rule)
                ovn_acl_up.assert_called_once_with(
                    mock.ANY, mock.ANY, mock.ANY,
                    'sg_id', rule, is_add_acl=False)
                mock_delrev.assert_called_once_with(
                    rule['id'], ovn_const.TYPE_SECURITY_GROUP_RULES)

    def test_add_acls_no_sec_group(self):
        fake_port_no_sg = fakes.FakePort.create_one_port().info()
        expected_acls = ovn_acl.drop_all_ip_traffic_for_port(fake_port_no_sg)
        acls = ovn_acl.add_acls(self.mech_driver._plugin,
                                mock.Mock(),
                                fake_port_no_sg,
                                {}, {}, self.mech_driver._nb_ovn)
        self.assertEqual(expected_acls, acls)

    def test_add_acls_no_sec_group_no_port_security(self):
        fake_port_no_sg_no_ps = fakes.FakePort.create_one_port(
            attrs={'port_security_enabled': False}).info()
        acls = ovn_acl.add_acls(self.mech_driver._plugin,
                                mock.Mock(),
                                fake_port_no_sg_no_ps,
                                {}, {}, self.mech_driver._nb_ovn)
        self.assertEqual([], acls)

    def _test_add_acls_with_sec_group_helper(self, native_dhcp=True):
        fake_port_sg = fakes.FakePort.create_one_port(
            attrs={'security_groups': [self.fake_sg['id']],
                   'fixed_ips': [{'subnet_id': self.fake_subnet['id'],
                                  'ip_address': '10.10.10.20'}]}
        ).info()

        expected_acls = []
        expected_acls += ovn_acl.drop_all_ip_traffic_for_port(
            fake_port_sg)
        expected_acls += ovn_acl.add_acl_dhcp(
            fake_port_sg, self.fake_subnet, native_dhcp)
        sg_rule_acl = ovn_acl.add_sg_rule_acl_for_port(
            fake_port_sg, self.fake_sg_rule,
            'outport == "' + fake_port_sg['id'] + '" ' +
            '&& ip4 && ip4.src == 0.0.0.0/0 ' +
            '&& tcp && tcp.dst == 22')
        expected_acls.append(sg_rule_acl)

        # Test with caches
        acls = ovn_acl.add_acls(self.mech_driver._plugin,
                                mock.Mock(),
                                fake_port_sg,
                                self.sg_cache,
                                self.subnet_cache,
                                self.mech_driver._nb_ovn)
        self.assertEqual(expected_acls, acls)

        # Test without caches
        with mock.patch('neutron.db.db_base_plugin_v2.'
                        'NeutronDbPluginV2.get_subnet',
                        return_value=self.fake_subnet), \
            mock.patch('neutron.db.securitygroups_db.'
                       'SecurityGroupDbMixin.get_security_group',
                       return_value=self.fake_sg):

            acls = ovn_acl.add_acls(self.mech_driver._plugin,
                                    mock.Mock(),
                                    fake_port_sg,
                                    {}, {}, self.mech_driver._nb_ovn)
            self.assertEqual(expected_acls, acls)

        # Test with security groups disabled
        with mock.patch('networking_ovn.common.acl.is_sg_enabled',
                        return_value=False):
            acls = ovn_acl.add_acls(self.mech_driver._plugin,
                                    mock.Mock(),
                                    fake_port_sg,
                                    self.sg_cache,
                                    self.subnet_cache,
                                    self.mech_driver._nb_ovn)
            self.assertEqual([], acls)

        # Test with multiple fixed IPs on the same subnet.
        fake_port_sg['fixed_ips'].append({'subnet_id': self.fake_subnet['id'],
                                          'ip_address': '10.10.10.21'})
        acls = ovn_acl.add_acls(self.mech_driver._plugin,
                                mock.Mock(),
                                fake_port_sg,
                                self.sg_cache,
                                self.subnet_cache,
                                self.mech_driver._nb_ovn)
        self.assertEqual(expected_acls, acls)

    def test_add_acls_with_sec_group_native_dhcp_enabled(self):
        self._test_add_acls_with_sec_group_helper()

    def test_port_invalid_binding_profile(self):
        invalid_binding_profiles = [
            {'tag': 0,
             'parent_name': 'fakename'},
            {'tag': 1024},
            {'tag': 1024, 'parent_name': 1024},
            {'parent_name': 'test'},
            {'tag': 'test'},
            {'vtep-physical-switch': 'psw1'},
            {'vtep-logical-switch': 'lsw1'},
            {'vtep-physical-switch': 'psw1', 'vtep-logical-switch': 1234},
            {'vtep-physical-switch': 1234, 'vtep-logical-switch': 'lsw1'},
            {'vtep-physical-switch': 'psw1', 'vtep-logical-switch': 'lsw1',
             'tag': 1024},
            {'vtep-physical-switch': 'psw1', 'vtep-logical-switch': 'lsw1',
             'parent_name': 'fakename'},
            {'vtep-physical-switch': 'psw1', 'vtep-logical-switch': 'lsw1',
             'tag': 1024, 'parent_name': 'fakename'},
        ]
        with self.network(set_context=True, tenant_id='test') as net1:
            with self.subnet(network=net1) as subnet1:
                # succeed without binding:profile
                with self.port(subnet=subnet1,
                               set_context=True, tenant_id='test'):
                    pass
                # fail with invalid binding profiles
                for invalid_profile in invalid_binding_profiles:
                    try:
                        kwargs = {ovn_const.OVN_PORT_BINDING_PROFILE:
                                  invalid_profile}
                        with self.port(
                                subnet=subnet1,
                                expected_res_status=403,
                                arg_list=(
                                ovn_const.OVN_PORT_BINDING_PROFILE,),
                                set_context=True, tenant_id='test',
                                **kwargs):
                            pass
                    except exc.HTTPClientError:
                        pass

    def test__validate_ignored_port_update_from_fip_port(self):
        p = {'id': 'id', 'device_owner': 'test'}
        ori_p = {'id': 'id', 'device_owner': const.DEVICE_OWNER_FLOATINGIP}
        self.assertRaises(mech_driver.OVNPortUpdateError,
                          self.mech_driver._validate_ignored_port,
                          p, ori_p)

    def test__validate_ignored_port_update_to_fip_port(self):
        p = {'id': 'id', 'device_owner': const.DEVICE_OWNER_FLOATINGIP}
        ori_p = {'id': 'port-id', 'device_owner': 'test'}
        self.assertRaises(mech_driver.OVNPortUpdateError,
                          self.mech_driver._validate_ignored_port,
                          p, ori_p)

    def test_create_and_update_ignored_fip_port(self):
        with self.network(set_context=True, tenant_id='test') as net1:
            with self.subnet(network=net1) as subnet1:
                with self.port(subnet=subnet1,
                               device_owner=const.DEVICE_OWNER_FLOATINGIP,
                               set_context=True, tenant_id='test') as port:
                    self.nb_ovn.create_lswitch_port.assert_not_called()
                    data = {'port': {'name': 'new'}}
                    req = self.new_update_request('ports', data,
                                                  port['port']['id'])
                    res = req.get_response(self.api)
                    self.assertEqual(exc.HTTPOk.code, res.status_int)
                    self.nb_ovn.set_lswitch_port.assert_not_called()

    def test_update_ignored_port_from_fip_device_owner(self):
        with self.network(set_context=True, tenant_id='test') as net1:
            with self.subnet(network=net1) as subnet1:
                with self.port(subnet=subnet1,
                               device_owner=const.DEVICE_OWNER_FLOATINGIP,
                               set_context=True, tenant_id='test') as port:
                    self.nb_ovn.create_lswitch_port.assert_not_called()
                    data = {'port': {'device_owner': 'test'}}
                    req = self.new_update_request('ports', data,
                                                  port['port']['id'])
                    res = req.get_response(self.api)
                    self.assertEqual(exc.HTTPBadRequest.code, res.status_int)
                    msg = jsonutils.loads(res.body)['NeutronError']['message']
                    expect_msg = ('Bad port request: Updating device_owner for'
                                  ' port %s owned by network:floatingip is'
                                  ' not supported.' % port['port']['id'])
                    self.assertEqual(msg, expect_msg)
                    self.nb_ovn.set_lswitch_port.assert_not_called()

    def test_update_ignored_port_to_fip_device_owner(self):
        with self.network(set_context=True, tenant_id='test') as net1:
            with self.subnet(network=net1) as subnet1:
                with self.port(subnet=subnet1,
                               device_owner='test',
                               set_context=True, tenant_id='test') as port:
                    self.assertEqual(
                        1, self.nb_ovn.create_lswitch_port.call_count)
                    data = {'port': {'device_owner':
                                     const.DEVICE_OWNER_FLOATINGIP}}
                    req = self.new_update_request('ports', data,
                                                  port['port']['id'])
                    res = req.get_response(self.api)
                    self.assertEqual(exc.HTTPBadRequest.code, res.status_int)
                    msg = jsonutils.loads(res.body)['NeutronError']['message']
                    expect_msg = ('Bad port request: Updating device_owner to'
                                  ' network:floatingip for port %s is'
                                  ' not supported.' % port['port']['id'])
                    self.assertEqual(msg, expect_msg)
                    self.nb_ovn.set_lswitch_port.assert_not_called()

    def test_create_port_security(self):
        kwargs = {'mac_address': '00:00:00:00:00:01',
                  'fixed_ips': [{'ip_address': '10.0.0.2'},
                                {'ip_address': '10.0.0.4'}]}
        with self.network(set_context=True, tenant_id='test') as net1:
            with self.subnet(network=net1) as subnet1:
                with self.port(subnet=subnet1,
                               arg_list=('mac_address', 'fixed_ips'),
                               set_context=True, tenant_id='test',
                               **kwargs) as port:
                    self.assertTrue(self.nb_ovn.create_lswitch_port.called)
                    called_args_dict = (
                        (self.nb_ovn.create_lswitch_port
                         ).call_args_list[0][1])
                    self.assertEqual(['00:00:00:00:00:01 10.0.0.2 10.0.0.4'],
                                     called_args_dict.get('port_security'))

                    data = {'port': {'mac_address': '00:00:00:00:00:02'}}
                    req = self.new_update_request(
                        'ports',
                        data, port['port']['id'])
                    req.get_response(self.api)
                    self.assertTrue(self.nb_ovn.set_lswitch_port.called)
                    called_args_dict = (
                        (self.nb_ovn.set_lswitch_port
                         ).call_args_list[0][1])
                    self.assertEqual(['00:00:00:00:00:02 10.0.0.2 10.0.0.4'],
                                     called_args_dict.get('port_security'))

    def test_create_port_with_disabled_security(self):
        kwargs = {'port_security_enabled': False}
        with self.network(set_context=True, tenant_id='test') as net1:
            with self.subnet(network=net1) as subnet1:
                with self.port(subnet=subnet1,
                               arg_list=('port_security_enabled',),
                               set_context=True, tenant_id='test',
                               **kwargs) as port:
                    self.assertTrue(self.nb_ovn.create_lswitch_port.called)
                    called_args_dict = (
                        (self.nb_ovn.create_lswitch_port
                         ).call_args_list[0][1])
                    self.assertEqual([],
                                     called_args_dict.get('port_security'))

                    data = {'port': {'mac_address': '00:00:00:00:00:01'}}
                    req = self.new_update_request(
                        'ports',
                        data, port['port']['id'])
                    req.get_response(self.api)
                    self.assertTrue(self.nb_ovn.set_lswitch_port.called)
                    called_args_dict = (
                        (self.nb_ovn.set_lswitch_port
                         ).call_args_list[0][1])
                    self.assertEqual([],
                                     called_args_dict.get('port_security'))

    def test_create_port_security_allowed_address_pairs(self):
        kwargs = {'allowed_address_pairs':
                  [{"ip_address": "1.1.1.1"},
                   {"ip_address": "2.2.2.2",
                    "mac_address": "22:22:22:22:22:22"}]}
        with self.network(set_context=True, tenant_id='test') as net1:
            with self.subnet(network=net1) as subnet1:
                with self.port(subnet=subnet1,
                               arg_list=('allowed_address_pairs',),
                               set_context=True, tenant_id='test',
                               **kwargs) as port:
                    port_ip = port['port'].get('fixed_ips')[0]['ip_address']
                    self.assertTrue(self.nb_ovn.create_lswitch_port.called)
                    called_args_dict = (
                        (self.nb_ovn.create_lswitch_port
                         ).call_args_list[0][1])
                    self.assertEqual(
                        tools.UnorderedList(
                            ["22:22:22:22:22:22 2.2.2.2",
                             port['port']['mac_address'] + ' ' + port_ip +
                             ' ' + '1.1.1.1']),
                        called_args_dict.get('port_security'))
                    self.assertEqual(
                        tools.UnorderedList(
                            ["22:22:22:22:22:22",
                             port['port']['mac_address'] + ' ' + port_ip]),
                        called_args_dict.get('addresses'))

                    old_mac = port['port']['mac_address']

                    # we are updating only the port mac address. So the
                    # mac address of the allowed address pair ip 1.1.1.1
                    # will have old mac address
                    data = {'port': {'mac_address': '00:00:00:00:00:01'}}
                    req = self.new_update_request(
                        'ports',
                        data, port['port']['id'])
                    req.get_response(self.api)
                    self.assertTrue(self.nb_ovn.set_lswitch_port.called)
                    called_args_dict = (
                        (self.nb_ovn.set_lswitch_port
                         ).call_args_list[0][1])
                    self.assertEqual(tools.UnorderedList(
                        ["22:22:22:22:22:22 2.2.2.2",
                         "00:00:00:00:00:01 " + port_ip,
                         old_mac + " 1.1.1.1"]),
                        called_args_dict.get('port_security'))
                    self.assertEqual(
                        tools.UnorderedList(
                            ["22:22:22:22:22:22",
                             "00:00:00:00:00:01 " + port_ip,
                             old_mac]),
                        called_args_dict.get('addresses'))

    def _create_fake_network_context(self,
                                     network_type,
                                     physical_network=None,
                                     segmentation_id=None):
        network_attrs = {'provider:network_type': network_type,
                         'provider:physical_network': physical_network,
                         'provider:segmentation_id': segmentation_id}
        segment_attrs = {'network_type': network_type,
                         'physical_network': physical_network,
                         'segmentation_id': segmentation_id}
        fake_network = \
            fakes.FakeNetwork.create_one_network(attrs=network_attrs).info()
        fake_segments = \
            [fakes.FakeSegment.create_one_segment(attrs=segment_attrs).info()]
        return fakes.FakeNetworkContext(fake_network, fake_segments)

    def _create_fake_mp_network_context(self):
        network_type = 'flat'
        network_attrs = {'segments': []}
        fake_segments = []
        for physical_network in ['physnet1', 'physnet2']:
            network_attrs['segments'].append(
                {'provider:network_type': network_type,
                 'provider:physical_network': physical_network})
            segment_attrs = {'network_type': network_type,
                             'physical_network': physical_network}
            fake_segments.append(
                fakes.FakeSegment.create_one_segment(
                    attrs=segment_attrs).info())
        fake_network = \
            fakes.FakeNetwork.create_one_network(attrs=network_attrs).info()
        fake_network.pop('provider:network_type')
        fake_network.pop('provider:physical_network')
        fake_network.pop('provider:segmentation_id')
        return fakes.FakeNetworkContext(fake_network, fake_segments)

    def test_network_precommit(self):
        # Test supported network types.
        fake_network_context = self._create_fake_network_context('local')
        self.mech_driver.create_network_precommit(fake_network_context)
        fake_network_context = self._create_fake_network_context(
            'flat', physical_network='physnet')
        self.mech_driver.update_network_precommit(fake_network_context)
        fake_network_context = self._create_fake_network_context(
            'geneve', segmentation_id=10)
        self.mech_driver.create_network_precommit(fake_network_context)
        fake_network_context = self._create_fake_network_context(
            'vlan', physical_network='physnet', segmentation_id=11)
        self.mech_driver.update_network_precommit(fake_network_context)
        fake_mp_network_context = self._create_fake_mp_network_context()
        self.mech_driver.create_network_precommit(fake_mp_network_context)

        # Test unsupported network types.
        fake_network_context = self._create_fake_network_context(
            'vxlan', segmentation_id=12)
        self.assertRaises(n_exc.InvalidInput,
                          self.mech_driver.create_network_precommit,
                          fake_network_context)
        fake_network_context = self._create_fake_network_context(
            'gre', segmentation_id=13)
        self.assertRaises(n_exc.InvalidInput,
                          self.mech_driver.update_network_precommit,
                          fake_network_context)

    def test_create_port_without_security_groups(self):
        kwargs = {'security_groups': []}
        with self.network(set_context=True, tenant_id='test') as net1:
            with self.subnet(network=net1) as subnet1:
                with self.port(subnet=subnet1,
                               arg_list=('security_groups',),
                               set_context=True, tenant_id='test',
                               **kwargs):
                    self.assertEqual(
                        1, self.nb_ovn.create_lswitch_port.call_count)
                    self.assertEqual(2, self.nb_ovn.add_acl.call_count)
                    self.nb_ovn.update_address_set.assert_not_called()

    def test_create_port_without_security_groups_no_ps(self):
        kwargs = {'security_groups': [], 'port_security_enabled': False}
        with self.network(set_context=True, tenant_id='test') as net1:
            with self.subnet(network=net1) as subnet1:
                with self.port(subnet=subnet1,
                               arg_list=('security_groups',
                                         'port_security_enabled'),
                               set_context=True, tenant_id='test',
                               **kwargs):
                    self.assertEqual(
                        1, self.nb_ovn.create_lswitch_port.call_count)
                    self.nb_ovn.add_acl.assert_not_called()
                    self.nb_ovn.update_address_set.assert_not_called()

    def _test_create_port_with_security_groups_helper(self,
                                                      add_acl_call_count):
        with self.network(set_context=True, tenant_id='test') as net1:
            with self.subnet(network=net1) as subnet1:
                with self.port(subnet=subnet1,
                               set_context=True, tenant_id='test'):
                    self.assertEqual(
                        1, self.nb_ovn.create_lswitch_port.call_count)
                    self.assertEqual(
                        add_acl_call_count, self.nb_ovn.add_acl.call_count)
                    self.assertEqual(
                        1, self.nb_ovn.update_address_set.call_count)

    def test_create_port_with_security_groups_native_dhcp_enabled(self):
        self._test_create_port_with_security_groups_helper(7)

    def test_update_port_changed_security_groups(self):
        with self.network(set_context=True, tenant_id='test') as net1:
            with self.subnet(network=net1) as subnet1:
                with self.port(subnet=subnet1,
                               set_context=True, tenant_id='test') as port1:
                    sg_id = port1['port']['security_groups'][0]
                    fake_lsp = (
                        fakes.FakeOVNPort.from_neutron_port(
                            port1['port']))
                    self.nb_ovn.lookup.return_value = fake_lsp

                    # Remove the default security group.
                    self.nb_ovn.set_lswitch_port.reset_mock()
                    self.nb_ovn.update_acls.reset_mock()
                    self.nb_ovn.update_address_set.reset_mock()
                    data = {'port': {'security_groups': []}}
                    self._update('ports', port1['port']['id'], data)
                    self.assertEqual(
                        1, self.nb_ovn.set_lswitch_port.call_count)
                    self.assertEqual(
                        1, self.nb_ovn.update_acls.call_count)
                    self.assertEqual(
                        1, self.nb_ovn.update_address_set.call_count)

                    # Add the default security group.
                    self.nb_ovn.set_lswitch_port.reset_mock()
                    self.nb_ovn.update_acls.reset_mock()
                    self.nb_ovn.update_address_set.reset_mock()
                    fake_lsp.external_ids.pop(ovn_const.OVN_SG_IDS_EXT_ID_KEY)
                    data = {'port': {'security_groups': [sg_id]}}
                    self._update('ports', port1['port']['id'], data)
                    self.assertEqual(
                        1, self.nb_ovn.set_lswitch_port.call_count)
                    self.assertEqual(
                        1, self.nb_ovn.update_acls.call_count)
                    self.assertEqual(
                        1, self.nb_ovn.update_address_set.call_count)

    def test_update_port_unchanged_security_groups(self):
        with self.network(set_context=True, tenant_id='test') as net1:
            with self.subnet(network=net1) as subnet1:
                with self.port(subnet=subnet1,
                               set_context=True, tenant_id='test') as port1:
                    fake_lsp = (
                        fakes.FakeOVNPort.from_neutron_port(
                            port1['port']))
                    self.nb_ovn.lookup.return_value = fake_lsp

                    # Update the port name.
                    self.nb_ovn.set_lswitch_port.reset_mock()
                    self.nb_ovn.update_acls.reset_mock()
                    self.nb_ovn.update_address_set.reset_mock()
                    data = {'port': {'name': 'rtheis'}}
                    self._update('ports', port1['port']['id'], data)
                    self.assertEqual(
                        1, self.nb_ovn.set_lswitch_port.call_count)
                    self.nb_ovn.update_acls.assert_not_called()
                    self.nb_ovn.update_address_set.assert_not_called()

                    # Update the port fixed IPs
                    self.nb_ovn.set_lswitch_port.reset_mock()
                    self.nb_ovn.update_acls.reset_mock()
                    self.nb_ovn.update_address_set.reset_mock()
                    data = {'port': {'fixed_ips': []}}
                    self._update('ports', port1['port']['id'], data)
                    self.assertEqual(
                        1, self.nb_ovn.set_lswitch_port.call_count)
                    self.assertEqual(
                        1, self.nb_ovn.update_acls.call_count)
                    self.assertEqual(
                        1, self.nb_ovn.update_address_set.call_count)

    def test_delete_port_without_security_groups(self):
        kwargs = {'security_groups': []}
        with self.network(set_context=True, tenant_id='test') as net1:
            with self.subnet(network=net1) as subnet1:
                with self.port(subnet=subnet1,
                               arg_list=('security_groups',),
                               set_context=True, tenant_id='test',
                               **kwargs) as port1:
                    fake_lsp = (
                        fakes.FakeOVNPort.from_neutron_port(
                            port1['port']))
                    self.nb_ovn.lookup.return_value = fake_lsp
                    self.nb_ovn.delete_lswitch_port.reset_mock()
                    self.nb_ovn.delete_acl.reset_mock()
                    self.nb_ovn.update_address_set.reset_mock()
                    self._delete('ports', port1['port']['id'])
                    self.assertEqual(
                        1, self.nb_ovn.delete_lswitch_port.call_count)
                    self.assertEqual(
                        1, self.nb_ovn.delete_acl.call_count)
                    self.nb_ovn.update_address_set.assert_not_called()

    def test_delete_port_with_security_groups(self):
        with self.network(set_context=True, tenant_id='test') as net1:
            with self.subnet(network=net1) as subnet1:
                with self.port(subnet=subnet1,
                               set_context=True, tenant_id='test') as port1:
                    fake_lsp = (
                        fakes.FakeOVNPort.from_neutron_port(
                            port1['port']))
                    self.nb_ovn.lookup.return_value = fake_lsp
                    self.nb_ovn.delete_lswitch_port.reset_mock()
                    self.nb_ovn.delete_acl.reset_mock()
                    self.nb_ovn.update_address_set.reset_mock()
                    self._delete('ports', port1['port']['id'])
                    self.assertEqual(
                        1, self.nb_ovn.delete_lswitch_port.call_count)
                    self.assertEqual(
                        1, self.nb_ovn.delete_acl.call_count)
                    self.assertEqual(
                        1, self.nb_ovn.update_address_set.call_count)

    def _test_set_port_status_up(self, is_compute_port=False):
        port_device_owner = 'compute:nova' if is_compute_port else ''
        self.mech_driver._plugin.nova_notifier = mock.Mock()
        with self.network(set_context=True, tenant_id='test') as net1, \
            self.subnet(network=net1) as subnet1, \
            self.port(subnet=subnet1, set_context=True,
                      tenant_id='test',
                      device_owner=port_device_owner) as port1, \
            mock.patch('neutron.db.provisioning_blocks.'
                       'provisioning_complete') as pc, \
            mock.patch.object(
                self.mech_driver,
                '_update_subport_host_if_needed') as upd_subport, \
            mock.patch.object(self.mech_driver,
                              '_update_dnat_entry_if_needed') as ude:
                self.mech_driver.set_port_status_up(port1['port']['id'])
                pc.assert_called_once_with(
                    mock.ANY,
                    port1['port']['id'],
                    resources.PORT,
                    provisioning_blocks.L2_AGENT_ENTITY
                )
                upd_subport.assert_called_once_with(port1['port']['id'])
                ude.assert_called_once_with(port1['port']['id'])

                # If the port does NOT bellong to compute, do not notify Nova
                # about it's status changes
                if not is_compute_port:
                    self.mech_driver._plugin.nova_notifier.\
                        notify_port_active_direct.assert_not_called()
                else:
                    self.mech_driver._plugin.nova_notifier.\
                        notify_port_active_direct.assert_called_once_with(
                            mock.ANY)

    def test_set_port_status_up(self):
        self._test_set_port_status_up(is_compute_port=False)

    def test_set_compute_port_status_up(self):
        self._test_set_port_status_up(is_compute_port=True)

    def _test_set_port_status_down(self, is_compute_port=False):
        port_device_owner = 'compute:nova' if is_compute_port else ''
        self.mech_driver._plugin.nova_notifier = mock.Mock()
        with self.network(set_context=True, tenant_id='test') as net1, \
            self.subnet(network=net1) as subnet1, \
            self.port(subnet=subnet1, set_context=True,
                      tenant_id='test',
                      device_owner=port_device_owner) as port1, \
            mock.patch('neutron.db.provisioning_blocks.'
                       'add_provisioning_component') as apc, \
            mock.patch.object(self.mech_driver,
                              '_update_dnat_entry_if_needed') as ude:
                self.mech_driver.set_port_status_down(port1['port']['id'])
                apc.assert_called_once_with(
                    mock.ANY,
                    port1['port']['id'],
                    resources.PORT,
                    provisioning_blocks.L2_AGENT_ENTITY
                )
                ude.assert_called_once_with(port1['port']['id'], False)

                # If the port does NOT bellong to compute, do not notify Nova
                # about it's status changes
                if not is_compute_port:
                    self.mech_driver._plugin.nova_notifier.\
                        record_port_status_changed.assert_not_called()
                    self.mech_driver._plugin.nova_notifier.\
                        send_port_status.assert_not_called()
                else:
                    self.mech_driver._plugin.nova_notifier.\
                        record_port_status_changed.assert_called_once_with(
                            mock.ANY, const.PORT_STATUS_ACTIVE,
                            const.PORT_STATUS_DOWN, None)
                    self.mech_driver._plugin.nova_notifier.\
                        send_port_status.assert_called_once_with(
                            None, None, mock.ANY)

    def test_set_port_status_down(self):
        self._test_set_port_status_down(is_compute_port=False)

    def test_set_compute_port_status_down(self):
        self._test_set_port_status_down(is_compute_port=True)

    def test_set_port_status_down_not_found(self):
        with mock.patch('neutron.db.provisioning_blocks.'
                        'add_provisioning_component') as apc, \
            mock.patch.object(self.mech_driver,
                              '_update_dnat_entry_if_needed'):
            self.mech_driver.set_port_status_down('foo')
            apc.assert_not_called()

    def test_set_port_status_concurrent_delete(self):
        exc = os_db_exc.DBReferenceError('', '', '', '')
        with self.network(set_context=True, tenant_id='test') as net1, \
            self.subnet(network=net1) as subnet1, \
            self.port(subnet=subnet1, set_context=True,
                      tenant_id='test') as port1, \
            mock.patch('neutron.db.provisioning_blocks.'
                       'add_provisioning_component',
                       side_effect=exc) as apc, \
            mock.patch.object(self.mech_driver,
                              '_update_dnat_entry_if_needed') as ude:
                self.mech_driver.set_port_status_down(port1['port']['id'])
                apc.assert_called_once_with(
                    mock.ANY,
                    port1['port']['id'],
                    resources.PORT,
                    provisioning_blocks.L2_AGENT_ENTITY
                )
                ude.assert_called_once_with(port1['port']['id'], False)

    def test__update_subport_host_if_needed(self):
        """Check that a subport is updated with parent's host_id."""
        binding_host_id = {'binding:host_id': 'hostname'}
        with mock.patch.object(self.mech_driver._ovn_client, 'get_parent_port',
                               return_value='parent'), \
            mock.patch.object(self.mech_driver._plugin, 'get_port',
                              return_value=binding_host_id) as get_port, \
            mock.patch.object(self.mech_driver._plugin, 'update_port') as upd:
                self.mech_driver._update_subport_host_if_needed('subport')

        get_port.assert_called_once_with(mock.ANY, 'parent')
        upd.assert_called_once_with(mock.ANY, 'subport',
                                    {'port': binding_host_id})

    def test_bind_port_unsupported_vnic_type(self):
        fake_port = fakes.FakePort.create_one_port(
            attrs={'binding:vnic_type': 'unknown'}).info()
        fake_port_context = fakes.FakePortContext(fake_port, 'host', [])
        self.mech_driver.bind_port(fake_port_context)
        self.sb_ovn.get_chassis_data_for_ml2_bind_port.assert_not_called()
        fake_port_context.set_binding.assert_not_called()

    def _test_bind_port_failed(self, fake_segments):
        fake_port = fakes.FakePort.create_one_port().info()
        fake_host = 'host'
        fake_port_context = fakes.FakePortContext(
            fake_port, fake_host, fake_segments)
        self.mech_driver.bind_port(fake_port_context)
        self.sb_ovn.get_chassis_data_for_ml2_bind_port.assert_called_once_with(
            fake_host)
        fake_port_context.set_binding.assert_not_called()

    def test_bind_port_host_not_found(self):
        self.sb_ovn.get_chassis_data_for_ml2_bind_port.side_effect = \
            RuntimeError
        self._test_bind_port_failed([])

    def test_bind_port_no_segments_to_bind(self):
        self._test_bind_port_failed([])

    def test_bind_port_physnet_not_found(self):
        segment_attrs = {'network_type': 'vlan',
                         'physical_network': 'unknown-physnet',
                         'segmentation_id': 23}
        fake_segments = \
            [fakes.FakeSegment.create_one_segment(attrs=segment_attrs).info()]
        self._test_bind_port_failed(fake_segments)

    def _test_bind_port(self, fake_segments):
        fake_port = fakes.FakePort.create_one_port().info()
        fake_host = 'host'
        fake_port_context = fakes.FakePortContext(
            fake_port, fake_host, fake_segments)
        self.mech_driver.bind_port(fake_port_context)
        self.sb_ovn.get_chassis_data_for_ml2_bind_port.assert_called_once_with(
            fake_host)
        fake_port_context.set_binding.assert_called_once_with(
            fake_segments[0]['id'],
            portbindings.VIF_TYPE_OVS,
            self.mech_driver.vif_details[portbindings.VIF_TYPE_OVS])

    def _test_bind_port_sriov(self, fake_segments):
        fake_port = fakes.FakePort.create_one_port(
            attrs={'binding:vnic_type': 'direct',
                   'binding:profile': {'capabilities': ['switchdev']}}).info()
        fake_host = 'host'
        fake_port_context = fakes.FakePortContext(
            fake_port, fake_host, fake_segments)
        self.mech_driver.bind_port(fake_port_context)
        self.sb_ovn.get_chassis_data_for_ml2_bind_port.assert_called_once_with(
            fake_host)
        fake_port_context.set_binding.assert_called_once_with(
            fake_segments[0]['id'],
            portbindings.VIF_TYPE_OVS,
            self.mech_driver.vif_details[portbindings.VIF_TYPE_OVS])

    def test_bind_port_geneve(self):
        segment_attrs = {'network_type': 'geneve',
                         'physical_network': None,
                         'segmentation_id': 1023}
        fake_segments = \
            [fakes.FakeSegment.create_one_segment(attrs=segment_attrs).info()]
        self._test_bind_port(fake_segments)

    def test_bind_sriov_port_geneve(self):
        """Test binding a SR-IOV port to a geneve segment."""
        segment_attrs = {'network_type': 'geneve',
                         'physical_network': None,
                         'segmentation_id': 1023}
        fake_segments = \
            [fakes.FakeSegment.create_one_segment(attrs=segment_attrs).info()]
        self._test_bind_port_sriov(fake_segments)

    def test_bind_port_vlan(self):
        segment_attrs = {'network_type': 'vlan',
                         'physical_network': 'fake-physnet',
                         'segmentation_id': 23}
        fake_segments = \
            [fakes.FakeSegment.create_one_segment(attrs=segment_attrs).info()]
        self._test_bind_port(fake_segments)

    def test_bind_port_flat(self):
        segment_attrs = {'network_type': 'flat',
                         'physical_network': 'fake-physnet',
                         'segmentation_id': None}
        fake_segments = \
            [fakes.FakeSegment.create_one_segment(attrs=segment_attrs).info()]
        self._test_bind_port(fake_segments)

    def test_bind_port_vxlan(self):
        segment_attrs = {'network_type': 'vxlan',
                         'physical_network': None,
                         'segmentation_id': 1024}
        fake_segments = \
            [fakes.FakeSegment.create_one_segment(attrs=segment_attrs).info()]
        self._test_bind_port(fake_segments)

    def test__is_port_provisioning_required(self):
        fake_port = fakes.FakePort.create_one_port(
            attrs={'binding:vnic_type': 'normal',
                   'status': const.PORT_STATUS_DOWN}).info()
        fake_host = 'fake-physnet'

        # Test host not changed
        self.assertFalse(self.mech_driver._is_port_provisioning_required(
            fake_port, fake_host, fake_host))

        # Test invalid vnic type.
        fake_port['binding:vnic_type'] = 'unknown'
        self.assertFalse(self.mech_driver._is_port_provisioning_required(
            fake_port, fake_host, None))
        fake_port['binding:vnic_type'] = 'normal'

        # Test invalid status.
        fake_port['status'] = const.PORT_STATUS_ACTIVE
        self.assertFalse(self.mech_driver._is_port_provisioning_required(
            fake_port, fake_host, None))
        fake_port['status'] = const.PORT_STATUS_DOWN

        # Test no host.
        self.assertFalse(self.mech_driver._is_port_provisioning_required(
            fake_port, None, None))

        # Test invalid host.
        self.sb_ovn.chassis_exists.return_value = False
        self.assertFalse(self.mech_driver._is_port_provisioning_required(
            fake_port, fake_host, None))
        self.sb_ovn.chassis_exists.return_value = True

        # Test port provisioning required.
        self.assertTrue(self.mech_driver._is_port_provisioning_required(
            fake_port, fake_host, None))

    def _test_add_subnet_dhcp_options_in_ovn(self, subnet, ovn_dhcp_opts=None,
                                             call_get_dhcp_opts=True,
                                             call_add_dhcp_opts=True):
        subnet['id'] = 'fake_id'
        with mock.patch.object(self.mech_driver._ovn_client,
                               '_get_ovn_dhcp_options') as get_opts:
            self.mech_driver._ovn_client._add_subnet_dhcp_options(
                subnet, mock.ANY, ovn_dhcp_opts)
            self.assertEqual(call_get_dhcp_opts, get_opts.called)
            self.assertEqual(
                call_add_dhcp_opts,
                self.mech_driver._nb_ovn.add_dhcp_options.called)

    def test_add_subnet_dhcp_options_in_ovn(self):
        subnet = {'ip_version': const.IP_VERSION_4}
        self._test_add_subnet_dhcp_options_in_ovn(subnet)

    def test_add_subnet_dhcp_options_in_ovn_with_given_ovn_dhcp_opts(self):
        subnet = {'ip_version': const.IP_VERSION_4}
        self._test_add_subnet_dhcp_options_in_ovn(
            subnet, ovn_dhcp_opts={'foo': 'bar', 'external_ids': {}},
            call_get_dhcp_opts=False)

    def test_add_subnet_dhcp_options_in_ovn_with_slaac_v6_subnet(self):
        subnet = {'ip_version': const.IP_VERSION_6,
                  'ipv6_address_mode': const.IPV6_SLAAC}
        self._test_add_subnet_dhcp_options_in_ovn(
            subnet, call_get_dhcp_opts=False, call_add_dhcp_opts=False)

    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_ports')
    @mock.patch('neutron_lib.utils.net.get_random_mac')
    def test_enable_subnet_dhcp_options_in_ovn_ipv4(self, grm, gps):
        grm.return_value = '01:02:03:04:05:06'
        gps.return_value = [
            {'id': 'port-id-1', 'device_owner': 'nova:compute'},
            {'id': 'port-id-2', 'device_owner': 'nova:compute',
             'extra_dhcp_opts': [
                 {'opt_value': '10.0.0.33', 'ip_version': 4,
                   'opt_name': 'router'}]},
            {'id': 'port-id-3', 'device_owner': 'nova:compute',
             'extra_dhcp_opts': [
                 {'opt_value': '1200', 'ip_version': 4,
                   'opt_name': 'mtu'}]},
            {'id': 'port-id-10', 'device_owner': 'network:foo'}]
        subnet = {'id': 'subnet-id', 'ip_version': 4, 'cidr': '10.0.0.0/24',
                  'network_id': 'network-id',
                  'gateway_ip': '10.0.0.1', 'enable_dhcp': True,
                  'dns_nameservers': [], 'host_routes': []}
        network = {'id': 'network-id', 'mtu': 1000}
        txn = self.mech_driver._nb_ovn.transaction().__enter__.return_value
        dhcp_option_command = mock.Mock()
        txn.add.return_value = dhcp_option_command

        self.mech_driver._ovn_client._enable_subnet_dhcp_options(
            subnet, network, txn)
        # Check adding DHCP_Options rows
        subnet_dhcp_options = {
            'external_ids': {'subnet_id': subnet['id'],
                             ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'},
            'cidr': subnet['cidr'], 'options': {
                'router': subnet['gateway_ip'],
                'server_id': subnet['gateway_ip'],
                'server_mac': '01:02:03:04:05:06',
                'dns_server': '{8.8.8.8}',
                'lease_time': str(12 * 60 * 60),
                'mtu': str(1000)}}
        ports_dhcp_options = [{
            'external_ids': {'subnet_id': subnet['id'],
                             ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
                             'port_id': 'port-id-2'},
            'cidr': subnet['cidr'], 'options': {
                'router': '10.0.0.33',
                'server_id': subnet['gateway_ip'],
                'dns_server': '{8.8.8.8}',
                'server_mac': '01:02:03:04:05:06',
                'lease_time': str(12 * 60 * 60),
                'mtu': str(1000)}}, {
            'external_ids': {'subnet_id': subnet['id'],
                             ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
                             'port_id': 'port-id-3'},
            'cidr': subnet['cidr'], 'options': {
                'router': subnet['gateway_ip'],
                'server_id': subnet['gateway_ip'],
                'dns_server': '{8.8.8.8}',
                'server_mac': '01:02:03:04:05:06',
                'lease_time': str(12 * 60 * 60),
                'mtu': str(1200)}}]
        add_dhcp_calls = [mock.call('subnet-id', **subnet_dhcp_options)]
        add_dhcp_calls.extend([mock.call(
            'subnet-id', port_id=port_dhcp_options['external_ids']['port_id'],
            **port_dhcp_options) for port_dhcp_options in ports_dhcp_options])
        self.assertEqual(len(add_dhcp_calls),
                         self.mech_driver._nb_ovn.add_dhcp_options.call_count)
        self.mech_driver._nb_ovn.add_dhcp_options.assert_has_calls(
            add_dhcp_calls, any_order=True)

        # Check setting lport rows
        set_lsp_calls = [mock.call(lport_name='port-id-1',
                                   dhcpv4_options=dhcp_option_command),
                         mock.call(lport_name='port-id-2',
                                   dhcpv4_options=dhcp_option_command),
                         mock.call(lport_name='port-id-3',
                                   dhcpv4_options=dhcp_option_command)]
        self.assertEqual(len(set_lsp_calls),
                         self.mech_driver._nb_ovn.set_lswitch_port.call_count)
        self.mech_driver._nb_ovn.set_lswitch_port.assert_has_calls(
            set_lsp_calls, any_order=True)

    @mock.patch('neutron.db.db_base_plugin_v2.NeutronDbPluginV2.get_ports')
    @mock.patch('neutron_lib.utils.net.get_random_mac')
    def test_enable_subnet_dhcp_options_in_ovn_ipv6(self, grm, gps):
        grm.return_value = '01:02:03:04:05:06'
        gps.return_value = [
            {'id': 'port-id-1', 'device_owner': 'nova:compute'},
            {'id': 'port-id-2', 'device_owner': 'nova:compute',
             'extra_dhcp_opts': [
                 {'opt_value': '11:22:33:44:55:66', 'ip_version': 6,
                   'opt_name': 'server-id'}]},
            {'id': 'port-id-3', 'device_owner': 'nova:compute',
             'extra_dhcp_opts': [
                 {'opt_value': '10::34', 'ip_version': 6,
                   'opt_name': 'dns-server'}]},
            {'id': 'port-id-10', 'device_owner': 'network:foo'}]
        subnet = {'id': 'subnet-id', 'ip_version': 6, 'cidr': '10::0/64',
                  'gateway_ip': '10::1', 'enable_dhcp': True,
                  'ipv6_address_mode': 'dhcpv6-stateless',
                  'dns_nameservers': [], 'host_routes': []}
        network = {'id': 'network-id', 'mtu': 1000}
        txn = self.mech_driver._nb_ovn.transaction().__enter__.return_value
        dhcp_option_command = mock.Mock()
        txn.add.return_value = dhcp_option_command

        self.mech_driver._ovn_client._enable_subnet_dhcp_options(
            subnet, network, txn)
        # Check adding DHCP_Options rows
        subnet_dhcp_options = {
            'external_ids': {'subnet_id': subnet['id'],
                             ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'},
            'cidr': subnet['cidr'], 'options': {
                'dhcpv6_stateless': 'true',
                'server_id': '01:02:03:04:05:06'}}
        ports_dhcp_options = [{
            'external_ids': {'subnet_id': subnet['id'],
                             ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
                             'port_id': 'port-id-2'},
            'cidr': subnet['cidr'], 'options': {
                'dhcpv6_stateless': 'true',
                'server_id': '11:22:33:44:55:66'}}, {
            'external_ids': {'subnet_id': subnet['id'],
                             ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1',
                             'port_id': 'port-id-3'},
            'cidr': subnet['cidr'], 'options': {
                'dhcpv6_stateless': 'true',
                'server_id': '01:02:03:04:05:06',
                'dns_server': '10::34'}}]
        add_dhcp_calls = [mock.call('subnet-id', **subnet_dhcp_options)]
        add_dhcp_calls.extend([mock.call(
            'subnet-id', port_id=port_dhcp_options['external_ids']['port_id'],
            **port_dhcp_options) for port_dhcp_options in ports_dhcp_options])
        self.assertEqual(len(add_dhcp_calls),
                         self.mech_driver._nb_ovn.add_dhcp_options.call_count)
        self.mech_driver._nb_ovn.add_dhcp_options.assert_has_calls(
            add_dhcp_calls, any_order=True)

        # Check setting lport rows
        set_lsp_calls = [mock.call(lport_name='port-id-1',
                                   dhcpv6_options=dhcp_option_command),
                         mock.call(lport_name='port-id-2',
                                   dhcpv6_options=dhcp_option_command),
                         mock.call(lport_name='port-id-3',
                                   dhcpv6_options=dhcp_option_command)]
        self.assertEqual(len(set_lsp_calls),
                         self.mech_driver._nb_ovn.set_lswitch_port.call_count)
        self.mech_driver._nb_ovn.set_lswitch_port.assert_has_calls(
            set_lsp_calls, any_order=True)

    def test_enable_subnet_dhcp_options_in_ovn_ipv6_slaac(self):
        subnet = {'id': 'subnet-id', 'ip_version': 6, 'enable_dhcp': True,
                  'ipv6_address_mode': 'slaac'}
        network = {'id': 'network-id'}

        self.mech_driver._ovn_client._enable_subnet_dhcp_options(
            subnet, network, mock.Mock())
        self.mech_driver._nb_ovn.add_dhcp_options.assert_not_called()
        self.mech_driver._nb_ovn.set_lswitch_port.assert_not_called()

    def _test_remove_subnet_dhcp_options_in_ovn(self, ip_version):
        opts = {'subnet': {'uuid': 'subnet-uuid'},
                'ports': [{'uuid': 'port1-uuid'}]}
        self.mech_driver._nb_ovn.get_subnet_dhcp_options.return_value = opts
        self.mech_driver._ovn_client._remove_subnet_dhcp_options(
            'subnet-id', mock.Mock())

        # Check deleting DHCP_Options rows
        delete_dhcp_calls = [mock.call('subnet-uuid'), mock.call('port1-uuid')]
        self.assertEqual(
            len(delete_dhcp_calls),
            self.mech_driver._nb_ovn.delete_dhcp_options.call_count)
        self.mech_driver._nb_ovn.delete_dhcp_options.assert_has_calls(
            delete_dhcp_calls, any_order=True)

    def test_remove_subnet_dhcp_options_in_ovn_ipv4(self):
        self._test_remove_subnet_dhcp_options_in_ovn(4)

    def test_remove_subnet_dhcp_options_in_ovn_ipv6(self):
        self._test_remove_subnet_dhcp_options_in_ovn(6)

    def test_update_subnet_dhcp_options_in_ovn_ipv4(self):
        subnet = {'id': 'subnet-id', 'ip_version': 4, 'cidr': '10.0.0.0/24',
                  'network_id': 'network-id',
                  'gateway_ip': '10.0.0.1', 'enable_dhcp': True,
                  'dns_nameservers': [], 'host_routes': []}
        network = {'id': 'network-id', 'mtu': 1000}
        orignal_options = {'subnet': {
            'external_ids': {'subnet_id': subnet['id']},
            'cidr': subnet['cidr'], 'options': {
                'router': '10.0.0.2',
                'server_id': '10.0.0.2',
                'server_mac': '01:02:03:04:05:06',
                'dns_server': '{8.8.8.8}',
                'lease_time': str(12 * 60 * 60),
                'mtu': str(1000)}}, 'ports': []}
        self.mech_driver._nb_ovn.get_subnet_dhcp_options.return_value =\
            orignal_options

        self.mech_driver._ovn_client._update_subnet_dhcp_options(
            subnet, network, mock.Mock())
        new_options = {
            'external_ids': {'subnet_id': subnet['id'],
                             ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'},
            'cidr': subnet['cidr'], 'options': {
                'router': subnet['gateway_ip'],
                'server_id': subnet['gateway_ip'],
                'dns_server': '{8.8.8.8}',
                'server_mac': '01:02:03:04:05:06',
                'lease_time': str(12 * 60 * 60),
                'mtu': str(1000)}}
        self.mech_driver._nb_ovn.add_dhcp_options.assert_called_once_with(
            subnet['id'], **new_options)

    def test_update_subnet_dhcp_options_in_ovn_ipv4_not_change(self):
        subnet = {'id': 'subnet-id', 'ip_version': 4, 'cidr': '10.0.0.0/24',
                  'network_id': 'network-id',
                  'gateway_ip': '10.0.0.1', 'enable_dhcp': True,
                  'dns_nameservers': [], 'host_routes': []}
        network = {'id': 'network-id', 'mtu': 1000}
        orignal_options = {'subnet': {
            'external_ids': {'subnet_id': subnet['id']},
            'cidr': subnet['cidr'], 'options': {
                'router': subnet['gateway_ip'],
                'server_id': subnet['gateway_ip'],
                'server_mac': '01:02:03:04:05:06',
                'dns_server': '{8.8.8.8}',
                'lease_time': str(12 * 60 * 60),
                'mtu': str(1000)}}, 'ports': []}
        self.mech_driver._nb_ovn.get_subnet_dhcp_options.return_value =\
            orignal_options

        self.mech_driver._ovn_client._update_subnet_dhcp_options(
            subnet, network, mock.Mock())
        self.mech_driver._nb_ovn.add_dhcp_options.assert_not_called()

    def test_update_subnet_dhcp_options_in_ovn_ipv6(self):
        subnet = {'id': 'subnet-id', 'ip_version': 6, 'cidr': '10::0/64',
                  'network_id': 'network-id',
                  'gateway_ip': '10::1', 'enable_dhcp': True,
                  'ipv6_address_mode': 'dhcpv6-stateless',
                  'dns_nameservers': ['10::3'], 'host_routes': []}
        network = {'id': 'network-id', 'mtu': 1000}
        orignal_options = {'subnet': {
            'external_ids': {'subnet_id': subnet['id']},
            'cidr': subnet['cidr'], 'options': {
                'dhcpv6_stateless': 'true',
                'server_id': '01:02:03:04:05:06'}}, 'ports': []}
        self.mech_driver._nb_ovn.get_subnet_dhcp_options.return_value =\
            orignal_options
        self.mech_driver._ovn_client._update_subnet_dhcp_options(
            subnet, network, mock.Mock())

        new_options = {
            'external_ids': {'subnet_id': subnet['id'],
                             ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'},
            'cidr': subnet['cidr'], 'options': {
                'dhcpv6_stateless': 'true',
                'dns_server': '{10::3}',
                'server_id': '01:02:03:04:05:06'}}
        self.mech_driver._nb_ovn.add_dhcp_options.assert_called_once_with(
            subnet['id'], **new_options)

    def test_update_subnet_dhcp_options_in_ovn_ipv6_not_change(self):
        subnet = {'id': 'subnet-id', 'ip_version': 6, 'cidr': '10::0/64',
                  'gateway_ip': '10::1', 'enable_dhcp': True,
                  'ipv6_address_mode': 'dhcpv6-stateless',
                  'dns_nameservers': [], 'host_routes': []}
        network = {'id': 'network-id', 'mtu': 1000}
        orignal_options = {'subnet': {
            'external_ids': {'subnet_id': subnet['id']},
            'cidr': subnet['cidr'], 'options': {
                'dhcpv6_stateless': 'true',
                'server_id': '01:02:03:04:05:06'}}, 'ports': []}
        self.mech_driver._nb_ovn.get_subnet_dhcp_options.return_value =\
            orignal_options

        self.mech_driver._ovn_client._update_subnet_dhcp_options(
            subnet, network, mock.Mock())
        self.mech_driver._nb_ovn.add_dhcp_options.assert_not_called()

    def test_update_subnet_dhcp_options_in_ovn_ipv6_slaac(self):
        subnet = {'id': 'subnet-id', 'ip_version': 6, 'enable_dhcp': True,
                  'ipv6_address_mode': 'slaac'}
        network = {'id': 'network-id'}
        self.mech_driver._ovn_client._update_subnet_dhcp_options(
            subnet, network, mock.Mock())
        self.mech_driver._nb_ovn.get_subnet_dhcp_options.assert_not_called()
        self.mech_driver._nb_ovn.add_dhcp_options.assert_not_called()

    def test_update_subnet_postcommit_ovn_do_nothing(self):
        context = fakes.FakeSubnetContext(
            subnet={'enable_dhcp': False, 'ip_version': 4, 'network_id': 'id',
                    'id': 'subnet_id'},
            network={'id': 'id'})
        with mock.patch.object(
                self.mech_driver._ovn_client,
                '_enable_subnet_dhcp_options') as esd,\
                mock.patch.object(
                    self.mech_driver._ovn_client,
                    '_remove_subnet_dhcp_options') as dsd,\
                mock.patch.object(
                    self.mech_driver._ovn_client,
                    '_update_subnet_dhcp_options') as usd,\
                mock.patch.object(
                    self.mech_driver._ovn_client,
                    '_find_metadata_port') as fmd,\
                mock.patch.object(
                    self.mech_driver._ovn_client,
                    'update_metadata_port') as umd:
            self.mech_driver.update_subnet_postcommit(context)
            esd.assert_not_called()
            dsd.assert_not_called()
            usd.assert_not_called()
            fmd.assert_not_called()
            umd.assert_not_called()

    def test_update_subnet_postcommit_enable_dhcp(self):
        context = fakes.FakeSubnetContext(
            subnet={'enable_dhcp': True, 'ip_version': 4, 'network_id': 'id',
                    'id': 'subnet_id'},
            network={'id': 'id'})
        with mock.patch.object(
                self.mech_driver._ovn_client,
                '_enable_subnet_dhcp_options') as esd,\
                mock.patch.object(
                self.mech_driver._ovn_client,
                'update_metadata_port') as umd:
            self.mech_driver.update_subnet_postcommit(context)
            esd.assert_called_once_with(
                context.current, context.network.current, mock.ANY)
            umd.assert_called_once_with(mock.ANY, 'id')

    def test_update_subnet_postcommit_disable_dhcp(self):
        self.mech_driver._nb_ovn.get_subnet_dhcp_options.return_value = {
            'subnet': mock.sentinel.subnet, 'ports': []}
        context = fakes.FakeSubnetContext(
            subnet={'enable_dhcp': False, 'id': 'fake_id', 'ip_version': 4,
                    'network_id': 'id'},
            network={'id': 'id'})
        with mock.patch.object(
                self.mech_driver._ovn_client,
                '_remove_subnet_dhcp_options') as dsd,\
                mock.patch.object(
                self.mech_driver._ovn_client,
                'update_metadata_port') as umd:
            self.mech_driver.update_subnet_postcommit(context)
            dsd.assert_called_once_with(context.current['id'], mock.ANY)
            umd.assert_called_once_with(mock.ANY, 'id')

    def test_update_subnet_postcommit_update_dhcp(self):
        self.mech_driver._nb_ovn.get_subnet_dhcp_options.return_value = {
            'subnet': mock.sentinel.subnet, 'ports': []}
        context = fakes.FakeSubnetContext(
            subnet={'enable_dhcp': True, 'ip_version': 4, 'network_id': 'id',
                    'id': 'subnet_id'},
            network={'id': 'id'})
        with mock.patch.object(
                self.mech_driver._ovn_client,
                '_update_subnet_dhcp_options') as usd,\
                mock.patch.object(
                self.mech_driver._ovn_client,
                'update_metadata_port') as umd:
            self.mech_driver.update_subnet_postcommit(context)
            usd.assert_called_once_with(
                context.current, context.network.current, mock.ANY)
            umd.assert_called_once_with(mock.ANY, 'id')

    @mock.patch.object(provisioning_blocks, 'is_object_blocked')
    @mock.patch.object(provisioning_blocks, 'provisioning_complete')
    def test_notify_dhcp_updated(self, mock_prov_complete, mock_is_obj_block):
        port_id = 'fake-port-id'
        mock_is_obj_block.return_value = True
        self.mech_driver._notify_dhcp_updated(port_id)
        mock_prov_complete.assert_called_once_with(
            mock.ANY, port_id, resources.PORT,
            provisioning_blocks.DHCP_ENTITY)

        mock_is_obj_block.return_value = False
        mock_prov_complete.reset_mock()
        self.mech_driver._notify_dhcp_updated(port_id)
        mock_prov_complete.assert_not_called()

    @mock.patch.object(mech_driver.OVNMechanismDriver,
                       '_is_port_provisioning_required', lambda *_: True)
    @mock.patch.object(mech_driver.OVNMechanismDriver, '_notify_dhcp_updated')
    @mock.patch.object(ovn_client.OVNClient, 'create_port')
    def test_create_port_postcommit(self, mock_create_port, mock_notify_dhcp):
        fake_port = fakes.FakePort.create_one_port(
            attrs={'status': const.PORT_STATUS_DOWN}).info()
        fake_ctx = mock.Mock(current=fake_port)

        self.mech_driver.create_port_postcommit(fake_ctx)

        mock_create_port.assert_called_once_with(fake_port)
        mock_notify_dhcp.assert_called_once_with(fake_port['id'])

    @mock.patch.object(mech_driver.OVNMechanismDriver,
                       '_is_port_provisioning_required', lambda *_: True)
    @mock.patch.object(mech_driver.OVNMechanismDriver, '_notify_dhcp_updated')
    @mock.patch.object(ovn_client.OVNClient, 'update_port')
    def test_update_port_postcommit(self, mock_update_port,
                                    mock_notify_dhcp):
        fake_port = fakes.FakePort.create_one_port(
            attrs={'status': const.PORT_STATUS_ACTIVE}).info()
        fake_ctx = mock.Mock(current=fake_port, original=fake_port)
        self.mech_driver.update_port_postcommit(fake_ctx)
        mock_update_port.assert_called_once_with(
            fake_port, port_object=fake_ctx.original)
        mock_notify_dhcp.assert_called_once_with(fake_port['id'])

    def _add_chassis_agent(self, nb_cfg, agent_type, updated_at=None):
        chassis = mock.Mock()
        chassis.nb_cfg = nb_cfg
        chassis.uuid = uuid.uuid4()
        id_ = chassis.uuid
        if agent_type == ovn_const.OVN_METADATA_AGENT:
            chassis.external_ids = {
                ovn_const.OVN_AGENT_METADATA_SB_CFG_KEY: nb_cfg}
            id_ = ovn_utils.ovn_metadata_name(chassis.uuid)

        stats.AgentStats.add_stat(id_, nb_cfg, updated_at)
        return chassis

    def test_agent_alive_true(self):
        for agent_type in (ovn_const.OVN_CONTROLLER_AGENT,
                           ovn_const.OVN_METADATA_AGENT):
            self.mech_driver._nb_ovn.nb_global.nb_cfg = 5
            chassis = self._add_chassis_agent(5, agent_type)
            self.assertTrue(self.mech_driver.agent_alive(chassis, agent_type))

    def test_agent_alive_not_timed_out(self):
        for agent_type in (ovn_const.OVN_CONTROLLER_AGENT,
                           ovn_const.OVN_METADATA_AGENT):
            self.mech_driver._nb_ovn.nb_global.nb_cfg = 5
            chassis = self._add_chassis_agent(4, agent_type)
            self.assertTrue(self.mech_driver.agent_alive(chassis, agent_type))

    def test_agent_alive_timed_out(self):
        for agent_type in (ovn_const.OVN_CONTROLLER_AGENT,
                           ovn_const.OVN_METADATA_AGENT):
            self.mech_driver._nb_ovn.nb_global.nb_cfg = 5
            now = timeutils.utcnow()
            updated_at = now - datetime.timedelta(cfg.CONF.agent_down_time + 1)
            chassis = self._add_chassis_agent(4, agent_type, updated_at)
            self.assertFalse(self.mech_driver.agent_alive(chassis, agent_type))

    def test_agent_not_found(self):
        agent_type = ovn_const.OVN_CONTROLLER_AGENT
        chassis = self._add_chassis_agent(1, agent_type)
        self.mech_driver._nb_ovn.nb_global.nb_cfg = 1

        # Assert that the agent has been registered and is alive
        self.assertTrue(self.mech_driver.agent_alive(chassis, agent_type))
        self.mech_driver._nb_ovn.nb_global.nb_cfg = 2
        # Delete the agent from the stats tracker
        stats.AgentStats.del_agent(chassis.uuid)
        # Assert that subsequently calls checking the status of the agent
        # shows it as "dead" instead of blowing up with an exception
        self.assertFalse(self.mech_driver.agent_alive(chassis, agent_type))


class OVNMechanismDriverTestCase(test_plugin.Ml2PluginV2TestCase):
    _mechanism_drivers = ['logger', 'ovn']

    def setUp(self):
        cfg.CONF.set_override('tenant_network_types',
                              ['geneve'],
                              group='ml2')
        cfg.CONF.set_override('vni_ranges',
                              ['1:65536'],
                              group='ml2_type_geneve')
        ovn_config.cfg.CONF.set_override('dns_servers',
                                         ['8.8.8.8'],
                                         group='ovn')
        super(OVNMechanismDriverTestCase, self).setUp()
        mm = directory.get_plugin().mechanism_manager
        self.mech_driver = mm.mech_drivers['ovn'].obj
        nb_ovn = fakes.FakeOvsdbNbOvnIdl()
        sb_ovn = fakes.FakeOvsdbSbOvnIdl()
        self.mech_driver._nb_ovn = nb_ovn
        self.mech_driver._sb_ovn = sb_ovn
        self.mech_driver._insert_port_provisioning_block = mock.Mock()
        p = mock.patch.object(ovn_utils, 'get_revision_number', return_value=1)
        p.start()
        self.addCleanup(p.stop)


class TestOVNMechansimDriverBasicGet(test_plugin.TestMl2BasicGet,
                                     OVNMechanismDriverTestCase):
    pass


class TestOVNMechansimDriverV2HTTPResponse(test_plugin.TestMl2V2HTTPResponse,
                                           OVNMechanismDriverTestCase):
    pass


class TestOVNMechansimDriverNetworksV2(test_plugin.TestMl2NetworksV2,
                                       OVNMechanismDriverTestCase):
    pass


class TestOVNMechansimDriverSubnetsV2(test_plugin.TestMl2SubnetsV2,
                                      OVNMechanismDriverTestCase):

    def setUp(self):
        # Disable metadata so that we don't interfere with existing tests
        # in Neutron tree. Doing this because some of the tests assume that
        # first IP address in a subnet will be available and this is not true
        # with metadata since it will book an IP address on each subnet.
        ovn_config.cfg.CONF.set_override('ovn_metadata_enabled',
                                         False,
                                         group='ovn')
        super(TestOVNMechansimDriverSubnetsV2, self).setUp()

    # NOTE(rtheis): Mock the OVN port update since it is getting subnet
    # information for ACL processing. This interferes with the update_port
    # mock already done by the test.
    def test_subnet_update_ipv4_and_ipv6_pd_v6stateless_subnets(self):
        with mock.patch.object(self.mech_driver._ovn_client, 'update_port'),\
                mock.patch.object(self.mech_driver._ovn_client,
                                  '_get_subnet_dhcp_options_for_port',
                                  return_value={}):
            super(TestOVNMechansimDriverSubnetsV2, self).\
                test_subnet_update_ipv4_and_ipv6_pd_v6stateless_subnets()

    # NOTE(rtheis): Mock the OVN port update since it is getting subnet
    # information for ACL processing. This interferes with the update_port
    # mock already done by the test.
    def test_subnet_update_ipv4_and_ipv6_pd_slaac_subnets(self):
        with mock.patch.object(self.mech_driver._ovn_client, 'update_port'),\
                mock.patch.object(self.mech_driver._ovn_client,
                                  '_get_subnet_dhcp_options_for_port',
                                  return_value={}):
            super(TestOVNMechansimDriverSubnetsV2, self).\
                test_subnet_update_ipv4_and_ipv6_pd_slaac_subnets()

    # NOTE(numans) Overriding the base test case here because the base test
    # case creates a network with vxlan type and OVN mech driver doesn't
    # support it.
    def test_create_subnet_check_mtu_in_mech_context(self):
        plugin = directory.get_plugin()
        plugin.mechanism_manager.create_subnet_precommit = mock.Mock()
        net_arg = {pnet.NETWORK_TYPE: 'geneve',
                   pnet.SEGMENTATION_ID: '1'}
        network = self._make_network(self.fmt, 'net1', True,
                                     arg_list=(pnet.NETWORK_TYPE,
                                               pnet.SEGMENTATION_ID,),
                                     **net_arg)
        with self.subnet(network=network):
            mock_subnet_pre = plugin.mechanism_manager.create_subnet_precommit
            observerd_mech_context = mock_subnet_pre.call_args_list[0][0][0]
            self.assertEqual(network['network']['mtu'],
                             observerd_mech_context.network.current['mtu'])


class TestOVNMechansimDriverPortsV2(test_plugin.TestMl2PortsV2,
                                    OVNMechanismDriverTestCase):

    def setUp(self):
        # Disable metadata so that we don't interfere with existing tests
        # in Neutron tree. Doing this because some of the tests assume that
        # first IP address in a subnet will be available and this is not true
        # with metadata since it will book an IP address on each subnet.
        ovn_config.cfg.CONF.set_override('ovn_metadata_enabled',
                                         False,
                                         group='ovn')
        super(TestOVNMechansimDriverPortsV2, self).setUp()

    # NOTE(rtheis): Override this test to verify that updating
    # a port MAC fails when the port is bound.
    def test_update_port_mac(self):
        self.check_update_port_mac(
            host_arg={portbindings.HOST_ID: 'fake-host'},
            arg_list=(portbindings.HOST_ID,),
            expected_status=exc.HTTPConflict.code,
            expected_error='PortBound')


class TestOVNMechansimDriverAllowedAddressPairs(
        test_plugin.TestMl2AllowedAddressPairs,
        OVNMechanismDriverTestCase):
    pass


class TestOVNMechansimDriverPortSecurity(
        test_ext_portsecurity.PSExtDriverTestCase,
        OVNMechanismDriverTestCase):
    pass


class TestOVNMechansimDriverSegment(test_segment.HostSegmentMappingTestCase):
    _mechanism_drivers = ['logger', 'ovn']

    def setUp(self):
        super(TestOVNMechansimDriverSegment, self).setUp()
        mm = directory.get_plugin().mechanism_manager
        self.mech_driver = mm.mech_drivers['ovn'].obj
        nb_ovn = fakes.FakeOvsdbNbOvnIdl()
        sb_ovn = fakes.FakeOvsdbSbOvnIdl()
        self.mech_driver._nb_ovn = nb_ovn
        self.mech_driver._sb_ovn = sb_ovn
        p = mock.patch.object(ovn_utils, 'get_revision_number', return_value=1)
        p.start()
        self.addCleanup(p.stop)

    def _test_segment_host_mapping(self):
        # Disable the callback to update SegmentHostMapping by default, so
        # that update_segment_host_mapping is the only path to add the mapping
        registry.unsubscribe(
            self.mech_driver._add_segment_host_mapping_for_segment,
            resources.SEGMENT, events.AFTER_CREATE)
        host = 'hostname'
        with self.network() as network:
            network = network['network']
        segment1 = self._test_create_segment(
            network_id=network['id'], physical_network='phys_net1',
            segmentation_id=200, network_type='vlan')['segment']

        # As geneve networks mtu shouldn't be more than 1450, update it
        data = {'network': {'mtu': 1450}}
        req = self.new_update_request('networks', data, network['id'])
        res = self.deserialize(self.fmt, req.get_response(self.api))
        self.assertEqual(1450, res['network']['mtu'])

        self._test_create_segment(
            network_id=network['id'],
            segmentation_id=200,
            network_type='geneve')['segment']
        self.mech_driver.update_segment_host_mapping(host, ['phys_net1'])
        segments_host_db = self._get_segments_for_host(host)
        self.assertEqual({segment1['id']}, set(segments_host_db))
        return network['id'], host

    def test_update_segment_host_mapping(self):
        network_id, host = self._test_segment_host_mapping()

        # Update the mapping
        segment2 = self._test_create_segment(
            network_id=network_id, physical_network='phys_net2',
            segmentation_id=201, network_type='vlan')['segment']
        self.mech_driver.update_segment_host_mapping(host, ['phys_net2'])
        segments_host_db = self._get_segments_for_host(host)
        self.assertEqual({segment2['id']}, set(segments_host_db))

    def test_clear_segment_host_mapping(self):
        _, host = self._test_segment_host_mapping()

        # Clear the mapping
        self.mech_driver.update_segment_host_mapping(host, [])
        segments_host_db = self._get_segments_for_host(host)
        self.assertEqual({}, segments_host_db)

    def test_update_segment_host_mapping_with_new_segment(self):
        hostname_with_physnets = {'hostname1': ['phys_net1', 'phys_net2'],
                                  'hostname2': ['phys_net1']}
        ovn_sb_api = self.mech_driver._sb_ovn
        ovn_sb_api.get_chassis_hostname_and_physnets.return_value = (
            hostname_with_physnets)
        self.mech_driver.subscribe()
        with self.network() as network:
            network_id = network['network']['id']
        segment = self._test_create_segment(
            network_id=network_id, physical_network='phys_net2',
            segmentation_id=201, network_type='vlan')['segment']
        segments_host_db1 = self._get_segments_for_host('hostname1')
        # A new SegmentHostMapping should be created for hostname1
        self.assertEqual({segment['id']}, set(segments_host_db1))

        segments_host_db2 = self._get_segments_for_host('hostname2')
        self.assertFalse(set(segments_host_db2))


@mock.patch.object(n_net, 'get_random_mac', lambda *_: '01:02:03:04:05:06')
class TestOVNMechansimDriverDHCPOptions(OVNMechanismDriverTestCase):

    def _test_get_ovn_dhcp_options_helper(self, subnet, network,
                                          expected_dhcp_options,
                                          service_mac=None):
        dhcp_options = self.mech_driver._ovn_client._get_ovn_dhcp_options(
            subnet, network, service_mac)
        self.assertEqual(expected_dhcp_options, dhcp_options)

    def test_get_ovn_dhcp_options(self):
        subnet = {'id': 'foo-subnet', 'network_id': 'network-id',
                  'cidr': '10.0.0.0/24',
                  'ip_version': 4,
                  'enable_dhcp': True,
                  'gateway_ip': '10.0.0.1',
                  'dns_nameservers': ['7.7.7.7', '8.8.8.8'],
                  'host_routes': [{'destination': '20.0.0.4',
                                   'nexthop': '10.0.0.100'}]}
        network = {'id': 'network-id', 'mtu': 1400}

        expected_dhcp_options = {'cidr': '10.0.0.0/24',
                                 'external_ids': {
                                     'subnet_id': 'foo-subnet',
                                     ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'}}
        expected_dhcp_options['options'] = {
            'server_id': subnet['gateway_ip'],
            'server_mac': '01:02:03:04:05:06',
            'lease_time': str(12 * 60 * 60),
            'mtu': '1400',
            'router': subnet['gateway_ip'],
            'dns_server': '{7.7.7.7, 8.8.8.8}',
            'classless_static_route':
            '{20.0.0.4,10.0.0.100, 0.0.0.0/0,10.0.0.1}'
        }

        self._test_get_ovn_dhcp_options_helper(subnet, network,
                                               expected_dhcp_options)
        expected_dhcp_options['options']['server_mac'] = '11:22:33:44:55:66'
        self._test_get_ovn_dhcp_options_helper(subnet, network,
                                               expected_dhcp_options,
                                               service_mac='11:22:33:44:55:66')

    def test_get_ovn_dhcp_options_dhcp_disabled(self):
        subnet = {'id': 'foo-subnet', 'network_id': 'network-id',
                  'cidr': '10.0.0.0/24',
                  'ip_version': 4,
                  'enable_dhcp': False,
                  'gateway_ip': '10.0.0.1',
                  'dns_nameservers': ['7.7.7.7', '8.8.8.8'],
                  'host_routes': [{'destination': '20.0.0.4',
                                   'nexthop': '10.0.0.100'}]}
        network = {'id': 'network-id', 'mtu': 1400}

        expected_dhcp_options = {'cidr': '10.0.0.0/24',
                                 'external_ids': {
                                     'subnet_id': 'foo-subnet',
                                     ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'},
                                 'options': {}}

        self._test_get_ovn_dhcp_options_helper(subnet, network,
                                               expected_dhcp_options)

    def test_get_ovn_dhcp_options_no_gw_ip(self):
        subnet = {'id': 'foo-subnet', 'network_id': 'network-id',
                  'cidr': '10.0.0.0/24',
                  'ip_version': 4,
                  'enable_dhcp': True,
                  'gateway_ip': None,
                  'dns_nameservers': ['7.7.7.7', '8.8.8.8'],
                  'host_routes': [{'destination': '20.0.0.4',
                                   'nexthop': '10.0.0.100'}]}
        network = {'id': 'network-id', 'mtu': 1400}

        expected_dhcp_options = {'cidr': '10.0.0.0/24',
                                 'external_ids': {
                                     'subnet_id': 'foo-subnet',
                                     ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'},
                                 'options': {}}

        self._test_get_ovn_dhcp_options_helper(subnet, network,
                                               expected_dhcp_options)

    def test_get_ovn_dhcp_options_no_gw_ip_but_metadata_ip(self):
        subnet = {'id': 'foo-subnet', 'network_id': 'network-id',
                  'cidr': '10.0.0.0/24',
                  'ip_version': 4,
                  'enable_dhcp': True,
                  'dns_nameservers': [],
                  'host_routes': [],
                  'gateway_ip': None}
        network = {'id': 'network-id', 'mtu': 1400}

        expected_dhcp_options = {
            'cidr': '10.0.0.0/24',
            'external_ids': {'subnet_id': 'foo-subnet',
                             ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'},
            'options': {'server_id': '10.0.0.2',
                        'server_mac': '01:02:03:04:05:06',
                        'dns_server': '{8.8.8.8}',
                        'lease_time': str(12 * 60 * 60),
                        'mtu': '1400',
                        'classless_static_route':
                            '{169.254.169.254/32,10.0.0.2}'}}

        with mock.patch.object(self.mech_driver._ovn_client,
                               '_find_metadata_port_ip',
                               return_value='10.0.0.2'):
            self._test_get_ovn_dhcp_options_helper(subnet, network,
                                                   expected_dhcp_options)

    def test_get_ovn_dhcp_options_with_global_options(self):
        ovn_config.cfg.CONF.set_override('ovn_dhcp4_global_options',
                                         'ntp_server:8.8.8.8,'
                                         'mtu:9000,'
                                         'wpad:',
                                         group='ovn')

        subnet = {'id': 'foo-subnet', 'network_id': 'network-id',
                  'cidr': '10.0.0.0/24',
                  'ip_version': 4,
                  'enable_dhcp': True,
                  'gateway_ip': '10.0.0.1',
                  'dns_nameservers': ['7.7.7.7', '8.8.8.8'],
                  'host_routes': [{'destination': '20.0.0.4',
                                   'nexthop': '10.0.0.100'}]}
        network = {'id': 'network-id', 'mtu': 1400}

        expected_dhcp_options = {'cidr': '10.0.0.0/24',
                                 'external_ids': {
                                     'subnet_id': 'foo-subnet',
                                     ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'}}
        expected_dhcp_options['options'] = {
            'server_id': subnet['gateway_ip'],
            'server_mac': '01:02:03:04:05:06',
            'lease_time': str(12 * 60 * 60),
            'mtu': '1400',
            'router': subnet['gateway_ip'],
            'ntp_server': '8.8.8.8',
            'dns_server': '{7.7.7.7, 8.8.8.8}',
            'classless_static_route':
            '{20.0.0.4,10.0.0.100, 0.0.0.0/0,10.0.0.1}'
        }

        self._test_get_ovn_dhcp_options_helper(subnet, network,
                                               expected_dhcp_options)
        expected_dhcp_options['options']['server_mac'] = '11:22:33:44:55:66'
        self._test_get_ovn_dhcp_options_helper(subnet, network,
                                               expected_dhcp_options,
                                               service_mac='11:22:33:44:55:66')

    def test_get_ovn_dhcp_options_with_global_options_ipv6(self):
        ovn_config.cfg.CONF.set_override('ovn_dhcp6_global_options',
                                         'ntp_server:8.8.8.8,'
                                         'server_id:01:02:03:04:05:04,'
                                         'wpad:',
                                         group='ovn')

        subnet = {'id': 'foo-subnet', 'network_id': 'network-id',
                  'cidr': 'ae70::/24',
                  'ip_version': 6,
                  'enable_dhcp': True,
                  'dns_nameservers': ['7.7.7.7', '8.8.8.8']}
        network = {'id': 'network-id', 'mtu': 1400}

        ext_ids = {'subnet_id': 'foo-subnet',
                   ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'}
        expected_dhcp_options = {
            'cidr': 'ae70::/24', 'external_ids': ext_ids,
            'options': {'server_id': '01:02:03:04:05:06',
                        'ntp_server': '8.8.8.8',
                        'dns_server': '{7.7.7.7, 8.8.8.8}'}}

        self._test_get_ovn_dhcp_options_helper(subnet, network,
                                               expected_dhcp_options)
        expected_dhcp_options['options']['server_id'] = '11:22:33:44:55:66'
        self._test_get_ovn_dhcp_options_helper(subnet, network,
                                               expected_dhcp_options,
                                               service_mac='11:22:33:44:55:66')

    def test_get_ovn_dhcp_options_ipv6_subnet(self):
        subnet = {'id': 'foo-subnet', 'network_id': 'network-id',
                  'cidr': 'ae70::/24',
                  'ip_version': 6,
                  'enable_dhcp': True,
                  'dns_nameservers': ['7.7.7.7', '8.8.8.8']}
        network = {'id': 'network-id', 'mtu': 1400}

        ext_ids = {'subnet_id': 'foo-subnet',
                   ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'}
        expected_dhcp_options = {
            'cidr': 'ae70::/24', 'external_ids': ext_ids,
            'options': {'server_id': '01:02:03:04:05:06',
                        'dns_server': '{7.7.7.7, 8.8.8.8}'}}

        self._test_get_ovn_dhcp_options_helper(subnet, network,
                                               expected_dhcp_options)
        expected_dhcp_options['options']['server_id'] = '11:22:33:44:55:66'
        self._test_get_ovn_dhcp_options_helper(subnet, network,
                                               expected_dhcp_options,
                                               service_mac='11:22:33:44:55:66')

    def test_get_ovn_dhcp_options_dhcpv6_stateless_subnet(self):
        subnet = {'id': 'foo-subnet', 'network_id': 'network-id',
                  'cidr': 'ae70::/24',
                  'ip_version': 6,
                  'enable_dhcp': True,
                  'dns_nameservers': ['7.7.7.7', '8.8.8.8'],
                  'ipv6_address_mode': const.DHCPV6_STATELESS}
        network = {'id': 'network-id', 'mtu': 1400}

        ext_ids = {'subnet_id': 'foo-subnet',
                   ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'}
        expected_dhcp_options = {
            'cidr': 'ae70::/24', 'external_ids': ext_ids,
            'options': {'server_id': '01:02:03:04:05:06',
                        'dns_server': '{7.7.7.7, 8.8.8.8}',
                        'dhcpv6_stateless': 'true'}}

        self._test_get_ovn_dhcp_options_helper(subnet, network,
                                               expected_dhcp_options)
        expected_dhcp_options['options']['server_id'] = '11:22:33:44:55:66'
        self._test_get_ovn_dhcp_options_helper(subnet, network,
                                               expected_dhcp_options,
                                               service_mac='11:22:33:44:55:66')

    def test_get_ovn_dhcp_options_metadata_route(self):
        subnet = {'id': 'foo-subnet', 'network_id': 'network-id',
                  'cidr': '10.0.0.0/24',
                  'ip_version': 4,
                  'enable_dhcp': True,
                  'gateway_ip': '10.0.0.1',
                  'dns_nameservers': ['7.7.7.7', '8.8.8.8'],
                  'host_routes': []}
        network = {'id': 'network-id', 'mtu': 1400}

        expected_dhcp_options = {'cidr': '10.0.0.0/24',
                                 'external_ids': {
                                     'subnet_id': 'foo-subnet',
                                     ovn_const.OVN_REV_NUM_EXT_ID_KEY: '1'}}
        expected_dhcp_options['options'] = {
            'server_id': subnet['gateway_ip'],
            'server_mac': '01:02:03:04:05:06',
            'lease_time': str(12 * 60 * 60),
            'mtu': '1400',
            'router': subnet['gateway_ip'],
            'dns_server': '{7.7.7.7, 8.8.8.8}',
            'classless_static_route':
            '{169.254.169.254/32,10.0.0.2, 0.0.0.0/0,10.0.0.1}'
        }

        with mock.patch.object(self.mech_driver._ovn_client,
                               '_find_metadata_port_ip',
                               return_value='10.0.0.2'):
            self._test_get_ovn_dhcp_options_helper(subnet, network,
                                                   expected_dhcp_options)

    def _test__get_port_dhcp_options_port_dhcp_opts_set(self, ip_version=4):
        if ip_version == 4:
            ip_address = '10.0.0.11'
        else:
            ip_address = 'aef0::4'

        port = {
            'id': 'foo-port',
            'device_owner': 'compute:None',
            'fixed_ips': [{'subnet_id': 'foo-subnet',
                           'ip_address': ip_address}]}
        if ip_version == 4:
            port['extra_dhcp_opts'] = [
                {'ip_version': 4, 'opt_name': 'mtu', 'opt_value': '1200'},
                {'ip_version': 4, 'opt_name': 'ntp-server',
                 'opt_value': '8.8.8.8'}]
        else:
            port['extra_dhcp_opts'] = [
                {'ip_version': 6, 'opt_name': 'domain-search',
                 'opt_value': 'foo-domain'},
                {'ip_version': 4, 'opt_name': 'dns-server',
                 'opt_value': '7.7.7.7'}]

        self.mech_driver._ovn_client._get_subnet_dhcp_options_for_port = (
            mock.Mock(
                return_value=({
                    'cidr': '10.0.0.0/24' if ip_version == 4 else 'aef0::/64',
                    'external_ids': {'subnet_id': 'foo-subnet'},
                    'options': (ip_version == 4) and {
                        'router': '10.0.0.1', 'mtu': '1400'} or {
                        'server_id': '01:02:03:04:05:06'},
                    'uuid': 'foo-uuid'})))

        if ip_version == 4:
            expected_dhcp_options = {
                'cidr': '10.0.0.0/24',
                'external_ids': {'subnet_id': 'foo-subnet',
                                 'port_id': 'foo-port'},
                'options': {'router': '10.0.0.1', 'mtu': '1200',
                            'ntp_server': '8.8.8.8'}}
        else:
            expected_dhcp_options = {
                'cidr': 'aef0::/64',
                'external_ids': {'subnet_id': 'foo-subnet',
                                 'port_id': 'foo-port'},
                'options': {'server_id': '01:02:03:04:05:06',
                            'domain_search': 'foo-domain'}}

        self.mech_driver._nb_ovn.add_dhcp_options.return_value = 'foo-val'
        dhcp_options = self.mech_driver._ovn_client._get_port_dhcp_options(
            port, ip_version)
        self.assertEqual({'cmd': 'foo-val'}, dhcp_options)
        self.mech_driver._nb_ovn.add_dhcp_options.assert_called_once_with(
            'foo-subnet', port_id='foo-port', **expected_dhcp_options)

    def test__get_port_dhcp_options_port_dhcp_opts_set_v4(self):
        self._test__get_port_dhcp_options_port_dhcp_opts_set(ip_version=4)

    def test__get_port_dhcp_options_port_dhcp_opts_set_v6(self):
        self._test__get_port_dhcp_options_port_dhcp_opts_set(ip_version=6)

    def _test__get_port_dhcp_options_port_dhcp_opts_not_set(
        self, ip_version=4):
        if ip_version == 4:
            port = {'id': 'foo-port',
                    'device_owner': 'compute:None',
                    'fixed_ips': [{'subnet_id': 'foo-subnet',
                                   'ip_address': '10.0.0.11'}]}
        else:
            port = {'id': 'foo-port',
                    'device_owner': 'compute:None',
                    'fixed_ips': [{'subnet_id': 'foo-subnet',
                                   'ip_address': 'aef0::4'}]}

        if ip_version == 4:
            expected_dhcp_opts = {
                'cidr': '10.0.0.0/24',
                'external_ids': {'subnet_id': 'foo-subnet'},
                'options': {'router': '10.0.0.1', 'mtu': '1400'}}
        else:
            expected_dhcp_opts = {
                'cidr': 'aef0::/64',
                'external_ids': {'subnet_id': 'foo-subnet'},
                'options': {'server_id': '01:02:03:04:05:06'}}

        self.mech_driver._ovn_client._get_subnet_dhcp_options_for_port = (
            mock.Mock(return_value=expected_dhcp_opts))

        self.assertEqual(
            expected_dhcp_opts,
            self.mech_driver._ovn_client._get_port_dhcp_options(
                port, ip_version=ip_version))

        # Since the port has no extra DHCPv4/v6 options defined, no new
        # DHCP_Options row should be created and logical switch port DHCPv4/v6
        # options should point to the subnet DHCPv4/v6 options.
        self.mech_driver._nb_ovn.add_dhcp_options.assert_not_called()

    def test__get_port_dhcp_options_port_dhcp_opts_not_set_v4(self):
        self._test__get_port_dhcp_options_port_dhcp_opts_not_set(ip_version=4)

    def test__get_port_dhcp_options_port_dhcp_opts_not_set_v6(self):
        self._test__get_port_dhcp_options_port_dhcp_opts_not_set(ip_version=6)

    def _test__get_port_dhcp_options_port_dhcp_disabled(self, ip_version=4):
        port = {
            'id': 'foo-port',
            'device_owner': 'compute:None',
            'fixed_ips': [{'subnet_id': 'foo-subnet',
                           'ip_address': '10.0.0.11'},
                          {'subnet_id': 'foo-subnet-v6',
                           'ip_address': 'aef0::11'}],
            'extra_dhcp_opts': [{'ip_version': 4, 'opt_name': 'dhcp_disabled',
                                 'opt_value': 'False'},
                                {'ip_version': 6, 'opt_name': 'dhcp_disabled',
                                 'opt_value': 'False'}]
            }

        subnet_dhcp_opts = mock.Mock()
        self.mech_driver._ovn_client._get_subnet_dhcp_options_for_port = (
            mock.Mock(return_value=subnet_dhcp_opts))

        # No dhcp_disabled set to true, subnet dhcp options will be get for
        # this port. Since it doesn't have any other extra dhcp options, but
        # dhcp_disabled, no port dhcp options will be created.
        self.assertEqual(
            subnet_dhcp_opts,
            self.mech_driver._ovn_client._get_port_dhcp_options(
                port, ip_version))
        self.assertEqual(
            1,
            self.mech_driver._ovn_client._get_subnet_dhcp_options_for_port.
            call_count)
        self.mech_driver._nb_ovn.add_dhcp_options.assert_not_called()

        # Set dhcp_disabled with ip_version specified by this test case to
        # true, no dhcp options will be get since it's dhcp_disabled now for
        # ip_version be tested.
        opt_index = 0 if ip_version == 4 else 1
        port['extra_dhcp_opts'][opt_index]['opt_value'] = 'True'
        self.mech_driver._ovn_client._get_subnet_dhcp_options_for_port.\
            reset_mock()
        self.assertIsNone(
            self.mech_driver._ovn_client._get_port_dhcp_options(
                port, ip_version))
        self.assertEqual(
            0,
            self.mech_driver._ovn_client._get_subnet_dhcp_options_for_port.
            call_count)
        self.mech_driver._nb_ovn.add_dhcp_options.assert_not_called()

        # Set dhcp_disabled with ip_version specified by this test case to
        # false, and set dhcp_disabled with ip_version not in test to true.
        # Subnet dhcp options will be get, since dhcp_disabled with ip_version
        # not in test should not affect.
        opt_index_1 = 1 if ip_version == 4 else 0
        port['extra_dhcp_opts'][opt_index]['opt_value'] = 'False'
        port['extra_dhcp_opts'][opt_index_1]['opt_value'] = 'True'
        self.assertEqual(
            subnet_dhcp_opts,
            self.mech_driver._ovn_client._get_port_dhcp_options(
                port, ip_version))
        self.assertEqual(
            1,
            self.mech_driver._ovn_client._get_subnet_dhcp_options_for_port.
            call_count)
        self.mech_driver._nb_ovn.add_dhcp_options.assert_not_called()

    def test__get_port_dhcp_options_port_dhcp_disabled_v4(self):
        self._test__get_port_dhcp_options_port_dhcp_disabled(ip_version=4)

    def test__get_port_dhcp_options_port_dhcp_disabled_v6(self):
        self._test__get_port_dhcp_options_port_dhcp_disabled(ip_version=6)

    def test__get_port_dhcp_options_port_with_invalid_device_owner(self):
        port = {
            'id': 'foo-port',
            'device_owner': 'neutron:router_interface',
            'fixed_ips': ['fake']
        }

        self.assertIsNone(
            self.mech_driver._ovn_client._get_port_dhcp_options(
                port, mock.ANY))

    def _test__get_subnet_dhcp_options_for_port(self, ip_version=4,
                                                enable_dhcp=True):
        port = {'fixed_ips': [
            {'ip_address': '10.0.0.4',
             'subnet_id': 'v4_snet_id_1' if enable_dhcp else 'v4_snet_id_2'},
            {'ip_address': '2001:dba::4',
             'subnet_id': 'v6_snet_id_1' if enable_dhcp else 'v6_snet_id_2'},
            {'ip_address': '2001:dbb::4', 'subnet_id': 'v6_snet_id_3'}]}

        def fake(subnets):
            fake_rows = {
                'v4_snet_id_1': 'foo',
                'v6_snet_id_1': {'options': {}},
                'v6_snet_id_3': {'options': {
                    ovn_const.DHCPV6_STATELESS_OPT: 'true'}}}
            return [fake_rows[row] for row in fake_rows if row in subnets]

        self.mech_driver._nb_ovn.get_subnets_dhcp_options.side_effect = fake

        if ip_version == 4:
            expected_opts = 'foo' if enable_dhcp else None
        else:
            expected_opts = {
                'options': {} if enable_dhcp else {
                    ovn_const.DHCPV6_STATELESS_OPT: 'true'}}

        self.assertEqual(
            expected_opts,
            self.mech_driver._ovn_client._get_subnet_dhcp_options_for_port(
                port, ip_version))

    def test__get_subnet_dhcp_options_for_port_v4(self):
        self._test__get_subnet_dhcp_options_for_port()

    def test__get_subnet_dhcp_options_for_port_v4_dhcp_disabled(self):
        self._test__get_subnet_dhcp_options_for_port(enable_dhcp=False)

    def test__get_subnet_dhcp_options_for_port_v6(self):
        self._test__get_subnet_dhcp_options_for_port(ip_version=6)

    def test__get_subnet_dhcp_options_for_port_v6_dhcp_disabled(self):
        self._test__get_subnet_dhcp_options_for_port(ip_version=6,
                                                     enable_dhcp=False)


class TestOVNMechanismDriverSecurityGroup(
    test_security_group.Ml2SecurityGroupsTestCase):
    # This set of test cases is supplement to test_acl.py, the purpose is to
    # test acl methods invoking. Content correctness of args of acl methods
    # is mainly guaranteed by acl_test.py.

    def setUp(self):
        cfg.CONF.set_override('mechanism_drivers',
                              ['logger', 'ovn'],
                              'ml2')
        cfg.CONF.set_override('dns_servers', ['8.8.8.8'], group='ovn')
        super(TestOVNMechanismDriverSecurityGroup, self).setUp()
        mm = directory.get_plugin().mechanism_manager
        self.mech_driver = mm.mech_drivers['ovn'].obj
        nb_ovn = fakes.FakeOvsdbNbOvnIdl()
        sb_ovn = fakes.FakeOvsdbSbOvnIdl()
        self.mech_driver._nb_ovn = nb_ovn
        self.mech_driver._sb_ovn = sb_ovn
        self.ctx = context.get_admin_context()
        revision_plugin.RevisionPlugin()

    def _delete_default_sg_rules(self, security_group_id):
        res = self._list(
            'security-group-rules',
            query_params='security_group_id=%s' % security_group_id)
        for r in res['security_group_rules']:
            self._delete('security-group-rules', r['id'])

    def _create_sg(self, sg_name):
        sg = self._make_security_group(self.fmt, sg_name, '')
        return sg['security_group']

    def _create_empty_sg(self, sg_name):
        sg = self._create_sg(sg_name)
        self._delete_default_sg_rules(sg['id'])
        return sg

    def _create_sg_rule(self, sg_id, direction, proto,
                        port_range_min=None, port_range_max=None,
                        remote_ip_prefix=None, remote_group_id=None,
                        ethertype=const.IPv4):
        r = self._build_security_group_rule(sg_id, direction, proto,
                                            port_range_min=port_range_min,
                                            port_range_max=port_range_max,
                                            remote_ip_prefix=remote_ip_prefix,
                                            remote_group_id=remote_group_id,
                                            ethertype=ethertype)
        res = self._create_security_group_rule(self.fmt, r)
        rule = self.deserialize(self.fmt, res)
        return rule['security_group_rule']

    def _delete_sg_rule(self, rule_id):
        self._delete('security-group-rules', rule_id)

    def test_create_security_group_with_port_group(self):
        self.mech_driver._nb_ovn.is_port_groups_supported.return_value = True
        sg = self._create_sg('sg')

        expected_pg_name = ovn_utils.ovn_port_group_name(sg['id'])
        expected_pg_add_calls = [
            mock.call(acls=[],
                      external_ids={'neutron:security_group_id': sg['id']},
                      name=expected_pg_name),
        ]
        self.mech_driver._nb_ovn.pg_add.assert_has_calls(
            expected_pg_add_calls)

    def test_delete_security_group_with_port_group(self):
        self.mech_driver._nb_ovn.is_port_groups_supported.return_value = True
        sg = self._create_sg('sg')
        self._delete('security-groups', sg['id'])

        expected_pg_name = ovn_utils.ovn_port_group_name(sg['id'])
        expected_pg_del_calls = [
            mock.call(name=expected_pg_name),
        ]
        self.mech_driver._nb_ovn.pg_del.assert_has_calls(
            expected_pg_del_calls)

    def test_create_port_with_port_group(self):
        self.mech_driver._nb_ovn.is_port_groups_supported.return_value = True
        with self.network() as n, self.subnet(n):
            sg = self._create_empty_sg('sg')
            self._make_port(self.fmt, n['network']['id'],
                            security_groups=[sg['id']])

            # Assert the port has been added to the right security groups
            expected_pg_name = ovn_utils.ovn_port_group_name(sg['id'])
            expected_pg_add_ports_calls = [
                mock.call('neutron_pg_drop', mock.ANY),
                mock.call(expected_pg_name, mock.ANY)
            ]
            self.mech_driver._nb_ovn.pg_add_ports.assert_has_calls(
                expected_pg_add_ports_calls)

            # Assert add_acl() is not used anymore
            self.assertFalse(self.mech_driver._nb_ovn.add_acl.called)

    def test_create_port_with_sg_default_rules(self):
        with self.network() as n, self.subnet(n):
            sg = self._create_sg('sg')
            self._make_port(self.fmt, n['network']['id'],
                            security_groups=[sg['id']])

            # One DHCP rule, one IPv6 rule, one IPv4 rule and
            # two default dropping rules.
            self.assertEqual(
                5, self.mech_driver._nb_ovn.add_acl.call_count)

    def test_create_port_with_empty_sg(self):
        with self.network() as n, self.subnet(n):
            sg = self._create_empty_sg('sg')
            self._make_port(self.fmt, n['network']['id'],
                            security_groups=[sg['id']])
            # One DHCP rule and two default dropping rules.
            self.assertEqual(
                3, self.mech_driver._nb_ovn.add_acl.call_count)

    def test_create_port_with_multi_sgs(self):
        with self.network() as n, self.subnet(n):
            sg1 = self._create_empty_sg('sg1')
            sg2 = self._create_empty_sg('sg2')
            self._create_sg_rule(sg1['id'], 'ingress', const.PROTO_NAME_TCP,
                                 port_range_min=22, port_range_max=23)
            self._create_sg_rule(sg2['id'], 'egress', const.PROTO_NAME_UDP,
                                 remote_ip_prefix='0.0.0.0/0')
            self._make_port(self.fmt, n['network']['id'],
                            security_groups=[sg1['id'], sg2['id']])

            # One DHCP rule, one TCP rule, one UDP rule and
            # two default dropping rules.
            self.assertEqual(
                5, self.mech_driver._nb_ovn.add_acl.call_count)

    def test_create_port_with_multi_sgs_duplicate_rules(self):
        with self.network() as n, self.subnet(n):
            sg1 = self._create_empty_sg('sg1')
            sg2 = self._create_empty_sg('sg2')
            self._create_sg_rule(sg1['id'], 'ingress', const.PROTO_NAME_TCP,
                                 port_range_min=22, port_range_max=23,
                                 remote_ip_prefix='20.0.0.0/24')
            self._create_sg_rule(sg2['id'], 'ingress', const.PROTO_NAME_TCP,
                                 port_range_min=22, port_range_max=23,
                                 remote_ip_prefix='20.0.0.0/24')
            self._make_port(self.fmt, n['network']['id'],
                            security_groups=[sg1['id'], sg2['id']])

            # One DHCP rule, two TCP rule and two default dropping rules.
            self.assertEqual(
                5, self.mech_driver._nb_ovn.add_acl.call_count)

    def test_update_port_with_sgs(self):
        with self.network() as n, self.subnet(n):
            sg1 = self._create_empty_sg('sg1')
            self._create_sg_rule(sg1['id'], 'ingress', const.PROTO_NAME_TCP,
                                 ethertype=const.IPv6)

            p = self._make_port(self.fmt, n['network']['id'],
                                security_groups=[sg1['id']])['port']
            # One DHCP rule, one TCP rule and two default dropping rules.
            self.assertEqual(
                4, self.mech_driver._nb_ovn.add_acl.call_count)

            sg2 = self._create_empty_sg('sg2')
            self._create_sg_rule(sg2['id'], 'egress', const.PROTO_NAME_UDP,
                                 remote_ip_prefix='30.0.0.0/24')
            data = {'port': {'security_groups': [sg1['id'], sg2['id']]}}
            req = self.new_update_request('ports', data, p['id'])
            req.get_response(self.api)
            self.assertEqual(
                1, self.mech_driver._nb_ovn.update_acls.call_count)

    def test_update_sg_change_rule(self):
        with self.network() as n, self.subnet(n):
            sg = self._create_empty_sg('sg')

            self._make_port(self.fmt, n['network']['id'],
                            security_groups=[sg['id']])
            # One DHCP rule and two default dropping rules.
            self.assertEqual(
                3, self.mech_driver._nb_ovn.add_acl.call_count)

            sg_r = self._create_sg_rule(sg['id'], 'ingress',
                                        const.PROTO_NAME_UDP,
                                        ethertype=const.IPv6)
            self.assertEqual(
                1, self.mech_driver._nb_ovn.update_acls.call_count)

            self._delete_sg_rule(sg_r['id'])
            self.assertEqual(
                2, self.mech_driver._nb_ovn.update_acls.call_count)

    def test_update_sg_change_rule_unrelated_port(self):
        with self.network() as n, self.subnet(n):
            sg1 = self._create_empty_sg('sg1')
            sg2 = self._create_empty_sg('sg2')
            self._create_sg_rule(sg1['id'], 'ingress', const.PROTO_NAME_TCP,
                                 remote_group_id=sg2['id'])

            self._make_port(self.fmt, n['network']['id'],
                            security_groups=[sg1['id']])
            # One DHCP rule, one TCP rule and two default dropping rules.
            self.assertEqual(
                4, self.mech_driver._nb_ovn.add_acl.call_count)

            sg2_r = self._create_sg_rule(sg2['id'], 'egress',
                                         const.PROTO_NAME_UDP)
            self.mech_driver._nb_ovn.update_acls.assert_not_called()

            self._delete_sg_rule(sg2_r['id'])
            self.mech_driver._nb_ovn.update_acls.assert_not_called()

    def test_update_sg_duplicate_rule(self):
        with self.network() as n, self.subnet(n):
            sg1 = self._create_empty_sg('sg1')
            sg2 = self._create_empty_sg('sg2')
            self._create_sg_rule(sg1['id'], 'ingress',
                                 const.PROTO_NAME_UDP,
                                 port_range_min=22, port_range_max=23)
            self._make_port(self.fmt, n['network']['id'],
                            security_groups=[sg1['id'], sg2['id']])
            # One DHCP rule, one UDP rule and two default dropping rules.
            self.assertEqual(
                4, self.mech_driver._nb_ovn.add_acl.call_count)

            # Add a new duplicate rule to sg2. It's expected to be added.
            sg2_r = self._create_sg_rule(sg2['id'], 'ingress',
                                         const.PROTO_NAME_UDP,
                                         port_range_min=22, port_range_max=23)
            self.mech_driver._nb_ovn.update_acls.assert_called_once()

            # Delete the duplicate rule. It's expected to be deleted.
            self._delete_sg_rule(sg2_r['id'])
            self.assertEqual(
                2, self.mech_driver._nb_ovn.update_acls.call_count)

    def test_update_sg_duplicate_rule_multi_ports(self):
        with self.network() as n, self.subnet(n):
            sg1 = self._create_empty_sg('sg1')
            sg2 = self._create_empty_sg('sg2')
            sg3 = self._create_empty_sg('sg3')
            self._create_sg_rule(sg1['id'], 'ingress',
                                 const.PROTO_NAME_UDP,
                                 remote_group_id=sg3['id'])
            self._create_sg_rule(sg2['id'], 'egress', const.PROTO_NAME_TCP,
                                 port_range_min=60, port_range_max=70)

            self._make_port(self.fmt, n['network']['id'],
                            security_groups=[sg1['id'], sg2['id']])
            self._make_port(self.fmt, n['network']['id'],
                            security_groups=[sg1['id'], sg2['id']])
            self._make_port(self.fmt, n['network']['id'],
                            security_groups=[sg2['id'], sg3['id']])
            # Rules include 5 + 5 + 4
            self.assertEqual(
                14, self.mech_driver._nb_ovn.add_acl.call_count)

            # Add a rule to sg1 duplicate with sg2. It's expected to be added.
            sg1_r = self._create_sg_rule(sg1['id'], 'egress',
                                         const.PROTO_NAME_TCP,
                                         port_range_min=60, port_range_max=70)
            self.mech_driver._nb_ovn.update_acls.assert_called_once()

            # Add a rule to sg2 duplicate with sg1 but not duplicate with sg3.
            # It's expected to be added as well.
            sg2_r = self._create_sg_rule(sg2['id'], 'ingress',
                                         const.PROTO_NAME_UDP,
                                         remote_group_id=sg3['id'])
            self.assertEqual(
                2, self.mech_driver._nb_ovn.update_acls.call_count)

            # Delete the duplicate rule in sg1. It's expected to be deleted.
            self._delete_sg_rule(sg1_r['id'])
            self.assertEqual(
                3, self.mech_driver._nb_ovn.update_acls.call_count)

            # Delete the duplicate rule in sg2. It's expected to be deleted.
            self._delete_sg_rule(sg2_r['id'])
            self.assertEqual(
                4, self.mech_driver._nb_ovn.update_acls.call_count)


class TestOVNMechanismDriverMetadataPort(test_plugin.Ml2PluginV2TestCase):

    _mechanism_drivers = ['logger', 'ovn']

    def setUp(self):
        super(TestOVNMechanismDriverMetadataPort, self).setUp()
        mm = directory.get_plugin().mechanism_manager
        self.mech_driver = mm.mech_drivers['ovn'].obj
        self.mech_driver._nb_ovn = fakes.FakeOvsdbNbOvnIdl()
        self.mech_driver._sb_ovn = fakes.FakeOvsdbSbOvnIdl()
        self.nb_ovn = self.mech_driver._nb_ovn
        self.sb_ovn = self.mech_driver._sb_ovn
        ovn_config.cfg.CONF.set_override('ovn_metadata_enabled',
                                         True,
                                         group='ovn')
        p = mock.patch.object(ovn_utils, 'get_revision_number', return_value=1)
        p.start()
        self.addCleanup(p.stop)

    def test_metadata_port_on_network_create(self):
        """Check metadata port create.

        Check that a localport is created when a neutron network is
        created.
        """
        with self.network():
            self.assertEqual(1, self.nb_ovn.create_lswitch_port.call_count)
            args, kwargs = self.nb_ovn.create_lswitch_port.call_args
            self.assertEqual('localport', kwargs['type'])

    def test_metadata_port_not_created_if_exists(self):
        """Check that metadata port is not created if it already exists.

        In the event of a sync, it might happen that a metadata port exists
        already. When we are creating the logical switch in OVN we don't want
        this port to be created again.
        """
        with mock.patch.object(
            self.mech_driver._ovn_client, '_get_metadata_ports',
                return_value=['metadata_port1']):
            with self.network():
                self.assertEqual(0, self.nb_ovn.create_lswitch_port.call_count)

    def test_metadata_ip_on_subnet_create(self):
        """Check metadata port update.

        Check that the metadata port is updated with a new IP address when a
        subnet is created.
        """
        with self.network() as net1:
            with self.subnet(network=net1, cidr='10.0.0.0/24'):
                self.assertEqual(1, self.nb_ovn.set_lswitch_port.call_count)
                args, kwargs = self.nb_ovn.set_lswitch_port.call_args
                self.assertEqual('localport', kwargs['type'])
                self.assertEqual('10.0.0.2/24',
                                 kwargs['external_ids'].get(
                                     ovn_const.OVN_CIDRS_EXT_ID_KEY, ''))

    def test_metadata_port_on_network_delete(self):
        """Check metadata port delete.

        Check that the metadata port is deleted when a network is deleted.
        """
        net = self._make_network(self.fmt, name="net1", admin_state_up=True)
        network_id = net['network']['id']
        req = self.new_delete_request('networks', network_id)
        res = req.get_response(self.api)
        self.assertEqual(exc.HTTPNoContent.code,
                         res.status_int)
        self.assertEqual(1, self.nb_ovn.delete_lswitch_port.call_count)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\ml2\test_qos_driver.py
===========File Type===========
.py
===========File Content===========
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mock
from neutron_lib import constants
from oslo_utils import uuidutils

from neutron.objects.qos import policy as qos_policy
from neutron.objects.qos import rule as qos_rule
from neutron.tests import base

from networking_ovn.common import utils
from networking_ovn.ml2 import qos_driver

context = 'context'


class TestOVNQosNotificationDriver(base.BaseTestCase):

    def setUp(self):
        super(TestOVNQosNotificationDriver, self).setUp()
        self.mech_driver = mock.Mock()
        self.mech_driver._ovn_client = mock.Mock()
        self.mech_driver._ovn_client._qos_driver = mock.Mock()
        self.driver = qos_driver.OVNQosNotificationDriver.create(
            self.mech_driver)
        self.policy = "policy"

    def test_create_policy(self):
        self.driver.create_policy(context, self.policy)
        self.driver._driver._ovn_client._qos_driver.create_policy.\
            assert_not_called()

    def test_update_policy(self):
        self.driver.update_policy(context, self.policy)
        self.driver._driver._ovn_client._qos_driver.update_policy.\
            assert_called_once_with(context, self.policy)

    def test_delete_policy(self):
        self.driver.delete_policy(context, self.policy)
        self.driver._driver._ovn_client._qos_driver.delete_policy.\
            assert_not_called()


class TestOVNQosDriver(base.BaseTestCase):

    def setUp(self):
        super(TestOVNQosDriver, self).setUp()
        self.plugin = mock.Mock()
        self.ovn_client = mock.Mock()
        self.driver = qos_driver.OVNQosDriver(self.ovn_client)
        self.driver._plugin_property = self.plugin
        self.port_id = uuidutils.generate_uuid()
        self.policy_id = uuidutils.generate_uuid()
        self.network_id = uuidutils.generate_uuid()
        self.network_policy_id = uuidutils.generate_uuid()
        self.policy = self._create_fake_policy()
        self.port = self._create_fake_port()
        self.rule = self._create_bw_limit_rule()
        self.expected = {'qos_max_rate': '1000', 'qos_burst': '100000'}

    def _create_bw_limit_rule(self):
        rule_obj = qos_rule.QosBandwidthLimitRule()
        rule_obj.id = uuidutils.generate_uuid()
        rule_obj.max_kbps = 1
        rule_obj.max_burst_kbps = 100
        rule_obj.obj_reset_changes()
        return rule_obj

    def _create_fake_policy(self):
        policy_dict = {'id': self.network_policy_id}
        policy_obj = qos_policy.QosPolicy(context, **policy_dict)
        policy_obj.obj_reset_changes()
        return policy_obj

    def _create_fake_port(self):
        return {'id': self.port_id,
                'qos_policy_id': self.policy_id,
                'network_id': self.network_id,
                'device_owner': 'compute:fake'}

    def _create_fake_network(self):
        return {'id': self.network_id,
                'qos_policy_id': self.network_policy_id}

    def test__is_network_device_port(self):
        self.assertFalse(utils.is_network_device_port(self.port))
        port = self._create_fake_port()
        port['device_owner'] = constants.DEVICE_OWNER_DHCP
        self.assertTrue(utils.is_network_device_port(port))
        port['device_owner'] = 'neutron:LOADBALANCERV2'
        self.assertTrue(utils.is_network_device_port(port))

    def _generate_port_options(self, policy_id, return_val, expected_result):
        with mock.patch.object(qos_rule, 'get_rules',
                               return_value=return_val) as get_rules:
            options = self.driver._generate_port_options(context, policy_id)
            if policy_id:
                get_rules.assert_called_once_with(qos_policy.QosPolicy,
                                                  context, policy_id)
            else:
                get_rules.assert_not_called()
            self.assertEqual(expected_result, options)

    def test__generate_port_options_no_policy_id(self):
        self._generate_port_options(None, [], {})

    def test__generate_port_options_no_rules(self):
        self._generate_port_options(self.policy_id, [], {})

    def test__generate_port_options_with_rule(self):
        self._generate_port_options(self.policy_id, [self.rule], self.expected)

    def _get_qos_options(self, port, port_policy, network_policy):
        with mock.patch.object(qos_policy.QosPolicy, 'get_network_policy',
                               return_value=self.policy) as get_network_policy:
            with mock.patch.object(self.driver, '_generate_port_options',
                                   return_value={}) as generate_port_options:
                options = self.driver.get_qos_options(port)
                if network_policy:
                    get_network_policy.\
                        assert_called_once_with(context, self.network_id)
                    generate_port_options. \
                        assert_called_once_with(context,
                                                self.network_policy_id)
                elif port_policy:
                    get_network_policy.assert_not_called()
                    generate_port_options.\
                        assert_called_once_with(context, self.policy_id)
                else:
                    get_network_policy.assert_not_called()
                    generate_port_options.assert_not_called()
                self.assertEqual({}, options)

    def test_get_qos_options_no_qos(self):
        port = self._create_fake_port()
        port.pop('qos_policy_id')
        self._get_qos_options(port, False, False)

    def test_get_qos_options_network_port(self):
        port = self._create_fake_port()
        port['device_owner'] = constants.DEVICE_OWNER_DHCP
        self._get_qos_options(port, False, False)

    @mock.patch('neutron_lib.context.get_admin_context', return_value=context)
    def test_get_qos_options_port_policy(self, *mocks):
        self._get_qos_options(self.port, True, False)

    @mock.patch('neutron_lib.context.get_admin_context', return_value=context)
    def test_get_qos_options_network_policy(self, *mocks):
        port = self._create_fake_port()
        port['qos_policy_id'] = None
        self._get_qos_options(port, False, True)

    def _update_network_ports(self, port, called):
        with mock.patch.object(self.plugin, 'get_ports',
                               return_value=[port]) as get_ports:
            with mock.patch.object(self.ovn_client,
                                   'update_port') as update_port:
                self.driver._update_network_ports(
                    context, self.network_id, {})
                get_ports.assert_called_once_with(
                    context, filters={'network_id': [self.network_id]})
                if called:
                    update_port.assert_called()
                else:
                    update_port.assert_not_called()

    def test__update_network_ports_port_policy(self):
        self._update_network_ports(self.port, False)

    def test__update_network_ports_network_device(self):
        port = self._create_fake_port()
        port['device_owner'] = constants.DEVICE_OWNER_DHCP
        self._update_network_ports(port, False)

    def test__update_network_ports(self):
        port = self._create_fake_port()
        port['qos_policy_id'] = None
        self._update_network_ports(port, True)

    def _update_network(self, network, called):
        with mock.patch.object(self.driver, '_generate_port_options',
                               return_value={}) as generate_port_options:
            with mock.patch.object(self.driver, '_update_network_ports'
                                   ) as update_network_ports:
                self.driver.update_network(network)
                if called:
                    generate_port_options.assert_called_once_with(
                        context, self.network_policy_id)
                    update_network_ports.assert_called_once_with(
                        context, self.network_id, {})
                else:
                    generate_port_options.assert_not_called()
                    update_network_ports.assert_not_called()

    @mock.patch('neutron_lib.context.get_admin_context', return_value=context)
    def test_update_network_no_qos(self, *mocks):
        network = self._create_fake_network()
        network.pop('qos_policy_id')
        self._update_network(network, False)

    @mock.patch('neutron_lib.context.get_admin_context', return_value=context)
    def test_update_network_policy_change(self, *mocks):
        network = self._create_fake_network()
        self._update_network(network, True)

    def test_update_policy(self):
        with mock.patch.object(self.driver, '_generate_port_options',
                               return_value={}) as generate_port_options, \
            mock.patch.object(self.policy, 'get_bound_networks',
                              return_value=[self.network_id]
                              ) as get_bound_networks, \
            mock.patch.object(self.driver, '_update_network_ports'
                              ) as update_network_ports, \
            mock.patch.object(self.policy, 'get_bound_ports',
                              return_value=[self.port_id]
                              ) as get_bound_ports, \
            mock.patch.object(self.plugin, 'get_port',
                              return_value=self.port) as get_port, \
            mock.patch.object(self.ovn_client, 'update_port',
                              ) as update_port:

            self.driver.update_policy(context, self.policy)

            generate_port_options.assert_called_once_with(
                context, self.network_policy_id)
            get_bound_networks.assert_called_once()
            update_network_ports.assert_called_once_with(
                context, self.network_id, {})
            get_bound_ports.assert_called_once()
            get_port.assert_called_once_with(context, self.port_id)
            update_port.assert_called_once_with(self.port, qos_options={})




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\ml2\test_trunk_driver.py
===========File Type===========
.py
===========File Content===========
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import mock
from neutron_lib.callbacks import events
from neutron_lib.callbacks import registry

from networking_ovn.common.constants import OVN_ML2_MECH_DRIVER_NAME
from networking_ovn.ml2 import trunk_driver
from networking_ovn.tests.unit import fakes

from neutron.services.trunk import constants as trunk_consts
from neutron.tests import base

from oslo_config import cfg


class TestTrunkHandler(base.BaseTestCase):
    def setUp(self):
        super(TestTrunkHandler, self).setUp()
        self.context = mock.Mock()
        self.plugin_driver = mock.Mock()
        self.plugin_driver._plugin = mock.Mock()
        self.plugin_driver._plugin.update_port = mock.Mock()
        self.plugin_driver._nb_ovn = fakes.FakeOvsdbNbOvnIdl()
        self.handler = trunk_driver.OVNTrunkHandler(self.plugin_driver)
        self.trunk_1 = mock.Mock()
        self.trunk_1.port_id = "parent_port_1"

        self.trunk_2 = mock.Mock()
        self.trunk_2.port_id = "parent_port_2"

        self.sub_port_1 = mock.Mock()
        self.sub_port_1.segmentation_id = 40
        self.sub_port_1.trunk_id = "trunk-1"
        self.sub_port_1.port_id = "sub_port_1"

        self.sub_port_2 = mock.Mock()
        self.sub_port_2.segmentation_id = 41
        self.sub_port_2.trunk_id = "trunk-1"
        self.sub_port_2.port_id = "sub_port_2"

        self.sub_port_3 = mock.Mock()
        self.sub_port_3.segmentation_id = 42
        self.sub_port_3.trunk_id = "trunk-2"
        self.sub_port_3.port_id = "sub_port_3"

        self.sub_port_4 = mock.Mock()
        self.sub_port_4.segmentation_id = 43
        self.sub_port_4.trunk_id = "trunk-2"
        self.sub_port_4.port_id = "sub_port_4"

        self.get_trunk_object = mock.patch(
            "neutron.objects.trunk.Trunk.get_object").start()
        self.get_trunk_object.side_effect = lambda ctxt, id: \
            self.trunk_1 if id == 'trunk-1' else self.trunk_2

    def _get_binding_profile_info(self, parent_name=None, tag=None):
        binding_profile = {}
        if parent_name and tag:
            binding_profile = {'parent_name': parent_name, 'tag': tag}
        info = {'port': {'binding:profile': binding_profile}}
        if not tag:
            info['port']['binding:host_id'] = None
        return info

    def _assert_update_port_calls(self, calls):
        self.assertEqual(len(calls),
                         self.plugin_driver._plugin.update_port.call_count)
        self.plugin_driver._plugin.update_port.assert_has_calls(
            calls, any_order=True)

    def test_create_trunk(self):
        self.trunk_1.sub_ports = []
        self.handler.trunk_created(self.trunk_1)
        self.plugin_driver._plugin.update_port.assert_not_called()

        self.trunk_1.sub_ports = [self.sub_port_1, self.sub_port_2]
        self.handler.trunk_created(self.trunk_1)

        calls = [mock.call(mock.ANY, s_port.port_id,
                           self._get_binding_profile_info(
                               trunk.port_id, s_port.segmentation_id))
                 for trunk, s_port in [(self.trunk_1, self.sub_port_1),
                                       (self.trunk_1, self.sub_port_2)]]
        self._assert_update_port_calls(calls)

    def test_delete_trunk(self):
        self.trunk_1.sub_ports = []
        self.handler.trunk_deleted(self.trunk_1)
        self.plugin_driver._plugin.update_port.assert_not_called()

        self.trunk_1.sub_ports = [self.sub_port_1, self.sub_port_2]
        self.handler.trunk_deleted(self.trunk_1)

        calls = [mock.call(mock.ANY, s_port.port_id,
                           self._get_binding_profile_info(
                               trunk.port_id, None))
                 for trunk, s_port in [(self.trunk_1, self.sub_port_1),
                                       (self.trunk_1, self.sub_port_2)]]
        self._assert_update_port_calls(calls)

    def test_subports_added(self):
        self.handler.subports_added(self.trunk_1,
                                    [self.sub_port_1, self.sub_port_2])
        self.handler.subports_added(self.trunk_2,
                                    [self.sub_port_3, self.sub_port_4])

        calls = [mock.call(mock.ANY, s_port.port_id,
                           self._get_binding_profile_info(
                               trunk.port_id, s_port.segmentation_id))
                 for trunk, s_port in [(self.trunk_1, self.sub_port_1),
                                       (self.trunk_1, self.sub_port_2),
                                       (self.trunk_2, self.sub_port_3),
                                       (self.trunk_2, self.sub_port_4)]]
        self._assert_update_port_calls(calls)

    def test_subports_deleted(self):
        self.handler.subports_deleted(self.trunk_1,
                                      [self.sub_port_1, self.sub_port_2])
        self.handler.subports_deleted(self.trunk_2,
                                      [self.sub_port_3, self.sub_port_4])
        calls = [mock.call(mock.ANY, s_port.port_id,
                           self._get_binding_profile_info(
                               trunk.port_id, None))
                 for trunk, s_port in [(self.trunk_1, self.sub_port_1),
                                       (self.trunk_1, self.sub_port_2),
                                       (self.trunk_2, self.sub_port_3),
                                       (self.trunk_2, self.sub_port_4)]]
        self._assert_update_port_calls(calls)

    def _fake_trunk_event_payload(self):
        payload = mock.Mock()
        payload.current_trunk = mock.Mock()
        payload.current_trunk.port_id = 'current_trunk_port_id'
        payload.original_trunk = mock.Mock()
        payload.original_trunk.port_id = 'original_trunk_port_id'
        current_subport = mock.Mock()
        current_subport.segmentation_id = 40
        current_subport.trunk_id = 'current_trunk_port_id'
        current_subport.port_id = 'current_subport_port_id'
        original_subport = mock.Mock()
        original_subport.segmentation_id = 41
        original_subport.trunk_id = 'original_trunk_port_id'
        original_subport.port_id = 'original_subport_port_id'
        payload.current_trunk.sub_ports = [current_subport]
        payload.original_trunk.sub_ports = [original_subport]
        return payload

    def test_trunk_event_create(self):
        fake_payload = self._fake_trunk_event_payload()
        self.handler.trunk_event(
            mock.ANY, events.AFTER_CREATE, mock.ANY, fake_payload)

        self.plugin_driver._plugin.update_port.assert_called_once_with(
            mock.ANY, fake_payload.current_trunk.sub_ports[0].port_id,
            self._get_binding_profile_info(
                fake_payload.current_trunk.port_id,
                fake_payload.current_trunk.sub_ports[0].segmentation_id))

    def test_trunk_event_delete(self):
        fake_payload = self._fake_trunk_event_payload()
        self.handler.trunk_event(
            mock.ANY, events.AFTER_DELETE, mock.ANY, fake_payload)

        self.plugin_driver._plugin.update_port.assert_called_once_with(
            mock.ANY, fake_payload.original_trunk.sub_ports[0].port_id,
            self._get_binding_profile_info(
                fake_payload.original_trunk.port_id, None))

    def test_trunk_event_invalid(self):
        fake_payload = self._fake_trunk_event_payload()
        self.handler.trunk_event(
            mock.ANY, events.BEFORE_DELETE, mock.ANY, fake_payload)

        self.plugin_driver._plugin.update_port.assert_not_called()

    def _fake_subport_event_payload(self):
        payload = mock.Mock()
        payload.original_trunk = mock.Mock()
        payload.original_trunk.port_id = 'original_trunk_port_id'
        original_subport = mock.Mock()
        original_subport.segmentation_id = 41
        original_subport.trunk_id = 'original_trunk_port_id'
        original_subport.port_id = 'original_subport_port_id'
        payload.subports = [original_subport]
        return payload

    def test_subport_event_create(self):
        fake_payload = self._fake_subport_event_payload()
        self.handler.subport_event(
            mock.ANY, events.AFTER_CREATE, mock.ANY, fake_payload)

        self.plugin_driver._plugin.update_port.assert_called_once_with(
            mock.ANY, fake_payload.subports[0].port_id,
            self._get_binding_profile_info(
                fake_payload.original_trunk.port_id,
                fake_payload.subports[0].segmentation_id))

    def test_subport_event_delete(self):
        fake_payload = self._fake_subport_event_payload()
        self.handler.subport_event(
            mock.ANY, events.AFTER_DELETE, mock.ANY, fake_payload)

        self.plugin_driver._plugin.update_port.assert_called_once_with(
            mock.ANY, fake_payload.subports[0].port_id,
            self._get_binding_profile_info(
                fake_payload.original_trunk.port_id, None))

    def test_subport_event_invalid(self):
        fake_payload = self._fake_trunk_event_payload()
        self.handler.subport_event(
            mock.ANY, events.BEFORE_DELETE, mock.ANY, fake_payload)

        self.plugin_driver._plugin.update_port.assert_not_called()


class TestTrunkDriver(base.BaseTestCase):
    def setUp(self):
        super(TestTrunkDriver, self).setUp()

    def test_is_loaded(self):
        driver = trunk_driver.OVNTrunkDriver.create(mock.Mock())
        cfg.CONF.set_override('mechanism_drivers',
                              ["logger", OVN_ML2_MECH_DRIVER_NAME],
                              group='ml2')
        self.assertTrue(driver.is_loaded)

        cfg.CONF.set_override('mechanism_drivers',
                              ['ovs', 'logger'],
                              group='ml2')
        self.assertFalse(driver.is_loaded)

        cfg.CONF.set_override('core_plugin', 'some_plugin')
        self.assertFalse(driver.is_loaded)

    def test_register(self):
        driver = trunk_driver.OVNTrunkDriver.create(mock.Mock())
        with mock.patch.object(registry, 'subscribe') as mock_subscribe:
            driver.register(mock.ANY, mock.ANY, mock.Mock())
            calls = [mock.call.mock_subscribe(mock.ANY,
                                              trunk_consts.TRUNK,
                                              events.AFTER_CREATE),
                     mock.call.mock_subscribe(mock.ANY,
                                              trunk_consts.SUBPORTS,
                                              events.AFTER_CREATE),
                     mock.call.mock_subscribe(mock.ANY,
                                              trunk_consts.TRUNK,
                                              events.AFTER_DELETE),
                     mock.call.mock_subscribe(mock.ANY,
                                              trunk_consts.SUBPORTS,
                                              events.AFTER_DELETE)]
            mock_subscribe.assert_has_calls(calls, any_order=True)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\ml2\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\ovsdb\test_commands.py
===========File Type===========
.py
===========File Content===========
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import mock

from ovsdbapp.backend.ovs_idl import idlutils

from networking_ovn.common import acl as ovn_acl
from networking_ovn.common import constants as ovn_const
from networking_ovn.common import utils as ovn_utils
from networking_ovn.ovsdb import commands
from networking_ovn.tests import base
from networking_ovn.tests.unit import fakes


class TestBaseCommandHelpers(base.TestCase):
    def setUp(self):
        super(TestBaseCommandHelpers, self).setUp()
        self.column = 'ovn'
        self.new_value = '1'
        self.old_value = '2'

    def _get_fake_row_mutate(self):
        return fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={self.column: []})

    def test__addvalue_to_list(self):
        fake_row_mutate = self._get_fake_row_mutate()
        commands._addvalue_to_list(
            fake_row_mutate, self.column, self.new_value)
        fake_row_mutate.addvalue.assert_called_once_with(
            self.column, self.new_value)
        fake_row_mutate.verify.assert_not_called()

    def test__delvalue_from_list(self):
        fake_row_mutate = self._get_fake_row_mutate()
        commands._delvalue_from_list(
            fake_row_mutate, self.column, self.old_value)
        fake_row_mutate.delvalue.assert_called_once_with(
            self.column, self.old_value)
        fake_row_mutate.verify.assert_not_called()

    def test__updatevalues_in_list_none(self):
        fake_row_mutate = self._get_fake_row_mutate()
        commands._updatevalues_in_list(fake_row_mutate, self.column)
        fake_row_mutate.addvalue.assert_not_called()
        fake_row_mutate.delvalue.assert_not_called()
        fake_row_mutate.verify.assert_not_called()

    def test__updatevalues_in_list_empty(self):
        fake_row_mutate = self._get_fake_row_mutate()
        commands._updatevalues_in_list(fake_row_mutate, self.column, [], [])
        fake_row_mutate.addvalue.assert_not_called()
        fake_row_mutate.delvalue.assert_not_called()
        fake_row_mutate.verify.assert_not_called()

    def test__updatevalues_in_list(self):
        fake_row_mutate = self._get_fake_row_mutate()
        commands._updatevalues_in_list(
            fake_row_mutate, self.column,
            new_values=[self.new_value],
            old_values=[self.old_value])
        fake_row_mutate.addvalue.assert_called_once_with(
            self.column, self.new_value)
        fake_row_mutate.delvalue.assert_called_once_with(
            self.column, self.old_value)
        fake_row_mutate.verify.assert_not_called()


class TestBaseCommand(base.TestCase):
    def setUp(self):
        super(TestBaseCommand, self).setUp()
        self.ovn_api = fakes.FakeOvsdbNbOvnIdl()
        self.transaction = fakes.FakeOvsdbTransaction()
        self.ovn_api.transaction = self.transaction


class TestLSwitchSetExternalIdsCommand(TestBaseCommand):

    def _test_lswitch_extid_update_no_exist(self, if_exists=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.LSwitchSetExternalIdsCommand(
                self.ovn_api, 'fake-lswitch',
                {ovn_const.OVN_NETWORK_NAME_EXT_ID_KEY: 'neutron-network'},
                if_exists=if_exists)
            if if_exists:
                cmd.run_idl(self.transaction)
            else:
                self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_lswitch_no_exist_ignore(self):
        self._test_lswitch_extid_update_no_exist(if_exists=True)

    def test_lswitch_no_exist_fail(self):
        self._test_lswitch_extid_update_no_exist(if_exists=False)

    def test_lswitch_extid_update(self):
        network_name = 'private'
        new_network_name = 'private-new'
        ext_ids = {ovn_const.OVN_NETWORK_NAME_EXT_ID_KEY: network_name}
        new_ext_ids = {ovn_const.OVN_NETWORK_NAME_EXT_ID_KEY: new_network_name}
        fake_lswitch = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'external_ids': ext_ids})
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lswitch):
            cmd = commands.LSwitchSetExternalIdsCommand(
                self.ovn_api, fake_lswitch.name,
                {ovn_const.OVN_NETWORK_NAME_EXT_ID_KEY: new_network_name},
                if_exists=True)
            cmd.run_idl(self.transaction)
            self.assertEqual(new_ext_ids, fake_lswitch.external_ids)


class TestAddLSwitchPortCommand(TestBaseCommand):

    def test_lswitch_not_found(self):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.AddLSwitchPortCommand(
                self.ovn_api, 'fake-lsp', 'fake-lswitch', may_exist=True)
            self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)
            self.transaction.insert.assert_not_called()

    def test_lswitch_port_exists(self):
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=mock.ANY):
            cmd = commands.AddLSwitchPortCommand(
                self.ovn_api, 'fake-lsp', 'fake-lswitch', may_exist=True)
            cmd.run_idl(self.transaction)
            self.transaction.insert.assert_not_called()

    def test_lswitch_port_add_exists(self):
        fake_lswitch = fakes.FakeOvsdbRow.create_one_ovsdb_row()
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lswitch):
            fake_lsp = fakes.FakeOvsdbRow.create_one_ovsdb_row()
            self.ovn_api._tables['Logical_Switch_Port'].rows[fake_lsp.uuid] = \
                fake_lsp
            self.transaction.insert.return_value = fake_lsp
            cmd = commands.AddLSwitchPortCommand(
                self.ovn_api, fake_lsp.name, fake_lswitch.name,
                may_exist=False)
            cmd.run_idl(self.transaction)
            # NOTE(rtheis): Mocking the transaction allows this insert
            # to succeed when it normally would fail due the duplicate name.
            self.transaction.insert.assert_called_once_with(
                self.ovn_api._tables['Logical_Switch_Port'])

    def _test_lswitch_port_add(self, may_exist=True):
        lsp_name = 'fake-lsp'
        fake_lswitch = fakes.FakeOvsdbRow.create_one_ovsdb_row()
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=[fake_lswitch, None]):
            fake_lsp = fakes.FakeOvsdbRow.create_one_ovsdb_row(
                attrs={'foo': None})
            self.transaction.insert.return_value = fake_lsp
            cmd = commands.AddLSwitchPortCommand(
                self.ovn_api, lsp_name, fake_lswitch.name,
                may_exist=may_exist, foo='bar')
            cmd.run_idl(self.transaction)
            self.transaction.insert.assert_called_once_with(
                self.ovn_api._tables['Logical_Switch_Port'])
            fake_lswitch.addvalue.assert_called_once_with(
                'ports', fake_lsp.uuid)
            self.assertEqual(lsp_name, fake_lsp.name)
            self.assertEqual('bar', fake_lsp.foo)

    def test_lswitch_port_add_may_exist(self):
        self._test_lswitch_port_add(may_exist=True)

    def test_lswitch_port_add_ignore_exists(self):
        self._test_lswitch_port_add(may_exist=False)

    def _test_lswitch_port_add_with_dhcp(self, dhcpv4_opts, dhcpv6_opts):
        lsp_name = 'fake-lsp'
        fake_lswitch = fakes.FakeOvsdbRow.create_one_ovsdb_row()
        fake_lsp = fakes.FakeOvsdbRow.create_one_ovsdb_row()
        self.transaction.insert.return_value = fake_lsp
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=[fake_lswitch, None]):
            cmd = commands.AddLSwitchPortCommand(
                self.ovn_api, lsp_name, fake_lswitch.name,
                may_exist=True, dhcpv4_options=dhcpv4_opts,
                dhcpv6_options=dhcpv6_opts)
            if not isinstance(dhcpv4_opts, list):
                dhcpv4_opts.result = 'fake-uuid-1'
            if not isinstance(dhcpv6_opts, list):
                dhcpv6_opts.result = 'fake-uuid-2'
            self.transaction.insert.reset_mock()
            cmd.run_idl(self.transaction)
            self.transaction.insert.assert_called_once_with(
                self.ovn_api.lsp_table)
            fake_lswitch.addvalue.assert_called_once_with(
                'ports', fake_lsp.uuid)
            self.assertEqual(lsp_name, fake_lsp.name)
            if isinstance(dhcpv4_opts, list):
                self.assertEqual(dhcpv4_opts, fake_lsp.dhcpv4_options)
            else:
                self.assertEqual(['fake-uuid-1'], fake_lsp.dhcpv4_options)
            if isinstance(dhcpv6_opts, list):
                self.assertEqual(dhcpv6_opts, fake_lsp.dhcpv6_options)
            else:
                self.assertEqual(['fake-uuid-2'], fake_lsp.dhcpv6_options)

    def test_lswitch_port_add_with_dhcp(self):
        dhcpv4_opts_cmd = commands.AddDHCPOptionsCommand(
            self.ovn_api, mock.ANY, port_id=mock.ANY)
        dhcpv6_opts_cmd = commands.AddDHCPOptionsCommand(
            self.ovn_api, mock.ANY, port_id=mock.ANY)
        for dhcpv4_opts in ([], ['fake-uuid-1'], dhcpv4_opts_cmd):
            for dhcpv6_opts in ([], ['fake-uuid-2'], dhcpv6_opts_cmd):
                self._test_lswitch_port_add_with_dhcp(dhcpv4_opts, dhcpv6_opts)


class TestSetLSwitchPortCommand(TestBaseCommand):

    def _test_lswitch_port_update_no_exist(self, if_exists=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.SetLSwitchPortCommand(
                self.ovn_api, 'fake-lsp', if_exists=if_exists)
            if if_exists:
                cmd.run_idl(self.transaction)
            else:
                self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_lswitch_port_no_exist_ignore(self):
        self._test_lswitch_port_update_no_exist(if_exists=True)

    def test_lswitch_port_no_exist_fail(self):
        self._test_lswitch_port_update_no_exist(if_exists=False)

    def test_lswitch_port_update(self):
        ext_ids = {ovn_const.OVN_PORT_NAME_EXT_ID_KEY: 'test'}
        new_ext_ids = {ovn_const.OVN_PORT_NAME_EXT_ID_KEY: 'test-new'}
        fake_lsp = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'external_ids': ext_ids})
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lsp):
            cmd = commands.SetLSwitchPortCommand(
                self.ovn_api, fake_lsp.name, if_exists=True,
                external_ids=new_ext_ids)
            cmd.run_idl(self.transaction)
            self.assertEqual(new_ext_ids, fake_lsp.external_ids)

    def _test_lswitch_port_update_del_dhcp(self, clear_v4_opts,
                                           clear_v6_opts, set_v4_opts=False,
                                           set_v6_opts=False):
        ext_ids = {ovn_const.OVN_PORT_NAME_EXT_ID_KEY: 'test'}
        dhcp_options_tbl = self.ovn_api._tables['DHCP_Options']
        fake_dhcpv4_opts = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'external_ids': {'port_id': 'fake-lsp'}})
        dhcp_options_tbl.rows[fake_dhcpv4_opts.uuid] = fake_dhcpv4_opts
        fake_dhcpv6_opts = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'external_ids': {'port_id': 'fake-lsp'}})
        dhcp_options_tbl.rows[fake_dhcpv6_opts.uuid] = fake_dhcpv6_opts
        fake_lsp = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'name': 'fake-lsp',
                   'external_ids': ext_ids,
                   'dhcpv4_options': [fake_dhcpv4_opts],
                   'dhcpv6_options': [fake_dhcpv6_opts]})

        columns = {}
        if clear_v4_opts:
            columns['dhcpv4_options'] = []
        elif set_v4_opts:
            columns['dhcpv4_options'] = [fake_dhcpv4_opts.uuid]
        if clear_v6_opts:
            columns['dhcpv6_options'] = []
        elif set_v6_opts:
            columns['dhcpv6_options'] = [fake_dhcpv6_opts.uuid]

        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lsp):
            cmd = commands.SetLSwitchPortCommand(
                self.ovn_api, fake_lsp.name, if_exists=True, **columns)
            cmd.run_idl(self.transaction)

            if clear_v4_opts and clear_v6_opts:
                fake_dhcpv4_opts.delete.assert_called_once_with()
                fake_dhcpv6_opts.delete.assert_called_once_with()
            elif clear_v4_opts:
                # not clear_v6_opts and set_v6_opts is any
                fake_dhcpv4_opts.delete.assert_called_once_with()
                fake_dhcpv6_opts.delete.assert_not_called()
            elif clear_v6_opts:
                # not clear_v4_opts and set_v6_opts is any
                fake_dhcpv4_opts.delete.assert_not_called()
                fake_dhcpv6_opts.delete.assert_called_once_with()
            else:
                # not clear_v4_opts and not clear_v6_opts and
                # set_v4_opts is any and set_v6_opts is any
                fake_dhcpv4_opts.delete.assert_not_called()
                fake_dhcpv6_opts.delete.assert_not_called()

    def test_lswitch_port_update_del_port_dhcpv4_options(self):
        self._test_lswitch_port_update_del_dhcp(True, False)

    def test_lswitch_port_update_del_port_dhcpv6_options(self):
        self._test_lswitch_port_update_del_dhcp(False, True)

    def test_lswitch_port_update_del_all_port_dhcp_options(self):
        self._test_lswitch_port_update_del_dhcp(True, True)

    def test_lswitch_port_update_del_no_port_dhcp_options(self):
        self._test_lswitch_port_update_del_dhcp(False, False)

    def test_lswitch_port_update_set_port_dhcpv4_options(self):
        self._test_lswitch_port_update_del_dhcp(False, True, set_v4_opts=True)

    def test_lswitch_port_update_set_port_dhcpv6_options(self):
        self._test_lswitch_port_update_del_dhcp(True, False, set_v6_opts=True)

    def test_lswitch_port_update_set_all_port_dhcp_options(self):
        self._test_lswitch_port_update_del_dhcp(False, False, set_v4_opts=True,
                                                set_v6_opts=True)

    def _test_lswitch_port_update_with_dhcp(self, dhcpv4_opts, dhcpv6_opts):
        ext_ids = {ovn_const.OVN_PORT_NAME_EXT_ID_KEY: 'test'}
        fake_lsp = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'name': 'fake-lsp',
                   'external_ids': ext_ids,
                   'dhcpv4_options': ['fake-v4-subnet-dhcp-opt'],
                   'dhcpv6_options': ['fake-v6-subnet-dhcp-opt']})
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lsp):
            cmd = commands.SetLSwitchPortCommand(
                self.ovn_api, fake_lsp.name, if_exists=True,
                external_ids=ext_ids, dhcpv4_options=dhcpv4_opts,
                dhcpv6_options=dhcpv6_opts)
            if not isinstance(dhcpv4_opts, list):
                dhcpv4_opts.result = 'fake-uuid-1'
            if not isinstance(dhcpv6_opts, list):
                dhcpv6_opts.result = 'fake-uuid-2'
            cmd.run_idl(self.transaction)
            if isinstance(dhcpv4_opts, list):
                self.assertEqual(dhcpv4_opts, fake_lsp.dhcpv4_options)
            else:
                self.assertEqual(['fake-uuid-1'], fake_lsp.dhcpv4_options)
            if isinstance(dhcpv6_opts, list):
                self.assertEqual(dhcpv6_opts, fake_lsp.dhcpv6_options)
            else:
                self.assertEqual(['fake-uuid-2'], fake_lsp.dhcpv6_options)

    def test_lswitch_port_update_with_dhcp(self):
        v4_dhcp_cmd = commands.AddDHCPOptionsCommand(self.ovn_api, mock.ANY,
                                                     port_id=mock.ANY)
        v6_dhcp_cmd = commands.AddDHCPOptionsCommand(self.ovn_api, mock.ANY,
                                                     port_id=mock.ANY)
        for dhcpv4_opts in ([], ['fake-v4-subnet-dhcp-opt'], v4_dhcp_cmd):
            for dhcpv6_opts in ([], ['fake-v6-subnet-dhcp-opt'], v6_dhcp_cmd):
                self._test_lswitch_port_update_with_dhcp(
                    dhcpv4_opts, dhcpv6_opts)


class TestDelLSwitchPortCommand(TestBaseCommand):

    def _test_lswitch_no_exist(self, if_exists=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=['fake-lsp', idlutils.RowNotFound]):
            cmd = commands.DelLSwitchPortCommand(
                self.ovn_api, 'fake-lsp', 'fake-lswitch', if_exists=if_exists)
            if if_exists:
                cmd.run_idl(self.transaction)
            else:
                self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_lswitch_no_exist_ignore(self):
        self._test_lswitch_no_exist(if_exists=True)

    def test_lswitch_no_exist_fail(self):
        self._test_lswitch_no_exist(if_exists=False)

    def _test_lswitch_port_del_no_exist(self, if_exists=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.DelLSwitchPortCommand(
                self.ovn_api, 'fake-lsp', 'fake-lswitch', if_exists=if_exists)
            if if_exists:
                cmd.run_idl(self.transaction)
            else:
                self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_lswitch_port_no_exist_ignore(self):
        self._test_lswitch_port_del_no_exist(if_exists=True)

    def test_lswitch_port_no_exist_fail(self):
        self._test_lswitch_port_del_no_exist(if_exists=False)

    def test_lswitch_port_del(self):
        fake_lsp = mock.MagicMock()
        fake_lswitch = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'ports': [fake_lsp]})
        self.ovn_api._tables['Logical_Switch_Port'].rows[fake_lsp.uuid] = \
            fake_lsp
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=[fake_lsp, fake_lswitch]):
            cmd = commands.DelLSwitchPortCommand(
                self.ovn_api, fake_lsp.name, fake_lswitch.name, if_exists=True)
            cmd.run_idl(self.transaction)
            fake_lswitch.delvalue.assert_called_once_with('ports', fake_lsp)
            fake_lsp.delete.assert_called_once_with()

    def _test_lswitch_port_del_delete_dhcp_opt(self, dhcpv4_opt_ext_ids,
                                               dhcpv6_opt_ext_ids):
        ext_ids = {ovn_const.OVN_PORT_NAME_EXT_ID_KEY: 'test'}
        fake_dhcpv4_options = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'external_ids': dhcpv4_opt_ext_ids})
        self.ovn_api._tables['DHCP_Options'].rows[fake_dhcpv4_options.uuid] = \
            fake_dhcpv4_options
        fake_dhcpv6_options = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'external_ids': dhcpv6_opt_ext_ids})
        self.ovn_api._tables['DHCP_Options'].rows[fake_dhcpv6_options.uuid] = \
            fake_dhcpv6_options
        fake_lsp = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'name': 'lsp',
                   'external_ids': ext_ids,
                   'dhcpv4_options': [fake_dhcpv4_options],
                   'dhcpv6_options': [fake_dhcpv6_options]})
        self.ovn_api._tables['Logical_Switch_Port'].rows[fake_lsp.uuid] = \
            fake_lsp
        fake_lswitch = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'ports': [fake_lsp]})
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=[fake_lsp, fake_lswitch]):
            cmd = commands.DelLSwitchPortCommand(
                self.ovn_api, fake_lsp.name, fake_lswitch.name, if_exists=True)
            cmd.run_idl(self.transaction)
            fake_lswitch.delvalue.assert_called_once_with('ports', fake_lsp)
            fake_lsp.delete.assert_called_once_with()
            if 'port_id' in dhcpv4_opt_ext_ids:
                fake_dhcpv4_options.delete.assert_called_once_with()
            else:
                fake_dhcpv4_options.delete.assert_not_called()
            if 'port_id' in dhcpv6_opt_ext_ids:
                fake_dhcpv6_options.delete.assert_called_once_with()
            else:
                fake_dhcpv6_options.delete.assert_not_called()

    def test_lswitch_port_del_delete_dhcp_opt(self):
        for v4_ext_ids in ({'subnet_id': 'fake-ls0'},
                           {'subnet_id': 'fake-ls0', 'port_id': 'lsp'}):
            for v6_ext_ids in ({'subnet_id': 'fake-ls1'},
                               {'subnet_id': 'fake-ls1', 'port_id': 'lsp'}):
                self._test_lswitch_port_del_delete_dhcp_opt(
                    v4_ext_ids, v6_ext_ids)


class TestAddLRouterCommand(TestBaseCommand):

    def test_lrouter_exists(self):
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=mock.ANY):
            cmd = commands.AddLRouterCommand(
                self.ovn_api, 'fake-lrouter', may_exist=True,
                a='1', b='2')
            cmd.run_idl(self.transaction)
            self.transaction.insert.assert_not_called()

    def test_lrouter_add_exists(self):
        fake_lrouter = fakes.FakeOvsdbRow.create_one_ovsdb_row()
        self.ovn_api._tables['Logical_Router'].rows[fake_lrouter.uuid] = \
            fake_lrouter
        self.transaction.insert.return_value = fake_lrouter
        cmd = commands.AddLRouterCommand(
            self.ovn_api, fake_lrouter.name, may_exist=False)
        cmd.run_idl(self.transaction)
        # NOTE(rtheis): Mocking the transaction allows this insert
        # to succeed when it normally would fail due the duplicate name.
        self.transaction.insert.assert_called_once_with(
            self.ovn_api._tables['Logical_Router'])

    def _test_lrouter_add(self, may_exist=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=None):
            fake_lrouter = fakes.FakeOvsdbRow.create_one_ovsdb_row()
            self.transaction.insert.return_value = fake_lrouter
            cmd = commands.AddLRouterCommand(
                self.ovn_api, 'fake-lrouter', may_exist=may_exist,
                a='1', b='2')
            cmd.run_idl(self.transaction)
            self.transaction.insert.assert_called_once_with(
                self.ovn_api._tables['Logical_Router'])
            self.assertEqual('fake-lrouter', fake_lrouter.name)
            self.assertEqual('1', fake_lrouter.a)
            self.assertEqual('2', fake_lrouter.b)

    def test_lrouter_add_may_exist(self):
        self._test_lrouter_add(may_exist=True)

    def test_lrouter_add_ignore_exists(self):
        self._test_lrouter_add(may_exist=False)


class TestUpdateLRouterCommand(TestBaseCommand):

    def _test_lrouter_update_no_exist(self, if_exists=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.UpdateLRouterCommand(
                self.ovn_api, 'fake-lrouter', if_exists=if_exists)
            if if_exists:
                cmd.run_idl(self.transaction)
            else:
                self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_lrouter_no_exist_ignore(self):
        self._test_lrouter_update_no_exist(if_exists=True)

    def test_lrouter_no_exist_fail(self):
        self._test_lrouter_update_no_exist(if_exists=False)

    def test_lrouter_update(self):
        ext_ids = {ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY: 'richard'}
        new_ext_ids = {ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY: 'richard-new'}
        fake_lrouter = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'external_ids': ext_ids})
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lrouter):
            cmd = commands.UpdateLRouterCommand(
                self.ovn_api, fake_lrouter.name, if_exists=True,
                external_ids=new_ext_ids)
            cmd.run_idl(self.transaction)
            self.assertEqual(new_ext_ids, fake_lrouter.external_ids)


class TestDelLRouterCommand(TestBaseCommand):

    def _test_lrouter_del_no_exist(self, if_exists=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.DelLRouterCommand(
                self.ovn_api, 'fake-lrouter', if_exists=if_exists)
            if if_exists:
                cmd.run_idl(self.transaction)
            else:
                self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_lrouter_no_exist_ignore(self):
        self._test_lrouter_del_no_exist(if_exists=True)

    def test_lrouter_no_exist_fail(self):
        self._test_lrouter_del_no_exist(if_exists=False)

    def test_lrouter_del(self):
        fake_lrouter = fakes.FakeOvsdbRow.create_one_ovsdb_row()
        self.ovn_api._tables['Logical_Router'].rows[fake_lrouter.uuid] = \
            fake_lrouter
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lrouter):
            cmd = commands.DelLRouterCommand(
                self.ovn_api, fake_lrouter.name, if_exists=True)
            cmd.run_idl(self.transaction)
            fake_lrouter.delete.assert_called_once_with()


class TestAddLRouterPortCommand(TestBaseCommand):

    def test_lrouter_not_found(self):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.AddLRouterPortCommand(
                self.ovn_api, 'fake-lrp', 'fake-lrouter', may_exist=False)
            self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)
            self.transaction.insert.assert_not_called()

    def test_lrouter_port_exists(self):
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=mock.ANY):
            cmd = commands.AddLRouterPortCommand(
                self.ovn_api, 'fake-lrp', 'fake-lrouter', may_exist=False)
            self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)
            self.transaction.insert.assert_not_called()

    def test_lrouter_port_may_exist(self):
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=mock.ANY):
            cmd = commands.AddLRouterPortCommand(
                self.ovn_api, 'fake-lrp', 'fake-lrouter', may_exist=True)
            cmd.run_idl(self.transaction)
            self.transaction.insert.assert_not_called()

    def test_lrouter_port_add(self):
        fake_lrouter = fakes.FakeOvsdbRow.create_one_ovsdb_row()
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=[fake_lrouter,
                                            idlutils.RowNotFound]):
            fake_lrp = fakes.FakeOvsdbRow.create_one_ovsdb_row(
                attrs={'foo': None})
            self.transaction.insert.return_value = fake_lrp
            cmd = commands.AddLRouterPortCommand(
                self.ovn_api, 'fake-lrp', fake_lrouter.name, may_exist=False,
                foo='bar')
            cmd.run_idl(self.transaction)
            self.transaction.insert.assert_called_once_with(
                self.ovn_api._tables['Logical_Router_Port'])
            self.assertEqual('fake-lrp', fake_lrp.name)
            fake_lrouter.addvalue.assert_called_once_with('ports', fake_lrp)
            self.assertEqual('bar', fake_lrp.foo)


class TestUpdateLRouterPortCommand(TestBaseCommand):

    def _test_lrouter_port_update_no_exist(self, if_exists=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.UpdateLRouterPortCommand(
                self.ovn_api, 'fake-lrp', if_exists=if_exists)
            if if_exists:
                cmd.run_idl(self.transaction)
            else:
                self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_lrouter_port_no_exist_ignore(self):
        self._test_lrouter_port_update_no_exist(if_exists=True)

    def test_lrouter_port_no_exist_fail(self):
        self._test_lrouter_port_update_no_exist(if_exists=False)

    def test_lrouter_port_update(self):
        old_networks = []
        new_networks = ['10.1.0.0/24']
        fake_lrp = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'networks': old_networks})
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lrp):
            cmd = commands.UpdateLRouterPortCommand(
                self.ovn_api, fake_lrp.name, if_exists=True,
                networks=new_networks)
            cmd.run_idl(self.transaction)
            self.assertEqual(new_networks, fake_lrp.networks)


class TestDelLRouterPortCommand(TestBaseCommand):

    def _test_lrouter_port_del_no_exist(self, if_exists=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.DelLRouterPortCommand(
                self.ovn_api, 'fake-lrp', 'fake-lrouter', if_exists=if_exists)
            if if_exists:
                cmd.run_idl(self.transaction)
            else:
                self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_lrouter_port_no_exist_ignore(self):
        self._test_lrouter_port_del_no_exist(if_exists=True)

    def test_lrouter_port_no_exist_fail(self):
        self._test_lrouter_port_del_no_exist(if_exists=False)

    def test_lrouter_no_exist(self):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=[mock.ANY, idlutils.RowNotFound]):
            cmd = commands.DelLRouterPortCommand(
                self.ovn_api, 'fake-lrp', 'fake-lrouter', if_exists=True)
            self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_lrouter_port_del(self):
        fake_lrp = mock.MagicMock()
        fake_lrouter = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'ports': [fake_lrp]})
        self.ovn_api._tables['Logical_Router_Port'].rows[fake_lrp.uuid] = \
            fake_lrp
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=[fake_lrp, fake_lrouter]):
            cmd = commands.DelLRouterPortCommand(
                self.ovn_api, fake_lrp.name, fake_lrouter.name, if_exists=True)
            cmd.run_idl(self.transaction)
            fake_lrouter.delvalue.assert_called_once_with('ports', fake_lrp)


class TestSetLRouterPortInLSwitchPortCommand(TestBaseCommand):

    def test_lswitch_port_no_exist_fail(self):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.SetLRouterPortInLSwitchPortCommand(
                self.ovn_api, 'fake-lsp', 'fake-lrp', False, False, 'router')
            self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_lswitch_port_no_exist_do_not_fail(self):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.SetLRouterPortInLSwitchPortCommand(
                self.ovn_api, 'fake-lsp', 'fake-lrp', False, True, 'router')
            cmd.run_idl(self.transaction)

    def test_lswitch_port_router_update(self):
        lrp_name = 'fake-lrp'
        fake_lsp = fakes.FakeOvsdbRow.create_one_ovsdb_row()
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lsp):
            cmd = commands.SetLRouterPortInLSwitchPortCommand(
                self.ovn_api, fake_lsp.name, lrp_name, True, True, 'router')
            cmd.run_idl(self.transaction)
            self.assertEqual({'router-port': lrp_name,
                             'nat-addresses': 'router'}, fake_lsp.options)
            self.assertEqual('router', fake_lsp.type)
            self.assertEqual('router', fake_lsp.addresses)


class TestAddACLCommand(TestBaseCommand):

    def test_lswitch_no_exist(self, if_exists=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.AddACLCommand(
                self.ovn_api, 'fake-lswitch', 'fake-lsp')
            self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_acl_add(self):
        fake_lswitch = fakes.FakeOvsdbRow.create_one_ovsdb_row()
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lswitch):
            fake_acl = fakes.FakeOvsdbRow.create_one_ovsdb_row()
            self.transaction.insert.return_value = fake_acl
            cmd = commands.AddACLCommand(
                self.ovn_api, fake_lswitch.name, 'fake-lsp', match='*')
            cmd.run_idl(self.transaction)
            self.transaction.insert.assert_called_once_with(
                self.ovn_api._tables['ACL'])
            fake_lswitch.addvalue.assert_called_once_with(
                'acls', fake_acl.uuid)
            self.assertEqual('*', fake_acl.match)


class TestDelACLCommand(TestBaseCommand):

    def _test_lswitch_no_exist(self, if_exists=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.DelACLCommand(
                self.ovn_api, 'fake-lswitch', 'fake-lsp',
                if_exists=if_exists)
            if if_exists:
                cmd.run_idl(self.transaction)
            else:
                self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_lswitch_no_exist_ignore(self):
        self._test_lswitch_no_exist(if_exists=True)

    def test_lswitch_no_exist_fail(self):
        self._test_lswitch_no_exist(if_exists=False)

    def test_acl_del(self):
        fake_lsp_name = 'fake-lsp'
        fake_acl_del = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'external_ids': {'neutron:lport': fake_lsp_name}})
        fake_acl_save = mock.ANY
        fake_lswitch = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'acls': [fake_acl_del, fake_acl_save]})
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lswitch):
            cmd = commands.DelACLCommand(
                self.ovn_api, fake_lswitch.name, fake_lsp_name,
                if_exists=True)
            cmd.run_idl(self.transaction)
            fake_lswitch.delvalue.assert_called_once_with('acls', mock.ANY)


class TestUpdateACLsCommand(TestBaseCommand):

    def test_lswitch_no_exist(self):
        fake_lswitch = fakes.FakeOvsdbRow.create_one_ovsdb_row()
        self.ovn_api.get_acls_for_lswitches.return_value = ({}, {}, {})
        cmd = commands.UpdateACLsCommand(
            self.ovn_api, [fake_lswitch.name], port_list=[],
            acl_new_values_dict={},
            need_compare=True)
        cmd.run_idl(self.transaction)
        self.transaction.insert.assert_not_called()
        fake_lswitch.addvalue.assert_not_called()
        fake_lswitch.delvalue.assert_not_called()

    def _test_acl_update_no_acls(self, need_compare):
        fake_lswitch = fakes.FakeOvsdbRow.create_one_ovsdb_row()
        self.ovn_api.get_acls_for_lswitches.return_value = (
            {}, {}, {fake_lswitch.name: fake_lswitch})
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lswitch):
            cmd = commands.UpdateACLsCommand(
                self.ovn_api, [fake_lswitch.name], port_list=[],
                acl_new_values_dict={},
                need_compare=need_compare)
            cmd.run_idl(self.transaction)
            self.transaction.insert.assert_not_called()
            fake_lswitch.addvalue.assert_not_called()
            fake_lswitch.delvalue.assert_not_called()

    def test_acl_update_compare_no_acls(self):
        self._test_acl_update_no_acls(need_compare=True)

    def test_acl_update_no_compare_no_acls(self):
        self._test_acl_update_no_acls(need_compare=False)

    def test_acl_update_compare_acls(self):
        fake_sg_rule = \
            fakes.FakeSecurityGroupRule.create_one_security_group_rule().info()
        fake_port = fakes.FakePort.create_one_port().info()
        fake_add_acl = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'match': 'add_acl'})
        fake_del_acl = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'match': 'del_acl'})
        fake_lswitch = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'name': ovn_utils.ovn_name(fake_port['network_id']),
                   'acls': []})
        add_acl = ovn_acl.add_sg_rule_acl_for_port(
            fake_port, fake_sg_rule, 'add_acl')
        self.ovn_api.get_acls_for_lswitches.return_value = (
            {fake_port['id']: [fake_del_acl.match]},
            {fake_del_acl.match: fake_del_acl},
            {fake_lswitch.name.replace('neutron-', ''): fake_lswitch})
        cmd = commands.UpdateACLsCommand(
            self.ovn_api, [fake_port['network_id']],
            [fake_port], {fake_port['id']: [add_acl]},
            need_compare=True)
        self.transaction.insert.return_value = fake_add_acl
        cmd.run_idl(self.transaction)
        self.transaction.insert.assert_called_once_with(
            self.ovn_api._tables['ACL'])
        fake_lswitch.addvalue.assert_called_with('acls', fake_add_acl.uuid)

    def test_acl_update_no_compare_add_acls(self):
        fake_sg_rule = \
            fakes.FakeSecurityGroupRule.create_one_security_group_rule().info()
        fake_port = fakes.FakePort.create_one_port().info()
        fake_acl = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'match': '*'})
        fake_lswitch = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'name': ovn_utils.ovn_name(fake_port['network_id'])})
        add_acl = ovn_acl.add_sg_rule_acl_for_port(
            fake_port, fake_sg_rule, '*')
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lswitch):
            self.transaction.insert.return_value = fake_acl
            cmd = commands.UpdateACLsCommand(
                self.ovn_api, [fake_port['network_id']],
                [fake_port], {fake_port['id']: add_acl},
                need_compare=False,
                is_add_acl=True)
            cmd.run_idl(self.transaction)
            self.transaction.insert.assert_called_once_with(
                self.ovn_api._tables['ACL'])
            fake_lswitch.addvalue.assert_called_once_with(
                'acls', fake_acl.uuid)

    def test_acl_update_no_compare_del_acls(self):
        fake_sg_rule = \
            fakes.FakeSecurityGroupRule.create_one_security_group_rule().info()
        fake_port = fakes.FakePort.create_one_port().info()
        fake_acl = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'match': '*', 'external_ids':
                   {'neutron:lport': fake_port['id'],
                    'neutron:security_group_rule_id': fake_sg_rule['id']}})
        fake_lswitch = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'name': ovn_utils.ovn_name(fake_port['network_id']),
                   'acls': [fake_acl]})
        del_acl = ovn_acl.add_sg_rule_acl_for_port(
            fake_port, fake_sg_rule, '*')
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lswitch):
            cmd = commands.UpdateACLsCommand(
                self.ovn_api, [fake_port['network_id']],
                [fake_port], {fake_port['id']: del_acl},
                need_compare=False,
                is_add_acl=False)
            cmd.run_idl(self.transaction)
            self.transaction.insert.assert_not_called()
            fake_lswitch.delvalue.assert_called_with('acls', mock.ANY)


class TestAddStaticRouteCommand(TestBaseCommand):

    def test_lrouter_not_found(self):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.AddStaticRouteCommand(self.ovn_api, 'fake-lrouter')
            self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)
            self.transaction.insert.assert_not_called()

    def test_static_route_add(self):
        fake_lrouter = fakes.FakeOvsdbRow.create_one_ovsdb_row()
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lrouter):
            fake_static_route = fakes.FakeOvsdbRow.create_one_ovsdb_row()
            self.transaction.insert.return_value = fake_static_route
            cmd = commands.AddStaticRouteCommand(
                self.ovn_api, fake_lrouter.name,
                nexthop='40.0.0.100',
                ip_prefix='30.0.0.0/24')
            cmd.run_idl(self.transaction)
            self.transaction.insert.assert_called_once_with(
                self.ovn_api._tables['Logical_Router_Static_Route'])
            self.assertEqual('40.0.0.100', fake_static_route.nexthop)
            self.assertEqual('30.0.0.0/24', fake_static_route.ip_prefix)
            fake_lrouter.addvalue.assert_called_once_with(
                'static_routes', fake_static_route.uuid)


class TestDelStaticRouteCommand(TestBaseCommand):

    def _test_lrouter_no_exist(self, if_exists=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.DelStaticRouteCommand(
                self.ovn_api, 'fake-lrouter',
                '30.0.0.0/24', '40.0.0.100',
                if_exists=if_exists)
            if if_exists:
                cmd.run_idl(self.transaction)
            else:
                self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_lrouter_no_exist_ignore(self):
        self._test_lrouter_no_exist(if_exists=True)

    def test_lrouter_no_exist_fail(self):
        self._test_lrouter_no_exist(if_exists=False)

    def test_static_route_del(self):
        fake_static_route = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'ip_prefix': '50.0.0.0/24', 'nexthop': '40.0.0.101'})
        fake_lrouter = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'static_routes': [fake_static_route]})
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lrouter):
            cmd = commands.DelStaticRouteCommand(
                self.ovn_api, fake_lrouter.name,
                fake_static_route.ip_prefix, fake_static_route.nexthop,
                if_exists=True)
            cmd.run_idl(self.transaction)
            fake_lrouter.delvalue.assert_called_once_with(
                'static_routes', mock.ANY)

    def test_static_route_del_not_found(self):
        fake_static_route1 = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'ip_prefix': '50.0.0.0/24', 'nexthop': '40.0.0.101'})
        fake_static_route2 = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'ip_prefix': '60.0.0.0/24', 'nexthop': '70.0.0.101'})
        fake_lrouter = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'static_routes': [fake_static_route2]})
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lrouter):
            cmd = commands.DelStaticRouteCommand(
                self.ovn_api, fake_lrouter.name,
                fake_static_route1.ip_prefix, fake_static_route1.nexthop,
                if_exists=True)
            cmd.run_idl(self.transaction)
            fake_lrouter.delvalue.assert_not_called()
            self.assertEqual([mock.ANY], fake_lrouter.static_routes)


class TestAddAddrSetCommand(TestBaseCommand):

    def test_addrset_exists(self):
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=mock.ANY):
            cmd = commands.AddAddrSetCommand(
                self.ovn_api, 'fake-addrset', may_exist=True)
            cmd.run_idl(self.transaction)
            self.transaction.insert.assert_not_called()

    def test_addrset_add_exists(self):
        fake_addrset = fakes.FakeOvsdbRow.create_one_ovsdb_row()
        self.ovn_api._tables['Address_Set'].rows[fake_addrset.uuid] = \
            fake_addrset
        self.transaction.insert.return_value = fake_addrset
        cmd = commands.AddAddrSetCommand(
            self.ovn_api, fake_addrset.name, may_exist=False)
        cmd.run_idl(self.transaction)
        # NOTE(rtheis): Mocking the transaction allows this insert
        # to succeed when it normally would fail due the duplicate name.
        self.transaction.insert.assert_called_once_with(
            self.ovn_api._tables['Address_Set'])

    def _test_addrset_add(self, may_exist=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=None):
            fake_addrset = fakes.FakeOvsdbRow.create_one_ovsdb_row(
                attrs={'foo': ''})
            self.transaction.insert.return_value = fake_addrset
            cmd = commands.AddAddrSetCommand(
                self.ovn_api, 'fake-addrset', may_exist=may_exist,
                foo='bar')
            cmd.run_idl(self.transaction)
            self.transaction.insert.assert_called_once_with(
                self.ovn_api._tables['Address_Set'])
            self.assertEqual('fake-addrset', fake_addrset.name)
            self.assertEqual('bar', fake_addrset.foo)

    def test_addrset_add_may_exist(self):
        self._test_addrset_add(may_exist=True)

    def test_addrset_add_ignore_exists(self):
        self._test_addrset_add(may_exist=False)


class TestDelAddrSetCommand(TestBaseCommand):

    def _test_addrset_del_no_exist(self, if_exists=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.DelAddrSetCommand(
                self.ovn_api, 'fake-addrset', if_exists=if_exists)
            if if_exists:
                cmd.run_idl(self.transaction)
            else:
                self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_addrset_no_exist_ignore(self):
        self._test_addrset_del_no_exist(if_exists=True)

    def test_addrset_no_exist_fail(self):
        self._test_addrset_del_no_exist(if_exists=False)

    def test_addrset_del(self):
        fake_addrset = fakes.FakeOvsdbRow.create_one_ovsdb_row()
        self.ovn_api._tables['Address_Set'].rows[fake_addrset.uuid] = \
            fake_addrset
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_addrset):
            cmd = commands.DelAddrSetCommand(
                self.ovn_api, fake_addrset.name, if_exists=True)
            cmd.run_idl(self.transaction)
            fake_addrset.delete.assert_called_once_with()


class TestUpdateAddrSetCommand(TestBaseCommand):

    def _test_addrset_update_no_exist(self, if_exists=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.UpdateAddrSetCommand(
                self.ovn_api, 'fake-addrset',
                addrs_add=[], addrs_remove=[],
                if_exists=if_exists)
            if if_exists:
                cmd.run_idl(self.transaction)
            else:
                self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_addrset_no_exist_ignore(self):
        self._test_addrset_update_no_exist(if_exists=True)

    def test_addrset_no_exist_fail(self):
        self._test_addrset_update_no_exist(if_exists=False)

    def _test_addrset_update(self, addrs_add=None, addrs_del=None):
        save_address = '10.0.0.1'
        initial_addresses = [save_address]
        final_addresses = [save_address]
        expected_addvalue_calls = []
        expected_delvalue_calls = []
        if addrs_add:
            for addr_add in addrs_add:
                final_addresses.append(addr_add)
                expected_addvalue_calls.append(
                    mock.call('addresses', addr_add))
        if addrs_del:
            for addr_del in addrs_del:
                initial_addresses.append(addr_del)
                expected_delvalue_calls.append(
                    mock.call('addresses', addr_del))
        fake_addrset = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'addresses': initial_addresses})
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_addrset):
            cmd = commands.UpdateAddrSetCommand(
                self.ovn_api, fake_addrset.name,
                addrs_add=addrs_add, addrs_remove=addrs_del,
                if_exists=True)
            cmd.run_idl(self.transaction)
            fake_addrset.addvalue.assert_has_calls(expected_addvalue_calls)
            fake_addrset.delvalue.assert_has_calls(expected_delvalue_calls)

    def test_addrset_update_add(self):
        self._test_addrset_update(addrs_add=['10.0.0.4'])

    def test_addrset_update_del(self):
        self._test_addrset_update(addrs_del=['10.0.0.2'])


class TestUpdateAddrSetExtIdsCommand(TestBaseCommand):
    def setUp(self):
        super(TestUpdateAddrSetExtIdsCommand, self).setUp()
        self.ext_ids = {ovn_const.OVN_SG_EXT_ID_KEY: 'default'}

    def _test_addrset_extids_update_no_exist(self, if_exists=True):
        with mock.patch.object(idlutils, 'row_by_value',
                               side_effect=idlutils.RowNotFound):
            cmd = commands.UpdateAddrSetExtIdsCommand(
                self.ovn_api, 'fake-addrset', self.ext_ids,
                if_exists=if_exists)
            if if_exists:
                cmd.run_idl(self.transaction)
            else:
                self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_addrset_no_exist_ignore(self):
        self._test_addrset_extids_update_no_exist(if_exists=True)

    def test_addrset_no_exist_fail(self):
        self._test_addrset_extids_update_no_exist(if_exists=False)

    def test_addrset_extids_update(self):
        new_ext_ids = {ovn_const.OVN_SG_EXT_ID_KEY: 'default-new'}
        fake_addrset = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'external_ids': self.ext_ids})
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_addrset):
            cmd = commands.UpdateAddrSetExtIdsCommand(
                self.ovn_api, fake_addrset.name,
                new_ext_ids, if_exists=True)
            cmd.run_idl(self.transaction)
            self.assertEqual(new_ext_ids, fake_addrset.external_ids)


class TestAddDHCPOptionsCommand(TestBaseCommand):

    def test_dhcp_options_exists(self):
        fake_ext_ids = {'subnet_id': 'fake-subnet-id',
                        'port_id': 'fake-port-id'}
        fake_dhcp_options = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'external_ids': fake_ext_ids})
        self.ovn_api._tables['DHCP_Options'].rows[fake_dhcp_options.uuid] = \
            fake_dhcp_options
        cmd = commands.AddDHCPOptionsCommand(
            self.ovn_api, fake_ext_ids['subnet_id'], fake_ext_ids['port_id'],
            may_exist=True, external_ids=fake_ext_ids)
        cmd.run_idl(self.transaction)
        self.transaction.insert.assert_not_called()
        self.assertEqual(fake_ext_ids, fake_dhcp_options.external_ids)

    def _test_dhcp_options_add(self, may_exist=True):
        fake_subnet_id = 'fake-subnet-id-' + str(may_exist)
        fake_port_id = 'fake-port-id-' + str(may_exist)
        fake_ext_ids1 = {'subnet_id': fake_subnet_id, 'port_id': fake_port_id}
        fake_dhcp_options1 = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'external_ids': fake_ext_ids1})
        self.ovn_api._tables['DHCP_Options'].rows[fake_dhcp_options1.uuid] = \
            fake_dhcp_options1
        fake_ext_ids2 = {'subnet_id': fake_subnet_id}
        fake_dhcp_options2 = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'external_ids': fake_ext_ids2})
        fake_dhcp_options3 = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'external_ids': {'subnet_id': 'nomatch'}})
        self.ovn_api._tables['DHCP_Options'].rows[fake_dhcp_options3.uuid] = \
            fake_dhcp_options3
        self.transaction.insert.return_value = fake_dhcp_options2
        cmd = commands.AddDHCPOptionsCommand(
            self.ovn_api, fake_ext_ids2['subnet_id'], may_exist=may_exist,
            external_ids=fake_ext_ids2)
        cmd.run_idl(self.transaction)
        self.transaction.insert.assert_called_once_with(
            self.ovn_api._tables['DHCP_Options'])
        self.assertEqual(fake_ext_ids2, fake_dhcp_options2.external_ids)

    def test_dhcp_options_add_may_exist(self):
        self._test_dhcp_options_add(may_exist=True)

    def test_dhcp_options_add_ignore_exists(self):
        self._test_dhcp_options_add(may_exist=False)

    def _test_dhcp_options_update_result(self, new_insert=False):
        fake_ext_ids = {'subnet_id': 'fake_subnet', 'port_id': 'fake_port'}
        fake_dhcp_opts = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'external_ids': fake_ext_ids})
        if new_insert:
            self.transaction.insert.return_value = fake_dhcp_opts
            self.transaction.get_insert_uuid = mock.Mock(
                return_value='fake-uuid')
        else:
            self.ovn_api._tables['DHCP_Options'].rows[fake_dhcp_opts.uuid] = \
                fake_dhcp_opts
            self.transaction.get_insert_uuid = mock.Mock(
                return_value=None)

        cmd = commands.AddDHCPOptionsCommand(
            self.ovn_api, fake_ext_ids['subnet_id'],
            port_id=fake_ext_ids['port_id'], may_exist=True,
            external_ids=fake_ext_ids)
        cmd.run_idl(self.transaction)
        cmd.post_commit(self.transaction)
        if new_insert:
            self.assertEqual('fake-uuid', cmd.result)
        else:
            self.assertEqual(fake_dhcp_opts.uuid, cmd.result)

    def test_dhcp_options_update_result_with_exist_row(self):
        self._test_dhcp_options_update_result(new_insert=False)

    def test_dhcp_options_update_result_with_new_row(self):
        self._test_dhcp_options_update_result(new_insert=True)


class TestDelDHCPOptionsCommand(TestBaseCommand):

    def _test_dhcp_options_del_no_exist(self, if_exists=True):
        cmd = commands.DelDHCPOptionsCommand(
            self.ovn_api, 'fake-dhcp-options', if_exists=if_exists)
        if if_exists:
            cmd.run_idl(self.transaction)
        else:
            self.assertRaises(RuntimeError, cmd.run_idl, self.transaction)

    def test_dhcp_options_no_exist_ignore(self):
        self._test_dhcp_options_del_no_exist(if_exists=True)

    def test_dhcp_options_no_exist_fail(self):
        self._test_dhcp_options_del_no_exist(if_exists=False)

    def test_dhcp_options_del(self):
        fake_dhcp_options = fakes.FakeOvsdbRow.create_one_ovsdb_row(
            attrs={'external_ids': {'subnet_id': 'fake-subnet-id'}})
        self.ovn_api._tables['DHCP_Options'].rows[fake_dhcp_options.uuid] = \
            fake_dhcp_options
        cmd = commands.DelDHCPOptionsCommand(
            self.ovn_api, fake_dhcp_options.uuid, if_exists=True)
        cmd.run_idl(self.transaction)
        fake_dhcp_options.delete.assert_called_once_with()


class TestSetNATRuleInLRouterCommand(TestBaseCommand):

    def test_set_nat_rule(self):
        fake_lrouter = fakes.FakeOvsdbRow.create_one_ovsdb_row()
        with mock.patch.object(idlutils, 'row_by_value',
                               return_value=fake_lrouter):
            fake_nat_rule_1 = fakes.FakeOvsdbRow.create_one_ovsdb_row(
                attrs={'external_ip': '192.168.1.10',
                       'logical_ip': '10.0.0.4', 'type': 'dnat_and_snat'})
            fake_nat_rule_2 = fakes.FakeOvsdbRow.create_one_ovsdb_row(
                attrs={'external_ip': '192.168.1.8',
                       'logical_ip': '10.0.0.5', 'type': 'dnat_and_snat'})
            fake_lrouter.nat = [fake_nat_rule_1, fake_nat_rule_2]
            self.ovn_api._tables['NAT'].rows[fake_nat_rule_1.uuid] = \
                fake_nat_rule_1
            self.ovn_api._tables['NAT'].rows[fake_nat_rule_2.uuid] = \
                fake_nat_rule_2
            cmd = commands.SetNATRuleInLRouterCommand(
                self.ovn_api, fake_lrouter.name, fake_nat_rule_1.uuid,
                logical_ip='10.0.0.10')
            cmd.run_idl(self.transaction)
            self.assertEqual('10.0.0.10', fake_nat_rule_1.logical_ip)
            self.assertEqual('10.0.0.5', fake_nat_rule_2.logical_ip)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\ovsdb\test_impl_idl_ovn.py
===========File Type===========
.py
===========File Content===========
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import copy

import mock

from networking_ovn.common import config
from networking_ovn.common import constants as ovn_const
from networking_ovn.common import utils
from networking_ovn.ovsdb import impl_idl_ovn
from networking_ovn.tests import base
from networking_ovn.tests.unit import fakes


class TestDBImplIdlOvn(base.TestCase):

    def _load_ovsdb_fake_rows(self, table, fake_attrs):
        for fake_attr in fake_attrs:
            fake_row = fakes.FakeOvsdbRow.create_one_ovsdb_row(
                attrs=fake_attr)
            # Pre-populate ovs idl "._data"
            fake_data = copy.deepcopy(fake_attr)
            try:
                del fake_data["unit_test_id"]
            except KeyError:
                pass
            setattr(fake_row, "_data", fake_data)
            table.rows[fake_row.uuid] = fake_row

    def _find_ovsdb_fake_row(self, table, key, value):
        for fake_row in table.rows.values():
            if getattr(fake_row, key) == value:
                return fake_row
        return None

    def _construct_ovsdb_references(self, fake_associations,
                                    parent_table, child_table,
                                    parent_key, child_key,
                                    reference_column_name):
        for p_name, c_names in fake_associations.items():
            p_row = self._find_ovsdb_fake_row(parent_table, parent_key, p_name)
            c_uuids = []
            for c_name in c_names:
                c_row = self._find_ovsdb_fake_row(child_table, child_key,
                                                  c_name)
                if not c_row:
                    continue
                # Fake IDL processing (uuid -> row)
                c_uuids.append(c_row)
            setattr(p_row, reference_column_name, c_uuids)


class TestNBImplIdlOvn(TestDBImplIdlOvn):

    fake_set = {
        'lswitches': [
            {'name': utils.ovn_name('ls-id-1'),
             'external_ids': {ovn_const.OVN_NETWORK_NAME_EXT_ID_KEY:
                              'ls-name-1'}},
            {'name': utils.ovn_name('ls-id-2'),
             'external_ids': {ovn_const.OVN_NETWORK_NAME_EXT_ID_KEY:
                              'ls-name-2'}},
            {'name': utils.ovn_name('ls-id-3'),
             'external_ids': {ovn_const.OVN_NETWORK_NAME_EXT_ID_KEY:
                              'ls-name-3'}},
            {'name': 'ls-id-4',
             'external_ids': {'not-neutron:network_name': 'ls-name-4'}},
            {'name': utils.ovn_name('ls-id-5'),
             'external_ids': {ovn_const.OVN_NETWORK_NAME_EXT_ID_KEY:
                              'ls-name-5'}}],
        'lswitch_ports': [
            {'name': 'lsp-id-11', 'addresses': ['10.0.1.1'],
             'external_ids': {ovn_const.OVN_PORT_NAME_EXT_ID_KEY:
                              'lsp-name-11'}},
            {'name': 'lsp-id-12', 'addresses': ['10.0.1.2'],
             'external_ids': {ovn_const.OVN_PORT_NAME_EXT_ID_KEY:
                              'lsp-name-12'}},
            {'name': 'lsp-rp-id-1', 'addresses': ['10.0.1.254'],
             'external_ids': {ovn_const.OVN_PORT_NAME_EXT_ID_KEY:
                              'lsp-rp-name-1'},
             'options': {'router-port':
                         utils.ovn_lrouter_port_name('orp-id-a1')}},
            {'name': 'provnet-ls-id-1', 'addresses': ['unknown'],
             'external_ids': {},
             'options': {'network_name': 'physnet1'}},
            {'name': 'lsp-id-21', 'addresses': ['10.0.2.1'],
             'external_ids': {ovn_const.OVN_PORT_NAME_EXT_ID_KEY:
                              'lsp-name-21'}},
            {'name': 'lsp-id-22', 'addresses': ['10.0.2.2'],
             'external_ids': {}},
            {'name': 'lsp-id-23', 'addresses': ['10.0.2.3'],
             'external_ids': {'not-neutron:port_name': 'lsp-name-23'}},
            {'name': 'lsp-rp-id-2', 'addresses': ['10.0.2.254'],
             'external_ids': {ovn_const.OVN_PORT_NAME_EXT_ID_KEY:
                              'lsp-rp-name-2'},
             'options': {'router-port':
                         utils.ovn_lrouter_port_name('orp-id-a2')}},
            {'name': 'provnet-ls-id-2', 'addresses': ['unknown'],
             'external_ids': {},
             'options': {'network_name': 'physnet2'}},
            {'name': 'lsp-id-31', 'addresses': ['10.0.3.1'],
             'external_ids': {ovn_const.OVN_PORT_NAME_EXT_ID_KEY:
                              'lsp-name-31'}},
            {'name': 'lsp-id-32', 'addresses': ['10.0.3.2'],
             'external_ids': {ovn_const.OVN_PORT_NAME_EXT_ID_KEY:
                              'lsp-name-32'}},
            {'name': 'lsp-rp-id-3', 'addresses': ['10.0.3.254'],
             'external_ids': {ovn_const.OVN_PORT_NAME_EXT_ID_KEY:
                              'lsp-rp-name-3'},
             'options': {'router-port':
                         utils.ovn_lrouter_port_name('orp-id-a3')}},
            {'name': 'lsp-vpn-id-3', 'addresses': ['10.0.3.253'],
             'external_ids': {ovn_const.OVN_PORT_NAME_EXT_ID_KEY:
                              'lsp-vpn-name-3'}},
            {'name': 'lsp-id-41', 'addresses': ['20.0.1.1'],
             'external_ids': {'not-neutron:port_name': 'lsp-name-41'}},
            {'name': 'lsp-rp-id-4', 'addresses': ['20.0.1.254'],
             'external_ids': {},
             'options': {'router-port': 'xrp-id-b1'}},
            {'name': 'lsp-id-51', 'addresses': ['20.0.2.1'],
             'external_ids': {ovn_const.OVN_PORT_NAME_EXT_ID_KEY:
                              'lsp-name-51'}},
            {'name': 'lsp-id-52', 'addresses': ['20.0.2.2'],
             'external_ids': {ovn_const.OVN_PORT_NAME_EXT_ID_KEY:
                              'lsp-name-52'}},
            {'name': 'lsp-rp-id-5', 'addresses': ['20.0.2.254'],
             'external_ids': {ovn_const.OVN_PORT_NAME_EXT_ID_KEY:
                              'lsp-rp-name-5'},
             'options': {'router-port':
                         utils.ovn_lrouter_port_name('orp-id-b2')}},
            {'name': 'lsp-vpn-id-5', 'addresses': ['20.0.2.253'],
             'external_ids': {ovn_const.OVN_PORT_NAME_EXT_ID_KEY:
                              'lsp-vpn-name-5'}}],
        'lrouters': [
            {'name': utils.ovn_name('lr-id-a'),
             'external_ids': {ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY:
                              'lr-name-a'}},
            {'name': utils.ovn_name('lr-id-b'),
             'external_ids': {ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY:
                              'lr-name-b'}},
            {'name': utils.ovn_name('lr-id-c'),
             'external_ids': {ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY:
                              'lr-name-c'}},
            {'name': utils.ovn_name('lr-id-d'),
             'external_ids': {ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY:
                              'lr-name-d'}},
            {'name': utils.ovn_name('lr-id-e'),
             'external_ids': {ovn_const.OVN_ROUTER_NAME_EXT_ID_KEY:
                              'lr-name-e'}}],
        'lrouter_ports': [
            {'name': utils.ovn_lrouter_port_name('orp-id-a1'),
             'external_ids': {}, 'networks': ['10.0.1.0/24'],
             'options': {ovn_const.OVN_GATEWAY_CHASSIS_KEY: 'host-1'}},
            {'name': utils.ovn_lrouter_port_name('orp-id-a2'),
             'external_ids': {}, 'networks': ['10.0.2.0/24'],
             'options': {ovn_const.OVN_GATEWAY_CHASSIS_KEY: 'host-1'}},
            {'name': utils.ovn_lrouter_port_name('orp-id-a3'),
             'external_ids': {}, 'networks': ['10.0.3.0/24'],
             'options': {ovn_const.OVN_GATEWAY_CHASSIS_KEY:
                         ovn_const.OVN_GATEWAY_INVALID_CHASSIS}},
            {'name': 'xrp-id-b1',
             'external_ids': {}, 'networks': ['20.0.1.0/24']},
            {'name': utils.ovn_lrouter_port_name('orp-id-b2'),
             'external_ids': {}, 'networks': ['20.0.2.0/24'],
             'options': {ovn_const.OVN_GATEWAY_CHASSIS_KEY: 'host-2'}},
            {'name': utils.ovn_lrouter_port_name('orp-id-b3'),
             'external_ids': {}, 'networks': ['20.0.3.0/24'],
             'options': {}}],
        'static_routes': [{'ip_prefix': '20.0.0.0/16',
                           'nexthop': '10.0.3.253'},
                          {'ip_prefix': '10.0.0.0/16',
                           'nexthop': '20.0.2.253'}],
        'nats': [{'external_ip': '10.0.3.1', 'logical_ip': '20.0.0.0/16',
                  'type': 'snat'},
                 {'external_ip': '20.0.2.1', 'logical_ip': '10.0.0.0/24',
                  'type': 'snat'},
                 {'external_ip': '20.0.2.4', 'logical_ip': '10.0.0.4',
                  'type': 'dnat_and_snat', 'external_mac': [],
                  'logical_port': []},
                 {'external_ip': '20.0.2.5', 'logical_ip': '10.0.0.5',
                  'type': 'dnat_and_snat',
                  'external_mac': ['00:01:02:03:04:05'],
                  'logical_port': ['lsp-id-001']}],
        'acls': [
            {'unit_test_id': 1,
             'action': 'allow-related', 'direction': 'from-lport',
             'external_ids': {'neutron:lport': 'lsp-id-11'},
             'match': 'inport == "lsp-id-11" && ip4'},
            {'unit_test_id': 2,
             'action': 'allow-related', 'direction': 'to-lport',
             'external_ids': {'neutron:lport': 'lsp-id-11'},
             'match': 'outport == "lsp-id-11" && ip4.src == $as_ip4_id_1'},
            {'unit_test_id': 3,
             'action': 'allow-related', 'direction': 'from-lport',
             'external_ids': {'neutron:lport': 'lsp-id-12'},
             'match': 'inport == "lsp-id-12" && ip4'},
            {'unit_test_id': 4,
             'action': 'allow-related', 'direction': 'to-lport',
             'external_ids': {'neutron:lport': 'lsp-id-12'},
             'match': 'outport == "lsp-id-12" && ip4.src == $as_ip4_id_1'},
            {'unit_test_id': 5,
             'action': 'allow-related', 'direction': 'from-lport',
             'external_ids': {'neutron:lport': 'lsp-id-21'},
             'match': 'inport == "lsp-id-21" && ip4'},
            {'unit_test_id': 6,
             'action': 'allow-related', 'direction': 'to-lport',
             'external_ids': {'neutron:lport': 'lsp-id-21'},
             'match': 'outport == "lsp-id-21" && ip4.src == $as_ip4_id_2'},
            {'unit_test_id': 7,
             'action': 'allow-related', 'direction': 'from-lport',
             'external_ids': {'neutron:lport': 'lsp-id-41'},
             'match': 'inport == "lsp-id-41" && ip4'},
            {'unit_test_id': 8,
             'action': 'allow-related', 'direction': 'to-lport',
             'external_ids': {'neutron:lport': 'lsp-id-41'},
             'match': 'outport == "lsp-id-41" && ip4.src == $as_ip4_id_4'},
            {'unit_test_id': 9,
             'action': 'allow-related', 'direction': 'from-lport',
             'external_ids': {'neutron:lport': 'lsp-id-52'},
             'match': 'inport == "lsp-id-52" && ip4'},
            {'unit_test_id': 10,
             'action': 'allow-related', 'direction': 'to-lport',
             'external_ids': {'neutron:lport': 'lsp-id-52'},
             'match': 'outport == "lsp-id-52" && ip4.src == $as_ip4_id_5'}],
        'dhcp_options': [
            {'cidr': '10.0.1.0/24',
             'external_ids': {'subnet_id': 'subnet-id-10-0-1-0'},
             'options': {'mtu': '1442', 'router': '10.0.1.254'}},
            {'cidr': '10.0.2.0/24',
             'external_ids': {'subnet_id': 'subnet-id-10-0-2-0'},
             'options': {'mtu': '1442', 'router': '10.0.2.254'}},
            {'cidr': '10.0.1.0/26',
             'external_ids': {'subnet_id': 'subnet-id-10-0-1-0',
                              'port_id': 'lsp-vpn-id-3'},
             'options': {'mtu': '1442', 'router': '10.0.1.1'}},
            {'cidr': '20.0.1.0/24',
             'external_ids': {'subnet_id': 'subnet-id-20-0-1-0'},
             'options': {'mtu': '1442', 'router': '20.0.1.254'}},
            {'cidr': '20.0.2.0/24',
             'external_ids': {'subnet_id': 'subnet-id-20-0-2-0',
                              'port_id': 'lsp-vpn-id-5'},
             'options': {'mtu': '1442', 'router': '20.0.2.254'}},
            {'cidr': '2001:dba::/64',
             'external_ids': {'subnet_id': 'subnet-id-2001-dba',
                              'port_id': 'lsp-vpn-id-5'},
             'options': {'server_id': '12:34:56:78:9a:bc'}},
            {'cidr': '30.0.1.0/24',
             'external_ids': {'port_id': 'port-id-30-0-1-0'},
             'options': {'mtu': '1442', 'router': '30.0.2.254'}},
            {'cidr': '30.0.2.0/24', 'external_ids': {}, 'options': {}}],
        'address_sets': [
            {'name': '$as_ip4_id_1',
             'addresses': ['10.0.1.1', '10.0.1.2'],
             'external_ids': {ovn_const.OVN_SG_EXT_ID_KEY: 'id_1'}},
            {'name': '$as_ip4_id_2',
             'addresses': ['10.0.2.1'],
             'external_ids': {ovn_const.OVN_SG_EXT_ID_KEY: 'id_2'}},
            {'name': '$as_ip4_id_3',
             'addresses': ['10.0.3.1', '10.0.3.2'],
             'external_ids': {ovn_const.OVN_SG_EXT_ID_KEY: 'id_3'}},
            {'name': '$as_ip4_id_4',
             'addresses': ['20.0.1.1', '20.0.1.2'],
             'external_ids': {}},
            {'name': '$as_ip4_id_5',
             'addresses': ['20.0.2.1', '20.0.2.2'],
             'external_ids': {ovn_const.OVN_SG_EXT_ID_KEY: 'id_5'}},
            ]}

    fake_associations = {
        'lstolsp': {
            utils.ovn_name('ls-id-1'): [
                'lsp-id-11', 'lsp-id-12', 'lsp-rp-id-1', 'provnet-ls-id-1'],
            utils.ovn_name('ls-id-2'): [
                'lsp-id-21', 'lsp-id-22', 'lsp-id-23', 'lsp-rp-id-2',
                'provnet-ls-id-2'],
            utils.ovn_name('ls-id-3'): [
                'lsp-id-31', 'lsp-id-32', 'lsp-rp-id-3', 'lsp-vpn-id-3'],
            'ls-id-4': [
                'lsp-id-41', 'lsp-rp-id-4'],
            utils.ovn_name('ls-id-5'): [
                'lsp-id-51', 'lsp-id-52', 'lsp-rp-id-5', 'lsp-vpn-id-5']},
        'lrtolrp': {
            utils.ovn_name('lr-id-a'): [
                utils.ovn_lrouter_port_name('orp-id-a1'),
                utils.ovn_lrouter_port_name('orp-id-a2'),
                utils.ovn_lrouter_port_name('orp-id-a3')],
            utils.ovn_name('lr-id-b'): [
                'xrp-id-b1',
                utils.ovn_lrouter_port_name('orp-id-b2')]},
        'lrtosroute': {
            utils.ovn_name('lr-id-a'): ['20.0.0.0/16'],
            utils.ovn_name('lr-id-b'): ['10.0.0.0/16']
            },
        'lrtonat': {
            utils.ovn_name('lr-id-a'): ['10.0.3.1'],
            utils.ovn_name('lr-id-b'): ['20.0.2.1', '20.0.2.4', '20.0.2.5'],
            },
        'lstoacl': {
            utils.ovn_name('ls-id-1'): [1, 2, 3, 4],
            utils.ovn_name('ls-id-2'): [5, 6],
            'ls-id-4': [7, 8],
            utils.ovn_name('ls-id-5'): [9, 10]}
        }

    def setUp(self):
        super(TestNBImplIdlOvn, self).setUp()

        self.lswitch_table = fakes.FakeOvsdbTable.create_one_ovsdb_table()
        self.lsp_table = fakes.FakeOvsdbTable.create_one_ovsdb_table()
        self.lrouter_table = fakes.FakeOvsdbTable.create_one_ovsdb_table()
        self.lrp_table = fakes.FakeOvsdbTable.create_one_ovsdb_table()
        self.sroute_table = fakes.FakeOvsdbTable.create_one_ovsdb_table()
        self.nat_table = fakes.FakeOvsdbTable.create_one_ovsdb_table()
        self.acl_table = fakes.FakeOvsdbTable.create_one_ovsdb_table()
        self.dhcp_table = fakes.FakeOvsdbTable.create_one_ovsdb_table()
        self.address_set_table = fakes.FakeOvsdbTable.create_one_ovsdb_table()

        self._tables = {}
        self._tables['Logical_Switch'] = self.lswitch_table
        self._tables['Logical_Switch_Port'] = self.lsp_table
        self._tables['Logical_Router'] = self.lrouter_table
        self._tables['Logical_Router_Port'] = self.lrp_table
        self._tables['Logical_Router_Static_Route'] = self.sroute_table
        self._tables['ACL'] = self.acl_table
        self._tables['DHCP_Options'] = self.dhcp_table
        self._tables['Address_Set'] = self.address_set_table

        with mock.patch.object(impl_idl_ovn, 'get_connection',
                               return_value=mock.Mock()):
            impl_idl_ovn.OvsdbNbOvnIdl.ovsdb_connection = None
            self.nb_ovn_idl = impl_idl_ovn.OvsdbNbOvnIdl(mock.Mock())

        self.nb_ovn_idl.idl.tables = self._tables

    def _load_nb_db(self):
        # Load Switches and Switch Ports
        fake_lswitches = TestNBImplIdlOvn.fake_set['lswitches']
        self._load_ovsdb_fake_rows(self.lswitch_table, fake_lswitches)
        fake_lsps = TestNBImplIdlOvn.fake_set['lswitch_ports']
        self._load_ovsdb_fake_rows(self.lsp_table, fake_lsps)
        # Associate switches and ports
        self._construct_ovsdb_references(
            TestNBImplIdlOvn.fake_associations['lstolsp'],
            self.lswitch_table, self.lsp_table,
            'name', 'name', 'ports')
        # Load Routers and Router Ports
        fake_lrouters = TestNBImplIdlOvn.fake_set['lrouters']
        self._load_ovsdb_fake_rows(self.lrouter_table, fake_lrouters)
        fake_lrps = TestNBImplIdlOvn.fake_set['lrouter_ports']
        self._load_ovsdb_fake_rows(self.lrp_table, fake_lrps)
        # Associate routers and router ports
        self._construct_ovsdb_references(
            TestNBImplIdlOvn.fake_associations['lrtolrp'],
            self.lrouter_table, self.lrp_table,
            'name', 'name', 'ports')
        # Load static routes
        fake_sroutes = TestNBImplIdlOvn.fake_set['static_routes']
        self._load_ovsdb_fake_rows(self.sroute_table, fake_sroutes)
        # Associate routers and static routes
        self._construct_ovsdb_references(
            TestNBImplIdlOvn.fake_associations['lrtosroute'],
            self.lrouter_table, self.sroute_table,
            'name', 'ip_prefix', 'static_routes')
        # Load nats
        fake_nats = TestNBImplIdlOvn.fake_set['nats']
        self._load_ovsdb_fake_rows(self.nat_table, fake_nats)
        # Associate routers and nats
        self._construct_ovsdb_references(
            TestNBImplIdlOvn.fake_associations['lrtonat'],
            self.lrouter_table, self.nat_table,
            'name', 'external_ip', 'nat')
        # Load acls
        fake_acls = TestNBImplIdlOvn.fake_set['acls']
        self._load_ovsdb_fake_rows(self.acl_table, fake_acls)
        # Associate switches and acls
        self._construct_ovsdb_references(
            TestNBImplIdlOvn.fake_associations['lstoacl'],
            self.lswitch_table, self.acl_table,
            'name', 'unit_test_id', 'acls')
        # Load dhcp options
        fake_dhcp_options = TestNBImplIdlOvn.fake_set['dhcp_options']
        self._load_ovsdb_fake_rows(self.dhcp_table, fake_dhcp_options)
        # Load address sets
        fake_address_sets = TestNBImplIdlOvn.fake_set['address_sets']
        self._load_ovsdb_fake_rows(self.address_set_table, fake_address_sets)

    @mock.patch.object(impl_idl_ovn.OvsdbNbOvnIdl, 'ovsdb_connection', None)
    @mock.patch.object(impl_idl_ovn, 'get_connection', mock.Mock())
    def test_setting_ovsdb_probe_timeout_default_value(self):
        inst = impl_idl_ovn.OvsdbNbOvnIdl(mock.Mock())
        inst.idl._session.reconnect.set_probe_interval.assert_called_with(
            60000)

    @mock.patch.object(impl_idl_ovn.OvsdbNbOvnIdl, 'ovsdb_connection', None)
    @mock.patch.object(impl_idl_ovn, 'get_connection', mock.Mock())
    @mock.patch.object(config, 'get_ovn_ovsdb_probe_interval')
    def test_setting_ovsdb_probe_timeout(self, mock_get_probe_interval):
        mock_get_probe_interval.return_value = 5000
        inst = impl_idl_ovn.OvsdbNbOvnIdl(mock.Mock())
        inst.idl._session.reconnect.set_probe_interval.assert_called_with(5000)

    def test_get_all_logical_switches_with_ports(self):
        # Test empty
        mapping = self.nb_ovn_idl.get_all_logical_switches_with_ports()
        self.assertItemsEqual(mapping, {})
        # Test loaded values
        self._load_nb_db()
        mapping = self.nb_ovn_idl.get_all_logical_switches_with_ports()
        expected = [{'name': utils.ovn_name('ls-id-1'),
                     'ports': ['lsp-id-11', 'lsp-id-12', 'lsp-rp-id-1'],
                     'provnet_port': 'provnet-ls-id-1'},
                    {'name': utils.ovn_name('ls-id-2'),
                     'ports': ['lsp-id-21', 'lsp-rp-id-2'],
                     'provnet_port': 'provnet-ls-id-2'},
                    {'name': utils.ovn_name('ls-id-3'),
                     'ports': ['lsp-id-31', 'lsp-id-32', 'lsp-rp-id-3',
                               'lsp-vpn-id-3'],
                     'provnet_port': None},
                    {'name': utils.ovn_name('ls-id-5'),
                     'ports': ['lsp-id-51', 'lsp-id-52', 'lsp-rp-id-5',
                               'lsp-vpn-id-5'],
                     'provnet_port': None}]
        self.assertItemsEqual(mapping, expected)

    def test_get_all_logical_routers_with_rports(self):
        # Test empty
        mapping = self.nb_ovn_idl.get_all_logical_switches_with_ports()
        self.assertItemsEqual(mapping, {})
        # Test loaded values
        self._load_nb_db()
        mapping = self.nb_ovn_idl.get_all_logical_routers_with_rports()
        expected = [{'name': 'lr-id-a',
                     'ports': {'orp-id-a1': ['10.0.1.0/24'],
                               'orp-id-a2': ['10.0.2.0/24'],
                               'orp-id-a3': ['10.0.3.0/24']},
                     'static_routes': [{'destination': '20.0.0.0/16',
                                        'nexthop': '10.0.3.253'}],
                     'snats': [{'external_ip': '10.0.3.1',
                                'logical_ip': '20.0.0.0/16',
                                'type': 'snat'}],
                     'dnat_and_snats': []},
                    {'name': 'lr-id-b',
                     'ports': {'xrp-id-b1': ['20.0.1.0/24'],
                               'orp-id-b2': ['20.0.2.0/24']},
                     'static_routes': [{'destination': '10.0.0.0/16',
                                        'nexthop': '20.0.2.253'}],
                     'snats': [{'external_ip': '20.0.2.1',
                                'logical_ip': '10.0.0.0/24',
                                'type': 'snat'}],
                     'dnat_and_snats': [{'external_ip': '20.0.2.4',
                                         'logical_ip': '10.0.0.4',
                                         'type': 'dnat_and_snat'},
                                        {'external_ip': '20.0.2.5',
                                         'logical_ip': '10.0.0.5',
                                         'type': 'dnat_and_snat',
                                         'external_mac': '00:01:02:03:04:05',
                                         'logical_port': 'lsp-id-001'}]},
                    {'name': 'lr-id-c', 'ports': {}, 'static_routes': [],
                     'snats': [], 'dnat_and_snats': []},
                    {'name': 'lr-id-d', 'ports': {}, 'static_routes': [],
                     'snats': [], 'dnat_and_snats': []},
                    {'name': 'lr-id-e', 'ports': {}, 'static_routes': [],
                     'snats': [], 'dnat_and_snats': []}]
        self.assertItemsEqual(mapping, expected)

    def test_get_acls_for_lswitches(self):
        self._load_nb_db()
        # Test neutron switches
        lswitches = ['ls-id-1', 'ls-id-2', 'ls-id-3', 'ls-id-5']
        acl_values, acl_objs, lswitch_ovsdb_dict = \
            self.nb_ovn_idl.get_acls_for_lswitches(lswitches)
        excepted_acl_values = {
            'lsp-id-11': [
                {'action': 'allow-related', 'lport': 'lsp-id-11',
                 'lswitch': 'neutron-ls-id-1',
                 'external_ids': {'neutron:lport': 'lsp-id-11'},
                 'direction': 'from-lport',
                 'match': 'inport == "lsp-id-11" && ip4'},
                {'action': 'allow-related', 'lport': 'lsp-id-11',
                 'lswitch': 'neutron-ls-id-1',
                 'external_ids': {'neutron:lport': 'lsp-id-11'},
                 'direction': 'to-lport',
                 'match': 'outport == "lsp-id-11" && ip4.src == $as_ip4_id_1'}
                ],
            'lsp-id-12': [
                {'action': 'allow-related', 'lport': 'lsp-id-12',
                 'lswitch': 'neutron-ls-id-1',
                 'external_ids': {'neutron:lport': 'lsp-id-12'},
                 'direction': 'from-lport',
                 'match': 'inport == "lsp-id-12" && ip4'},
                {'action': 'allow-related', 'lport': 'lsp-id-12',
                 'lswitch': 'neutron-ls-id-1',
                 'external_ids': {'neutron:lport': 'lsp-id-12'},
                 'direction': 'to-lport',
                 'match': 'outport == "lsp-id-12" && ip4.src == $as_ip4_id_1'}
                ],
            'lsp-id-21': [
                {'action': 'allow-related', 'lport': 'lsp-id-21',
                 'lswitch': 'neutron-ls-id-2',
                 'external_ids': {'neutron:lport': 'lsp-id-21'},
                 'direction': 'from-lport',
                 'match': 'inport == "lsp-id-21" && ip4'},
                {'action': 'allow-related', 'lport': 'lsp-id-21',
                 'lswitch': 'neutron-ls-id-2',
                 'external_ids': {'neutron:lport': 'lsp-id-21'},
                 'direction': 'to-lport',
                 'match': 'outport == "lsp-id-21" && ip4.src == $as_ip4_id_2'}
                ],
            'lsp-id-52': [
                {'action': 'allow-related', 'lport': 'lsp-id-52',
                 'lswitch': 'neutron-ls-id-5',
                 'external_ids': {'neutron:lport': 'lsp-id-52'},
                 'direction': 'from-lport',
                 'match': 'inport == "lsp-id-52" && ip4'},
                {'action': 'allow-related', 'lport': 'lsp-id-52',
                 'lswitch': 'neutron-ls-id-5',
                 'external_ids': {'neutron:lport': 'lsp-id-52'},
                 'direction': 'to-lport',
                 'match': 'outport == "lsp-id-52" && ip4.src == $as_ip4_id_5'}
                ]}
        self.assertItemsEqual(acl_values, excepted_acl_values)
        self.assertEqual(len(acl_objs), 8)
        self.assertEqual(len(lswitch_ovsdb_dict), len(lswitches))

        # Test non-neutron switches
        lswitches = ['ls-id-4']
        acl_values, acl_objs, lswitch_ovsdb_dict = \
            self.nb_ovn_idl.get_acls_for_lswitches(lswitches)
        self.assertItemsEqual(acl_values, {})
        self.assertEqual(len(acl_objs), 0)
        self.assertEqual(len(lswitch_ovsdb_dict), 0)

    def test_get_all_chassis_gateway_bindings(self):
        self._load_nb_db()
        bindings = self.nb_ovn_idl.get_all_chassis_gateway_bindings()
        expected = {'host-1': [utils.ovn_lrouter_port_name('orp-id-a1'),
                               utils.ovn_lrouter_port_name('orp-id-a2')],
                    'host-2': [utils.ovn_lrouter_port_name('orp-id-b2')],
                    ovn_const.OVN_GATEWAY_INVALID_CHASSIS: [
                        utils.ovn_name('orp-id-a3')]}
        self.assertItemsEqual(bindings, expected)

        bindings = self.nb_ovn_idl.get_all_chassis_gateway_bindings([])
        self.assertItemsEqual(bindings, expected)

        bindings = self.nb_ovn_idl.get_all_chassis_gateway_bindings(['host-1'])
        expected = {'host-1': [utils.ovn_lrouter_port_name('orp-id-a1'),
                               utils.ovn_lrouter_port_name('orp-id-a2')]}
        self.assertItemsEqual(bindings, expected)

    def test_get_gateway_chassis_binding(self):
        self._load_nb_db()
        chassis = self.nb_ovn_idl.get_gateway_chassis_binding(
            utils.ovn_lrouter_port_name('orp-id-a1'))
        self.assertEqual(chassis, ['host-1'])
        chassis = self.nb_ovn_idl.get_gateway_chassis_binding(
            utils.ovn_lrouter_port_name('orp-id-b2'))
        self.assertEqual(chassis, ['host-2'])
        chassis = self.nb_ovn_idl.get_gateway_chassis_binding(
            utils.ovn_lrouter_port_name('orp-id-a3'))
        self.assertEqual(chassis, ['neutron-ovn-invalid-chassis'])
        chassis = self.nb_ovn_idl.get_gateway_chassis_binding(
            utils.ovn_lrouter_port_name('orp-id-b3'))
        self.assertEqual(chassis, [])
        chassis = self.nb_ovn_idl.get_gateway_chassis_binding('bad')
        self.assertEqual(chassis, [])

    def test_get_unhosted_gateways(self):
        self._load_nb_db()
        # Test only host-1 in the valid list
        unhosted_gateways = self.nb_ovn_idl.get_unhosted_gateways(
            {}, {'host-1': 'physnet1'}, [])
        expected = {
            utils.ovn_lrouter_port_name('orp-id-b2'): {
                ovn_const.OVN_GATEWAY_CHASSIS_KEY: 'host-2'},
            utils.ovn_lrouter_port_name('orp-id-a3'): {
                ovn_const.OVN_GATEWAY_CHASSIS_KEY:
                    ovn_const.OVN_GATEWAY_INVALID_CHASSIS}}
        self.assertItemsEqual(unhosted_gateways, expected)
        # Test both host-1, host-2 in valid list
        unhosted_gateways = self.nb_ovn_idl.get_unhosted_gateways(
            {}, {'host-1': 'physnet1', 'host-2': 'physnet2'}, [])
        expected = {utils.ovn_lrouter_port_name('orp-id-a3'): {
            ovn_const.OVN_GATEWAY_CHASSIS_KEY:
                ovn_const.OVN_GATEWAY_INVALID_CHASSIS}}
        self.assertItemsEqual(unhosted_gateways, expected)
        # Schedule unhosted_gateways on host-2
        for unhosted_gateway in unhosted_gateways:
            router_row = self._find_ovsdb_fake_row(self.lrp_table,
                                                   'name', unhosted_gateway)
            setattr(router_row, 'options', {
                ovn_const.OVN_GATEWAY_CHASSIS_KEY: 'host-2'})
        unhosted_gateways = self.nb_ovn_idl.get_unhosted_gateways(
            {}, {'host-1': 'physnet1', 'host-2': 'physnet2'}, [])
        self.assertItemsEqual(unhosted_gateways, {})

    def test_get_subnet_dhcp_options(self):
        self._load_nb_db()
        subnet_options = self.nb_ovn_idl.get_subnet_dhcp_options(
            'subnet-id-10-0-2-0')
        expected_row = self._find_ovsdb_fake_row(self.dhcp_table,
                                                 'cidr', '10.0.2.0/24')
        self.assertEqual({
            'subnet': {'cidr': expected_row.cidr,
                       'external_ids': expected_row.external_ids,
                       'options': expected_row.options,
                       'uuid': expected_row.uuid},
            'ports': []}, subnet_options)
        subnet_options = self.nb_ovn_idl.get_subnet_dhcp_options(
            'subnet-id-11-0-2-0')['subnet']
        self.assertIsNone(subnet_options)
        subnet_options = self.nb_ovn_idl.get_subnet_dhcp_options(
            'port-id-30-0-1-0')['subnet']
        self.assertIsNone(subnet_options)

    def test_get_subnet_dhcp_options_with_ports(self):
        # Test empty
        subnet_options = self.nb_ovn_idl.get_subnet_dhcp_options(
            'subnet-id-10-0-1-0', with_ports=True)
        self.assertItemsEqual({'subnet': None, 'ports': []}, subnet_options)
        # Test loaded values
        self._load_nb_db()
        # Test getting both subnet and port dhcp options
        subnet_options = self.nb_ovn_idl.get_subnet_dhcp_options(
            'subnet-id-10-0-1-0', with_ports=True)
        dhcp_rows = [
            self._find_ovsdb_fake_row(self.dhcp_table, 'cidr', '10.0.1.0/24'),
            self._find_ovsdb_fake_row(self.dhcp_table, 'cidr', '10.0.1.0/26')]
        expected_rows = [{'cidr': dhcp_row.cidr,
                          'external_ids': dhcp_row.external_ids,
                          'options': dhcp_row.options,
                          'uuid': dhcp_row.uuid} for dhcp_row in dhcp_rows]
        self.assertItemsEqual(expected_rows, [
            subnet_options['subnet']] + subnet_options['ports'])
        # Test getting only subnet dhcp options
        subnet_options = self.nb_ovn_idl.get_subnet_dhcp_options(
            'subnet-id-10-0-2-0', with_ports=True)
        dhcp_rows = [
            self._find_ovsdb_fake_row(self.dhcp_table, 'cidr', '10.0.2.0/24')]
        expected_rows = [{'cidr': dhcp_row.cidr,
                          'external_ids': dhcp_row.external_ids,
                          'options': dhcp_row.options,
                          'uuid': dhcp_row.uuid} for dhcp_row in dhcp_rows]
        self.assertItemsEqual(expected_rows, [
            subnet_options['subnet']] + subnet_options['ports'])
        # Test getting no dhcp options
        subnet_options = self.nb_ovn_idl.get_subnet_dhcp_options(
            'subnet-id-11-0-2-0', with_ports=True)
        self.assertItemsEqual({'subnet': None, 'ports': []}, subnet_options)

    def test_get_subnets_dhcp_options(self):
        self._load_nb_db()

        def get_row_dict(row):
            return {'cidr': row.cidr, 'external_ids': row.external_ids,
                    'options': row.options, 'uuid': row.uuid}

        subnets_options = self.nb_ovn_idl.get_subnets_dhcp_options(
            ['subnet-id-10-0-1-0', 'subnet-id-10-0-2-0'])
        expected_rows = [
            get_row_dict(
                self._find_ovsdb_fake_row(self.dhcp_table, 'cidr', cidr))
            for cidr in ('10.0.1.0/24', '10.0.2.0/24')]
        self.assertItemsEqual(expected_rows, subnets_options)

        subnets_options = self.nb_ovn_idl.get_subnets_dhcp_options(
            ['subnet-id-11-0-2-0', 'subnet-id-20-0-1-0'])
        expected_row = get_row_dict(
            self._find_ovsdb_fake_row(self.dhcp_table, 'cidr', '20.0.1.0/24'))
        self.assertItemsEqual([expected_row], subnets_options)

        subnets_options = self.nb_ovn_idl.get_subnets_dhcp_options(
            ['port-id-30-0-1-0', 'fake-not-exist'])
        self.assertEqual([], subnets_options)

    def test_get_all_dhcp_options(self):
        self._load_nb_db()
        dhcp_options = self.nb_ovn_idl.get_all_dhcp_options()
        self.assertEqual(len(dhcp_options['subnets']), 3)
        self.assertEqual(len(dhcp_options['ports_v4']), 2)

    def test_get_address_sets(self):
        self._load_nb_db()
        address_sets = self.nb_ovn_idl.get_address_sets()
        self.assertEqual(len(address_sets), 4)


class TestSBImplIdlOvn(TestDBImplIdlOvn):

    fake_set = {
        'chassis': [
            {'name': 'host-1', 'hostname': 'host-1.localdomain.com',
             'external_ids': {'ovn-bridge-mappings':
                              'public:br-ex,private:br-0'}},
            {'name': 'host-2', 'hostname': 'host-2.localdomain.com',
             'external_ids': {'ovn-bridge-mappings':
                              'public:br-ex,public2:br-ex'}},
            {'name': 'host-3', 'hostname': 'host-3.localdomain.com',
             'external_ids': {'ovn-bridge-mappings':
                              'public:br-ex'}},
            ]
        }

    def setUp(self):
        super(TestSBImplIdlOvn, self).setUp()

        self.chassis_table = fakes.FakeOvsdbTable.create_one_ovsdb_table()
        self._tables = {}
        self._tables['Chassis'] = self.chassis_table

        with mock.patch.object(impl_idl_ovn, 'get_connection',
                               return_value=mock.Mock()):
            impl_idl_ovn.OvsdbSbOvnIdl.ovsdb_connection = None
            self.sb_ovn_idl = impl_idl_ovn.OvsdbSbOvnIdl(mock.Mock())

        self.sb_ovn_idl.idl.tables = self._tables

    def _load_sb_db(self):
        # Load Chassis
        fake_chassis = TestSBImplIdlOvn.fake_set['chassis']
        self._load_ovsdb_fake_rows(self.chassis_table, fake_chassis)

    @mock.patch.object(impl_idl_ovn.OvsdbSbOvnIdl, 'ovsdb_connection', None)
    @mock.patch.object(impl_idl_ovn, 'get_connection', mock.Mock())
    def test_setting_ovsdb_probe_timeout_default_value(self):
        inst = impl_idl_ovn.OvsdbSbOvnIdl(mock.Mock())
        inst.idl._session.reconnect.set_probe_interval.assert_called_with(
            60000)

    @mock.patch.object(impl_idl_ovn.OvsdbSbOvnIdl, 'ovsdb_connection', None)
    @mock.patch.object(impl_idl_ovn, 'get_connection', mock.Mock())
    @mock.patch.object(config, 'get_ovn_ovsdb_probe_interval')
    def test_setting_ovsdb_probe_timeout(self, mock_get_probe_interval):
        mock_get_probe_interval.return_value = 5000
        inst = impl_idl_ovn.OvsdbSbOvnIdl(mock.Mock())
        inst.idl._session.reconnect.set_probe_interval.assert_called_with(5000)




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\ovsdb\test_ovsdb_monitor.py
===========File Type===========
.py
===========File Content===========
# Copyright 2016 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import copy
import os

import mock
from neutron_lib.plugins import constants
from neutron_lib.plugins import directory
from oslo_utils import uuidutils
from ovs.db import idl as ovs_idl
from ovs import poller
from ovs.stream import Stream
from ovsdbapp.backend.ovs_idl import connection
from ovsdbapp.backend.ovs_idl import idlutils

from networking_ovn.common import config as ovn_config
from networking_ovn.ovsdb import ovsdb_monitor
from networking_ovn.tests import base
from networking_ovn.tests.unit.ml2 import test_mech_driver

basedir = os.path.dirname(os.path.abspath(__file__))
schema_files = {
    'OVN_Northbound': os.path.join(basedir, 'schemas', 'ovn-nb.ovsschema'),
    'OVN_Southbound': os.path.join(basedir, 'schemas', 'ovn-sb.ovsschema'),
}

OVN_NB_SCHEMA = {
    "name": "OVN_Northbound", "version": "3.0.0",
    "tables": {
        "Logical_Switch_Port": {
            "columns": {
                "name": {"type": "string"},
                "type": {"type": "string"},
                "addresses": {"type": {"key": "string",
                                       "min": 0,
                                       "max": "unlimited"}},
                "port_security": {"type": {"key": "string",
                                           "min": 0,
                                           "max": "unlimited"}},
                "up": {"type": {"key": "boolean", "min": 0, "max": 1}}},
            "indexes": [["name"]],
            "isRoot": False,
        },
        "Logical_Switch": {
            "columns": {"name": {"type": "string"}},
            "indexes": [["name"]],
            "isRoot": True,
        }
    }
}


OVN_SB_SCHEMA = {
    "name": "OVN_Southbound", "version": "1.3.0",
    "tables": {
        "Chassis": {
            "columns": {
                "name": {"type": "string"},
                "hostname": {"type": "string"},
                "external_ids": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}}},
            "isRoot": True,
            "indexes": [["name"]]
        }
    }
}


class TestOvnNbIdlNotifyHandler(test_mech_driver.OVNMechanismDriverTestCase):

    def setUp(self):
        super(TestOvnNbIdlNotifyHandler, self).setUp()
        helper = ovs_idl.SchemaHelper(schema_json=OVN_NB_SCHEMA)
        helper.register_all()
        self.idl = ovsdb_monitor.OvnNbIdl(self.driver, "remote", helper)
        self.idl.lock_name = self.idl.event_lock_name
        self.idl.has_lock = True
        self.lp_table = self.idl.tables.get('Logical_Switch_Port')
        self.driver.set_port_status_up = mock.Mock()
        self.driver.set_port_status_down = mock.Mock()

    def _test_lsp_helper(self, event, new_row_json, old_row_json=None,
                         table=None):
        row_uuid = uuidutils.generate_uuid()
        if not table:
            table = self.lp_table
        lp_row = ovs_idl.Row.from_json(self.idl, table,
                                       row_uuid, new_row_json)
        if old_row_json:
            old_row = ovs_idl.Row.from_json(self.idl, table,
                                            row_uuid, old_row_json)
        else:
            old_row = None
        self.idl.notify(event, lp_row, updates=old_row)
        # Add a STOP EVENT to the queue
        self.idl.notify_handler.shutdown()
        # Execute the notifications queued
        self.idl.notify_handler.notify_loop()

    def test_lsp_up_create_event(self):
        row_data = {"up": True, "name": "foo-name"}
        self._test_lsp_helper('create', row_data)
        self.driver.set_port_status_up.assert_called_once_with("foo-name")
        self.assertFalse(self.driver.set_port_status_down.called)

    def test_lsp_down_create_event(self):
        row_data = {"up": False, "name": "foo-name"}
        self._test_lsp_helper('create', row_data)
        self.driver.set_port_status_down.assert_called_once_with("foo-name")
        self.assertFalse(self.driver.set_port_status_up.called)

    def test_lsp_up_not_set_event(self):
        row_data = {"up": ['set', []], "name": "foo-name"}
        self._test_lsp_helper('create', row_data)
        self.assertFalse(self.driver.set_port_status_up.called)
        self.assertFalse(self.driver.set_port_status_down.called)

    def test_unwatch_logical_switch_port_create_events(self):
        self.idl.unwatch_logical_switch_port_create_events()
        row_data = {"up": True, "name": "foo-name"}
        self._test_lsp_helper('create', row_data)
        self.assertFalse(self.driver.set_port_status_up.called)
        self.assertFalse(self.driver.set_port_status_down.called)

        row_data["up"] = False
        self._test_lsp_helper('create', row_data)
        self.assertFalse(self.driver.set_port_status_up.called)
        self.assertFalse(self.driver.set_port_status_down.called)

    def test_post_connect(self):
        self.idl.post_connect()
        self.assertIsNone(self.idl._lsp_create_up_event)
        self.assertIsNone(self.idl._lsp_create_down_event)

    def test_lsp_up_update_event(self):
        new_row_json = {"up": True, "name": "foo-name"}
        old_row_json = {"up": False}
        self._test_lsp_helper('update', new_row_json,
                              old_row_json=old_row_json)
        self.driver.set_port_status_up.assert_called_once_with("foo-name")
        self.assertFalse(self.driver.set_port_status_down.called)

    def test_lsp_down_update_event(self):
        new_row_json = {"up": False, "name": "foo-name"}
        old_row_json = {"up": True}
        self._test_lsp_helper('update', new_row_json,
                              old_row_json=old_row_json)
        self.driver.set_port_status_down.assert_called_once_with("foo-name")
        self.assertFalse(self.driver.set_port_status_up.called)

    def test_lsp_up_update_event_no_old_data(self):
        new_row_json = {"up": True, "name": "foo-name"}
        self._test_lsp_helper('update', new_row_json,
                              old_row_json=None)
        self.assertFalse(self.driver.set_port_status_up.called)
        self.assertFalse(self.driver.set_port_status_down.called)

    def test_lsp_down_update_event_no_old_data(self):
        new_row_json = {"up": False, "name": "foo-name"}
        self._test_lsp_helper('update', new_row_json,
                              old_row_json=None)
        self.assertFalse(self.driver.set_port_status_up.called)
        self.assertFalse(self.driver.set_port_status_down.called)

    def test_lsp_other_column_update_event(self):
        new_row_json = {"up": False, "name": "foo-name",
                        "addresses": ["10.0.0.2"]}
        old_row_json = {"addresses": ["10.0.0.3"]}
        self._test_lsp_helper('update', new_row_json,
                              old_row_json=old_row_json)
        self.assertFalse(self.driver.set_port_status_up.called)
        self.assertFalse(self.driver.set_port_status_down.called)

    def test_notify_other_table(self):
        new_row_json = {"name": "foo-name"}
        self._test_lsp_helper('create', new_row_json,
                              table=self.idl.tables.get("Logical_Switch"))
        self.assertFalse(self.driver.set_port_status_up.called)
        self.assertFalse(self.driver.set_port_status_down.called)

    def test_notify_no_ovsdb_lock(self):
        self.idl.is_lock_contended = True
        self.idl.notify_handler.notify = mock.Mock()
        self.idl.notify("create", mock.ANY)
        self.assertFalse(self.idl.notify_handler.notify.called)

    def test_notify_ovsdb_lock_not_yet_contended(self):
        self.idl.is_lock_contended = False
        self.idl.notify_handler.notify = mock.Mock()
        self.idl.notify("create", mock.ANY)
        self.assertTrue(self.idl.notify_handler.notify.called)


class TestOvnSbIdlNotifyHandler(test_mech_driver.OVNMechanismDriverTestCase):

    l3_plugin = 'networking_ovn.l3.l3_ovn.OVNL3RouterPlugin'

    def setUp(self):
        super(TestOvnSbIdlNotifyHandler, self).setUp()
        sb_helper = ovs_idl.SchemaHelper(schema_json=OVN_SB_SCHEMA)
        sb_helper.register_table('Chassis')
        self.sb_idl = ovsdb_monitor.OvnSbIdl(self.driver, "remote", sb_helper)
        self.sb_idl.lock_name = self.sb_idl.event_lock_name
        self.sb_idl.has_lock = True
        self.sb_idl.post_connect()
        self.chassis_table = self.sb_idl.tables.get('Chassis')
        self.driver.update_segment_host_mapping = mock.Mock()
        self.l3_plugin = directory.get_plugin(constants.L3)
        self.l3_plugin.schedule_unhosted_gateways = mock.Mock()

        self.row_json = {
            "name": "fake-name",
            "hostname": "fake-hostname",
            "external_ids": ['map', [["ovn-bridge-mappings",
                                      "fake-phynet1:fake-br1"]]]
        }

    def _test_chassis_helper(self, event, new_row_json, old_row_json=None):
        row_uuid = uuidutils.generate_uuid()
        table = self.chassis_table
        row = ovs_idl.Row.from_json(self.sb_idl, table, row_uuid, new_row_json)
        if old_row_json:
            old_row = ovs_idl.Row.from_json(self.sb_idl, table,
                                            row_uuid, old_row_json)
        else:
            old_row = None
        self.sb_idl.notify(event, row, updates=old_row)
        # Add a STOP EVENT to the queue
        self.sb_idl.notify_handler.shutdown()
        # Execute the notifications queued
        self.sb_idl.notify_handler.notify_loop()

    def test_chassis_create_event(self):
        self._test_chassis_helper('create', self.row_json)
        self.driver.update_segment_host_mapping.assert_called_once_with(
            'fake-hostname', ['fake-phynet1'])
        self.assertEqual(
            1,
            self.l3_plugin.schedule_unhosted_gateways.call_count)

    def test_chassis_delete_event(self):
        self._test_chassis_helper('delete', self.row_json)
        self.driver.update_segment_host_mapping.assert_called_once_with(
            'fake-hostname', [])
        self.assertEqual(
            1,
            self.l3_plugin.schedule_unhosted_gateways.call_count)

    def test_chassis_update_event(self):
        old_row_json = copy.deepcopy(self.row_json)
        old_row_json['external_ids'][1][0][1] = (
            "fake-phynet2:fake-br2")
        self._test_chassis_helper('update', self.row_json, old_row_json)
        self.driver.update_segment_host_mapping.assert_called_once_with(
            'fake-hostname', ['fake-phynet1'])
        self.assertEqual(
            1,
            self.l3_plugin.schedule_unhosted_gateways.call_count)


class TestOvnDbNotifyHandler(base.TestCase):

    def setUp(self):
        super(TestOvnDbNotifyHandler, self).setUp()
        self.handler = ovsdb_monitor.OvnDbNotifyHandler(mock.ANY)
        self.watched_events = self.handler._RowEventHandler__watched_events

    def test_watch_and_unwatch_events(self):
        expected_events = set()
        networking_event = mock.Mock()
        ovn_event = mock.Mock()
        unknown_event = mock.Mock()

        self.assertItemsEqual(set(), self.watched_events)

        expected_events.add(networking_event)
        self.handler.watch_event(networking_event)
        self.assertItemsEqual(expected_events, self.watched_events)

        expected_events.add(ovn_event)
        self.handler.watch_events([ovn_event])
        self.assertItemsEqual(expected_events, self.watched_events)

        self.handler.unwatch_events([networking_event, ovn_event])
        self.handler.unwatch_event(unknown_event)
        self.handler.unwatch_events([unknown_event])
        self.assertItemsEqual(set(), self.watched_events)

    def test_shutdown(self):
        self.handler.shutdown()


# class TestOvnBaseConnection(base.TestCase):
#
# Each test is being deleted, but for reviewers sake I wanted to exaplain why:
#
#     @mock.patch.object(idlutils, 'get_schema_helper')
#     def testget_schema_helper_success(self, mock_gsh):
#
# 1. OvnBaseConnection and OvnConnection no longer exist
# 2. get_schema_helper is no longer a part of the Connection class
#
#     @mock.patch.object(idlutils, 'get_schema_helper')
#     def testget_schema_helper_initial_exception(self, mock_gsh):
#
#     @mock.patch.object(idlutils, 'get_schema_helper')
#     def testget_schema_helper_all_exception(self, mock_gsh):
#
# 3. The only reason get_schema_helper had a retry loop was for Neutron's
#    use case of trying to set the Manager to listen on ptcp:127.0.0.1:6640
#    if it wasn't already set up. Since that code being removed was the whole
#    reason to re-implement get_schema_helper here,the exception retry is not
#    needed and therefor is not a part of ovsdbapp's implementation of
#    idlutils.get_schema_helper which we now use directly in from_server()
# 4. These tests now would be testing the various from_server() calls, but
#    there is almost nothing to test in those except maybe SSL being set up
#    but that was done below.

class TestOvnConnection(base.TestCase):

    def setUp(self):
        super(TestOvnConnection, self).setUp()

    @mock.patch.object(idlutils, 'get_schema_helper')
    @mock.patch.object(idlutils, 'wait_for_change')
    def _test_connection_start(self, mock_wfc, mock_gsh,
                               idl_class, schema):
        mock_gsh.return_value = ovs_idl.SchemaHelper(
            location=schema_files[schema])
        _idl = idl_class.from_server('punix:/tmp/fake', schema, mock.Mock())
        self.ovn_connection = connection.Connection(_idl, mock.Mock())
        with mock.patch.object(poller, 'Poller'), \
            mock.patch('threading.Thread'):
            self.ovn_connection.start()
            # A second start attempt shouldn't re-register.
            self.ovn_connection.start()

        self.ovn_connection.thread.start.assert_called_once_with()

    def test_connection_nb_start(self):
        ovn_config.cfg.CONF.set_override('ovn_nb_private_key', 'foo-key',
                                         'ovn')
        Stream.ssl_set_private_key_file = mock.Mock()
        Stream.ssl_set_certificate_file = mock.Mock()
        Stream.ssl_set_ca_cert_file = mock.Mock()

        self._test_connection_start(idl_class=ovsdb_monitor.OvnNbIdl,
                                    schema='OVN_Northbound')

        Stream.ssl_set_private_key_file.assert_called_once_with('foo-key')
        Stream.ssl_set_certificate_file.assert_not_called()
        Stream.ssl_set_ca_cert_file.assert_not_called()

    def test_connection_sb_start(self):
        self._test_connection_start(idl_class=ovsdb_monitor.OvnSbIdl,
                                    schema='OVN_Southbound')




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\ovsdb\__init__.py
===========File Type===========
.py
===========File Content===========




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\ovsdb\schemas\ovn-nb.ovsschema
===========File Type===========
.ovsschema
===========File Content===========
{
    "name": "OVN_Northbound",
    "version": "5.5.0",
    "cksum": "2099428463 14236",
    "tables": {
        "NB_Global": {
            "columns": {
                "nb_cfg": {"type": {"key": "integer"}},
                "sb_cfg": {"type": {"key": "integer"}},
                "hv_cfg": {"type": {"key": "integer"}},
                "external_ids": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}},
                "connections": {
                    "type": {"key": {"type": "uuid",
                                     "refTable": "Connection"},
                                     "min": 0,
                                     "max": "unlimited"}},
                "ssl": {
                    "type": {"key": {"type": "uuid",
                                     "refTable": "SSL"},
                                     "min": 0, "max": 1}}},
            "maxRows": 1,
            "isRoot": true},
        "Logical_Switch": {
            "columns": {
                "name": {"type": "string"},
                "ports": {"type": {"key": {"type": "uuid",
                                           "refTable": "Logical_Switch_Port",
                                           "refType": "strong"},
                                   "min": 0,
                                   "max": "unlimited"}},
                "acls": {"type": {"key": {"type": "uuid",
                                          "refTable": "ACL",
                                          "refType": "strong"},
                                  "min": 0,
                                  "max": "unlimited"}},
                "qos_rules": {"type": {"key": {"type": "uuid",
                                          "refTable": "QoS",
                                          "refType": "strong"},
                                  "min": 0,
                                  "max": "unlimited"}},
                "load_balancer": {"type": {"key": {"type": "uuid",
                                                  "refTable": "Load_Balancer",
                                                  "refType": "strong"},
                                           "min": 0,
                                           "max": "unlimited"}},
                "other_config": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}},
                "external_ids": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}}},
            "isRoot": true},
        "Logical_Switch_Port": {
            "columns": {
                "name": {"type": "string"},
                "type": {"type": "string"},
                "options": {
                     "type": {"key": "string",
                              "value": "string",
                              "min": 0,
                              "max": "unlimited"}},
                "parent_name": {"type": {"key": "string", "min": 0, "max": 1}},
                "tag_request": {
                     "type": {"key": {"type": "integer",
                                      "minInteger": 0,
                                      "maxInteger": 4095},
                              "min": 0, "max": 1}},
                "tag": {
                     "type": {"key": {"type": "integer",
                                      "minInteger": 1,
                                      "maxInteger": 4095},
                              "min": 0, "max": 1}},
                "addresses": {"type": {"key": "string",
                                       "min": 0,
                                       "max": "unlimited"}},
                "dynamic_addresses": {"type": {"key": "string",
                                       "min": 0,
                                       "max": 1}},
                "port_security": {"type": {"key": "string",
                                           "min": 0,
                                           "max": "unlimited"}},
                "up": {"type": {"key": "boolean", "min": 0, "max": 1}},
                "enabled": {"type": {"key": "boolean", "min": 0, "max": 1}},
                "dhcpv4_options": {"type": {"key": {"type": "uuid",
                                            "refTable": "DHCP_Options",
                                            "refType": "weak"},
                                 "min": 0,
                                 "max": 1}},
                "dhcpv6_options": {"type": {"key": {"type": "uuid",
                                            "refTable": "DHCP_Options",
                                            "refType": "weak"},
                                 "min": 0,
                                 "max": 1}},
                "external_ids": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}}},
            "indexes": [["name"]],
            "isRoot": false},
        "Address_Set": {
            "columns": {
                "name": {"type": "string"},
                "addresses": {"type": {"key": "string",
                                       "min": 0,
                                       "max": "unlimited"}},
                "external_ids": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}}},
            "indexes": [["name"]],
            "isRoot": true},
        "Load_Balancer": {
            "columns": {
		"name": {"type": "string"},
                "vips": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}},
                "protocol": {
                    "type": {"key": {"type": "string",
                             "enum": ["set", ["tcp", "udp"]]},
                             "min": 0, "max": 1}},
                "external_ids": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}}},
            "isRoot": true},
        "ACL": {
            "columns": {
                "priority": {"type": {"key": {"type": "integer",
                                              "minInteger": 0,
                                              "maxInteger": 32767}}},
                "direction": {"type": {"key": {"type": "string",
                                            "enum": ["set", ["from-lport", "to-lport"]]}}},
                "match": {"type": "string"},
                "action": {"type": {"key": {"type": "string",
                                            "enum": ["set", ["allow", "allow-related", "drop", "reject"]]}}},
                "log": {"type": "boolean"},
                "external_ids": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}}},
            "isRoot": false},
        "QoS": {
            "columns": {
                "priority": {"type": {"key": {"type": "integer",
                                              "minInteger": 0,
                                              "maxInteger": 32767}}},
                "direction": {"type": {"key": {"type": "string",
                                            "enum": ["set", ["from-lport", "to-lport"]]}}},
                "match": {"type": "string"},
                "action": {"type": {"key": {"type": "string",
                                            "enum": ["set", ["dscp"]]},
                                    "value": {"type": "integer",
                                              "minInteger": 0,
                                              "maxInteger": 63}}},
                "external_ids": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}}},
            "isRoot": false},
        "Logical_Router": {
            "columns": {
                "name": {"type": "string"},
                "ports": {"type": {"key": {"type": "uuid",
                                           "refTable": "Logical_Router_Port",
                                           "refType": "strong"},
                                   "min": 0,
                                   "max": "unlimited"}},
                "static_routes": {"type": {"key": {"type": "uuid",
                                            "refTable": "Logical_Router_Static_Route",
                                            "refType": "strong"},
                                   "min": 0,
                                   "max": "unlimited"}},
                "enabled": {"type": {"key": "boolean", "min": 0, "max": 1}},
                "nat": {"type": {"key": {"type": "uuid",
                                         "refTable": "NAT",
                                         "refType": "strong"},
                                 "min": 0,
                                 "max": "unlimited"}},
                "load_balancer": {"type": {"key": {"type": "uuid",
                                                  "refTable": "Load_Balancer",
                                                  "refType": "strong"},
                                           "min": 0,
                                           "max": "unlimited"}},
                "options": {
                     "type": {"key": "string",
                              "value": "string",
                              "min": 0,
                              "max": "unlimited"}},
                "external_ids": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}}},
            "isRoot": true},
        "Logical_Router_Port": {
            "columns": {
                "name": {"type": "string"},
                "options": {
                    "type": {"key": "string",
                             "value": "string",
                             "min": 0,
                             "max": "unlimited"}},
                "networks": {"type": {"key": "string",
                                      "min": 1,
                                      "max": "unlimited"}},
                "mac": {"type": "string"},
                "peer": {"type": {"key": "string", "min": 0, "max": 1}},
                "enabled": {"type": {"key": "boolean", "min": 0, "max": 1}},
                "external_ids": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}}},
            "indexes": [["name"]],
            "isRoot": false},
        "Logical_Router_Static_Route": {
            "columns": {
                "ip_prefix": {"type": "string"},
                "policy": {"type": {"key": {"type": "string",
                                            "enum": ["set", ["src-ip",
                                                             "dst-ip"]]},
                                    "min": 0, "max": 1}},
                "nexthop": {"type": "string"},
                "output_port": {"type": {"key": "string", "min": 0, "max": 1}}},
            "isRoot": false},
        "NAT": {
            "columns": {
                "external_ip": {"type": "string"},
                "external_mac": {"type": {"key": "string",
                                          "min": 0, "max": 1}},
                "logical_ip": {"type": "string"},
                "logical_port": {"type": {"key": "string",
                                          "min": 0, "max": 1}},
                "type": {"type": {"key": {"type": "string",
                                           "enum": ["set", ["dnat",
                                                             "snat",
                                                             "dnat_and_snat"
                                                               ]]}}}},
            "isRoot": false},
        "DHCP_Options": {
            "columns": {
                "cidr": {"type": "string"},
                "options": {"type": {"key": "string", "value": "string",
                                     "min": 0, "max": "unlimited"}},
                "external_ids": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}}},
            "isRoot": true},
        "Connection": {
            "columns": {
                "target": {"type": "string"},
                "max_backoff": {"type": {"key": {"type": "integer",
                                         "minInteger": 1000},
                                         "min": 0,
                                         "max": 1}},
                "inactivity_probe": {"type": {"key": "integer",
                                              "min": 0,
                                              "max": 1}},
                "other_config": {"type": {"key": "string",
                                          "value": "string",
                                          "min": 0,
                                          "max": "unlimited"}},
                "external_ids": {"type": {"key": "string",
                                 "value": "string",
                                 "min": 0,
                                 "max": "unlimited"}},
                "is_connected": {"type": "boolean", "ephemeral": true},
                "status": {"type": {"key": "string",
                                    "value": "string",
                                    "min": 0,
                                    "max": "unlimited"},
                                    "ephemeral": true}},
            "indexes": [["target"]]},
        "SSL": {
            "columns": {
                "private_key": {"type": "string"},
                "certificate": {"type": "string"},
                "ca_cert": {"type": "string"},
                "bootstrap_ca_cert": {"type": "boolean"},
                "external_ids": {"type": {"key": "string",
                                          "value": "string",
                                          "min": 0,
                                          "max": "unlimited"}}},
            "maxRows": 1}}}




===========Repository Name===========
networking-ovn
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\networking-ovn\networking_ovn\tests\unit\ovsdb\schemas\ovn-sb.ovsschema
===========File Type===========
.ovsschema
===========File Content===========
{
    "name": "OVN_Southbound",
    "version": "1.10.0",
    "cksum": "860871483 9898",
    "tables": {
        "SB_Global": {
            "columns": {
                "nb_cfg": {"type": {"key": "integer"}},
                "external_ids": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}},
                "connections": {
                    "type": {"key": {"type": "uuid",
                                     "refTable": "Connection"},
                                     "min": 0,
                                     "max": "unlimited"}},
                "ssl": {
                    "type": {"key": {"type": "uuid",
                                     "refTable": "SSL"},
                                     "min": 0, "max": 1}}},
            "maxRows": 1,
            "isRoot": true},
        "Chassis": {
            "columns": {
                "name": {"type": "string"},
                "hostname": {"type": "string"},
                "encaps": {"type": {"key": {"type": "uuid",
                                            "refTable": "Encap"},
                                    "min": 1, "max": "unlimited"}},
                "vtep_logical_switches" : {"type": {"key": "string",
                                                    "min": 0,
                                                    "max": "unlimited"}},
                "nb_cfg": {"type": {"key": "integer"}},
                "external_ids": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}}},
            "isRoot": true,
            "indexes": [["name"]]},
        "Encap": {
            "columns": {
                "type": {"type": {"key": {
                           "type": "string",
                           "enum": ["set", ["geneve", "stt", "vxlan"]]}}},
                "options": {"type": {"key": "string",
                                     "value": "string",
                                     "min": 0,
                                     "max": "unlimited"}},
                "ip": {"type": "string"}}},
        "Address_Set": {
            "columns": {
                "name": {"type": "string"},
                "addresses": {"type": {"key": "string",
                                       "min": 0,
                                       "max": "unlimited"}}},
            "indexes": [["name"]],
            "isRoot": true},
        "Logical_Flow": {
            "columns": {
                "logical_datapath": {"type": {"key": {"type": "uuid",
                                                      "refTable": "Datapath_Binding"}}},
                "pipeline": {"type": {"key": {"type": "string",
                                      "enum": ["set", ["ingress",
                                                       "egress"]]}}},
                "table_id": {"type": {"key": {"type": "integer",
                                              "minInteger": 0,
                                              "maxInteger": 15}}},
                "priority": {"type": {"key": {"type": "integer",
                                              "minInteger": 0,
                                              "maxInteger": 65535}}},
                "match": {"type": "string"},
                "actions": {"type": "string"},
                "external_ids": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}}},
            "isRoot": true},
        "Multicast_Group": {
            "columns": {
                "datapath": {"type": {"key": {"type": "uuid",
                                              "refTable": "Datapath_Binding"}}},
                "name": {"type": "string"},
                "tunnel_key": {
                    "type": {"key": {"type": "integer",
                                     "minInteger": 32768,
                                     "maxInteger": 65535}}},
                "ports": {"type": {"key": {"type": "uuid",
                                           "refTable": "Port_Binding",
                                           "refType": "weak"},
                                   "min": 1, "max": "unlimited"}}},
            "indexes": [["datapath", "tunnel_key"],
                        ["datapath", "name"]],
            "isRoot": true},
        "Datapath_Binding": {
            "columns": {
                "tunnel_key": {
                     "type": {"key": {"type": "integer",
                                      "minInteger": 1,
                                      "maxInteger": 16777215}}},
                "external_ids": {
                    "type": {"key": "string", "value": "string",
                             "min": 0, "max": "unlimited"}}},
            "indexes": [["tunnel_key"]],
            "isRoot": true},
        "Port_Binding": {
            "columns": {
                "logical_port": {"type": "string"},
                "type": {"type": "string"},
                "options": {
                     "type": {"key": "string",
                              "value": "string",
                              "min": 0,
                              "max": "unlimited"}},
                "datapath": {"type": {"key": {"type": "uuid",
                                              "refTable": "Datapath_Binding"}}},
                "tunnel_key": {
                     "type": {"key": {"type": "integer",
                                      "minInteger": 1,
                                      "maxInteger": 32767}}},
                "parent_port": {"type": {"key": "string", "min": 0, "max": 1}},
                "tag": {
                     "type": {"key": {"type": "integer",
                                      "minInteger": 1,
                                      "maxInteger": 4095},
                              "min": 0, "max": 1}},
                "chassis": {"type": {"key": {"type": "uuid",
                                             "refTable": "Chassis",
                                             "refType": "weak"},
                                     "min": 0, "max": 1}},
                "mac": {"type": {"key": "string",
                                 "min": 0,
                                 "max": "unlimited"}},
                "nat_addresses": {"type": {"key": "string",
                                           "min": 0,
                                           "max": "unlimited"}}},
            "indexes": [["datapath", "tunnel_key"], ["logical_port"]],
            "isRoot": true},
        "MAC_Binding": {
            "columns": {
                "logical_port": {"type": "string"},
                "ip": {"type": "string"},
                "mac": {"type": "string"},
                "datapath": {"type": {"key": {"type": "uuid",
                                              "refTable": "Datapath_Binding"}}}},
            "indexes": [["logical_port", "ip"]],
            "isRoot": true},
        "DHCP_Options": {
            "columns": {
                "name": {"type": "string"},
                "code": {
                    "type": {"key": {"type": "integer",
                                     "minInteger": 0, "maxInteger": 254}}},
                "type": {
                    "type": {"key": {
                        "type": "string",
                        "enum": ["set", ["bool", "uint8", "uint16", "uint32",
                                         "ipv4", "static_routes", "str"]]}}}},
            "isRoot": true},
        "DHCPv6_Options": {
            "columns": {
                "name": {"type": "string"},
                "code": {
                    "type": {"key": {"type": "integer",
                                     "minInteger": 0, "maxInteger": 254}}},
                "type": {
                    "type": {"key": {
                        "type": "string",
                        "enum": ["set", ["ipv6", "str", "mac"]]}}}},
            "isRoot": true},
        "Connection": {
            "columns": {
                "target": {"type": "string"},
                "max_backoff": {"type": {"key": {"type": "integer",
                                         "minInteger": 1000},
                                         "min": 0,
                                         "max": 1}},
                "inactivity_probe": {"type": {"key": "integer",
                                              "min": 0,
                                              "max": 1}},
                "read_only": {"type": "boolean"},
                "other_config": {"type": {"key": "string",
                                          "value": "string",
                                          "min": 0,
                                          "max": "unlimited"}},
                "external_ids": {"type": {"key": "string",
                                 "value": "string",
                                 "min": 0,
                                 "max": "unlimited"}},
                "is_connected": {"type": "boolean", "ephemeral": true},
                "status": {"type": {"key": "string",
                                    "value": "string",
                                    "min": 0,
                                    "max": "unlimited"},
                                    "ephemeral": true}},
            "indexes": [["target"]]},
        "SSL": {
            "columns": {
                "private_key": {"type": "string"},
                "certificate": {"type": "string"},
                "ca_cert": {"type": "string"},
                "bootstrap_ca_cert": {"type": "boolean"},
                "external_ids": {"type": {"key": "string",
                                          "value": "string",
                                          "min": 0,
                                          "max": "unlimited"}}},
            "maxRows": 1}}}




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tox.ini
===========File Type===========
.ini
===========File Content===========
[tox]
minversion = 2.0
skipsdist = True
envlist = linters,docs,releasenotes,inventory,py3-inventory


[testenv]
usedevelop = True
basepython = python2.7
install_command =
    pip install -c{env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt} {opts} {packages}
deps =
    -r{toxinidir}/global-requirement-pins.txt
    -r{toxinidir}/test-requirements.txt
passenv =
    COMMON_TESTS_PATH
    HOME
    http_proxy
    HTTP_PROXY
    https_proxy
    HTTPS_PROXY
    no_proxy
    NO_PROXY
    TESTING_BRANCH
    TESTING_HOME
    USER
whitelist_externals =
    bash
setenv =
    PYTHONUNBUFFERED=1
    PYTHONWARNINGS=default::DeprecationWarning
    VIRTUAL_ENV={envdir}
    WORKING_DIR={toxinidir}
    ANSIBLE_EXTRA_ROLE_DIRS={toxinidir}/playbooks/roles:{homedir}/.ansible/roles/ceph-ansible/roles
    ANSIBLE_ROLE_REQUIREMENTS_PATH={toxinidir}/ansible-role-requirements.yml
    TEST_PLAYBOOK={toxinidir}/tests/bootstrap-aio.yml {toxinidir}/playbooks/setup-everything.yml
    ANSIBLE_LINT_PARAMS=--exclude={homedir}/.ansible/roles



[testenv:docs]
basepython = python3
deps = -r{toxinidir}/doc/requirements.txt
commands=
    doc8 doc
    bashate tools/build-docs.sh
    {toxinidir}/tools/build-docs.sh



[testenv:deploy-guide]
basepython = python3
deps = -r{toxinidir}/doc/requirements.txt
commands = sphinx-build -a -E -W -d deploy-guide/build/doctrees -b html deploy-guide/source deploy-guide/build/html



[doc8]
# Settings for doc8:
extensions = .rst



[testenv:releasenotes]
basepython = python3
deps = -r{toxinidir}/doc/requirements.txt
commands =
    sphinx-build -a -E -W -d releasenotes/build/doctrees -b html releasenotes/source releasenotes/build/html



# environment used by the -infra templated docs job
[testenv:venv]
basepython = python3
commands =
    {posargs}



[testenv:pep8]
basepython = python3
commands =
    bash -c "{toxinidir}/tests/common/test-pep8.sh"



[flake8]
# Ignores the following rules due to how ansible modules work in general
#     F403 'from ansible.module_utils.basic import *' used;
#           unable to detect undefined names
ignore=F403



[testenv:bashate]
basepython = python3
commands =
    bash -c "{toxinidir}/tests/common/test-bashate.sh"



# The deps URL should be set to the appropriate git URL.
# In the tests repo itself, the variable is uniquely set to
# the toxinidir so that the role is able to test itself, but
# the tox config is exactly the same as other repositories.
#
# The value for other repositories must be:
# http://git.openstack.org/cgit/openstack/openstack-ansible-tests/plain/test-ansible-deps.txt
# or for a stable branch:
# http://git.openstack.org/cgit/openstack/openstack-ansible-tests/plain/test-ansible-deps.txt?h=stable/newton
[testenv:ansible]
basepython = python3
deps =
    {[testenv]deps}
    -r{toxinidir}/global-requirement-pins.txt
    -rhttps://git.openstack.org/cgit/openstack/openstack-ansible-tests/plain/test-ansible-deps.txt



[testenv:ansible-syntax]
basepython = python3
deps =
    {[testenv:ansible]deps}
commands =
    bash -c "{toxinidir}/tests/common/test-ansible-syntax.sh"



[testenv:ansible-lint]
basepython = python3
deps =
    {[testenv:ansible]deps}
commands =
    bash -c "{toxinidir}/tests/common/test-ansible-lint.sh"



[testenv:inventory]
basepython = python3
# Use a fixed seed since some inventory tests rely on specific ordering
setenv =
    {[testenv]setenv}
    PYTHONHASHSEED = 100
commands =
    coverage erase
    coverage run -a {toxinidir}/tests/test_inventory.py
    coverage run -a {toxinidir}/tests/test_manage.py
    coverage run -a {toxinidir}/tests/test_dictutils.py
    coverage run -a {toxinidir}/tests/test_ip.py
    coverage run -a {toxinidir}/tests/test_filesystem.py
    coverage report --show-missing --include={toxinidir}/inventory/*,{toxinidir}/osa_toolkit/*



[testenv:py3-inventory]
basepython = python3.5
setenv =
    {[testenv:inventory]setenv}
commands =
    {[testenv:inventory]commands}



[testenv:linters]
basepython = python3
deps =
    {[testenv:docs]deps}
    {[testenv:ansible]deps}
commands =
    {[testenv:pep8]commands}
    {[testenv:bashate]commands}
    {[testenv:ansible-lint]commands}
    {[testenv:ansible-syntax]commands}
    {[testenv:inventory]commands}
    {[testenv:docs]commands}




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\bootstrap-aio.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2015, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Bootstrap the All-In-One (AIO)
  hosts: localhost
  gather_facts: True
  user: root
  roles:
    - role: "sshd"
    - role: "bootstrap-host"
  vars_files:
    - "{{ playbook_dir }}/../playbooks/defaults/repo_packages/openstack_services.yml"
    - vars/bootstrap-aio-vars.yml
  environment: "{{ deployment_environment_variables | default({}) }}"
  vars:
    ansible_python_interpreter: "/usr/bin/python"
    pip_install_upper_constraints_proto: "{{ ansible_python_version | version_compare('2.7.9', '>=') | ternary('https','http') }}"
    pip_install_upper_constraints: >-
        {{ (playbook_dir ~ '/../global-requirement-pins.txt') | realpath }}
        --constraint {{ pip_install_upper_constraints_proto }}://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt?id={{ requirements_git_install_branch | regex_replace(' #.*$','') }}
    sftp_subsystem:
        'apt': 'sftp /usr/lib/openssh/sftp-server'
        'yum': 'sftp /usr/libexec/openssh/sftp-server'
        'zypper': 'sftp /usr/lib/ssh/sftp-server'
    sshd:
      ListenAddress:
        - 0.0.0.0
        - '::'
      Port: 22
      Protocol: 2
      HostKey:
        - "/etc/ssh/ssh_host_rsa_key"
        - "/etc/ssh/ssh_host_ecdsa_key"
        - "/etc/ssh/ssh_host_ed25519_key"
      UsePrivilegeSeparation: yes
      KeyRegenerationInterval: 3600
      ServerKeyBits: 1024
      SyslogFacility: "AUTH"
      LogLevel: "INFO"
      LoginGraceTime: 120
      StrictModes: yes
      RSAAuthentication: yes
      PubkeyAuthentication: yes
      IgnoreRhosts: yes
      RhostsRSAAuthentication: no
      HostbasedAuthentication: no
      PermitEmptyPasswords: no
      PermitRootLogin: yes
      ChallengeResponseAuthentication: no
      PasswordAuthentication: no
      X11DisplayOffset: 10
      PrintMotd: no
      PrintLastLog: no
      TCPKeepAlive: yes
      AcceptEnv: "LANG LC_*"
      Subsystem: "{{ sftp_subsystem[ansible_pkg_mgr] }}"
      UsePAM: yes
      UseDNS: no
      X11Forwarding: no
      Compression: yes
      CompressionLevel: 6
      MaxSessions: 100
      MaxStartups: "100:100:100"
      GSSAPIAuthentication: no
      GSSAPICleanupCredentials: no
  post_tasks:
    - name: Check that new network interfaces are up
      assert:
        that:
          - ansible_eth12['active'] == true
          - ansible_eth13['active'] == true
          - ansible_eth14['active'] == true
      when:
        - (bootstrap_host_container_tech | default('unknown')) != 'nspawn'




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\test-inventory.ini
===========File Type===========
.ini
===========File Content===========
localhost ansible_connection=local



===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\test-vars-overrides.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2017, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Ensuring group vars
  hosts: "hosts"
  gather_facts: no
  connection: local
  user: root
  tasks:
    - name: Ensuring existing group vars are well applied
      assert:
        that:
          - "openstack_release is defined"
    - name: Ensuring babar is well defined
      assert:
        that:
          - "babar == 'elephant'"
    - name: Ensuring lxc_hosts_package_state is well overridden
      assert:
        that:
          - "lxc_hosts_package_state == 'present'"

- name: Ensuring host vars
  hosts: localhost
  gather_facts: no
  connection: local
  user: root
  tasks:
    - name: Ensuring tintin has milou
      assert:
        that:
          - "tintin == 'milou'"
    - name: Ensuring security_package_state is overridden
      assert:
        that:
          - "security_package_state == 'present'"




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\test_dictutils.py
===========File Type===========
.py
===========File Content===========
#!/usr/bin/env python

# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import unittest

from osa_toolkit import dictutils as du


class TestMergeDictUnit(unittest.TestCase):
    def test_merging_dict(self):
        base = {'key1': 'value1'}
        target = {'key2': 'value2'}

        new = du.merge_dict(base, target)

        self.assertIn('key2', new.keys())
        self.assertEqual('value2', new['key2'])

    def test_base_dict_is_modified(self):
        base = {'key1': 'value1'}
        target = {'key2': 'value2'}

        new = du.merge_dict(base, target)

        self.assertIs(base, new)

    def test_merging_nested_dicts(self):
        base = {'key1': 'value1'}
        target = {'key2': {'key2.1': 'value2'}}

        new = du.merge_dict(base, target)

        self.assertIn('key2', new.keys())
        self.assertIn('key2.1', new['key2'].keys())

    def test_merging_nested_dicts_with_same_key(self):
        base = {'same_key': {'inside_key1': 'inside_key1'}}
        target = {'same_key': {'inside_key2': 'inside_key2'}}

        new = du.merge_dict(base, target)

        self.assertIn('inside_key1', new['same_key'].keys())
        self.assertIn('inside_key2', new['same_key'].keys())


class TestAppendIfUnit(unittest.TestCase):
    def test_appending_not_present(self):
        base = ['foo', 'bar']
        target = 'baz'

        retval = du.append_if(base, target)

        self.assertIn(target, base)
        self.assertTrue(retval)

    def test_appending_present(self):
        base = ['foo', 'bar', 'baz']
        target = 'baz'

        retval = du.append_if(base, target)

        self.assertFalse(retval)


class TestListRemovalUnit(unittest.TestCase):
    def setUp(self):
        # Can't just use a member list, since it remains changed after each
        # test
        self.base = ['foo', 'bar']

    def test_removing_one_target(self):
        target = ['bar']

        du.recursive_list_removal(self.base, target)

        self.assertNotIn('bar', self.base)

    def test_removing_entire_list(self):
        # Use a copy so we're not hitting the exact same object in memory.
        target = list(self.base)

        du.recursive_list_removal(self.base, target)

        self.assertEqual(0, len(self.base))

    def test_using_base_as_target(self):
        target = self.base

        du.recursive_list_removal(self.base, target)

        self.assertEqual(1, len(self.base))
        self.assertEqual(1, len(target))
        self.assertIn('bar', self.base)

    def test_using_bare_string(self):
        target = 'foo'

        du.recursive_list_removal(self.base, target)

        self.assertEqual(2, len(self.base))


class TestDictRemovalUnit(unittest.TestCase):
    def test_deleting_single_item_in_single_level_noop(self):
        """The function only operates on nested dictionaries"""
        base = {'key1': 'value1'}
        target = ['value1']

        du.recursive_dict_removal(base, target)

        self.assertEqual('value1', base['key1'])

    def test_deleting_single_item(self):
        base = {'key1': {'key1.1': 'value1'}}
        target = ['value1']

        du.recursive_dict_removal(base, target)

        self.assertEqual('value1', base['key1']['key1.1'])

    def test_deleting_single_item_from_list(self):
        base = {'key1': {'key1.1': ['value1']}}
        target = ['value1']

        du.recursive_dict_removal(base, target)

        self.assertEqual(0, len(base['key1']['key1.1']))
        self.assertNotIn('value1', base['key1']['key1.1'])

    def test_deleting_single_item_from_nested_list(self):
        """The function only operates on the 2nd level dictionary"""
        base = {'key1': {'key1.1': {'key1.1.1': ['value1']}}}
        target = ['value1']

        du.recursive_dict_removal(base, target)

        self.assertEqual(1, len(base['key1']['key1.1']['key1.1.1']))
        self.assertIn('value1', base['key1']['key1.1']['key1.1.1'])

    def test_deleting_single_item_top_level_list(self):
        base = {'key1': ['value1']}
        target = ['value1']

        du.recursive_dict_removal(base, target)

        self.assertEqual(0, len(base['key1']))

    def test_deleting_single_nested_key(self):
        base = {'key1': {'key1.1': {'key1.1.1': ['value1']}}}
        target = ['key1.1.1']

        du.recursive_dict_removal(base, target)

        self.assertNotIn('key1.1.1', base['key1']['key1.1'])


if __name__ == '__main__':
    unittest.main()




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\test_filesystem.py
===========File Type===========
.py
===========File Content===========
#!/usr/bin/env python
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#

import mock
import os
from os import path
from osa_toolkit import filesystem as fs
import sys
import unittest

from test_inventory import cleanup
from test_inventory import get_inventory
from test_inventory import make_config

INV_DIR = 'inventory'

sys.path.append(path.join(os.getcwd(), INV_DIR))

TARGET_DIR = path.join(os.getcwd(), 'tests', 'inventory')
USER_CONFIG_FILE = path.join(TARGET_DIR, 'openstack_user_config.yml')


def setUpModule():
    # The setUpModule function is used by the unittest framework.
    make_config()


def tearDownModule():
    # This file should only be removed after all tests are run,
    # thus it is excluded from cleanup.
    os.remove(USER_CONFIG_FILE)


class TestMultipleRuns(unittest.TestCase):
    def test_creating_backup_file(self):
        inventory_file_path = os.path.join(TARGET_DIR,
                                           'openstack_inventory.json')
        get_backup_name_path = 'osa_toolkit.filesystem._get_backup_name'
        backup_name = 'openstack_inventory.json-20160531_171804.json'

        tar_file = mock.MagicMock()
        tar_file.__enter__.return_value = tar_file

        # run make backup with faked tarfiles and date
        with mock.patch('osa_toolkit.filesystem.tarfile.open') as tar_open:
            tar_open.return_value = tar_file
            with mock.patch(get_backup_name_path) as backup_mock:
                backup_mock.return_value = backup_name
                fs._make_backup(TARGET_DIR, inventory_file_path)

        backup_path = path.join(TARGET_DIR, 'backup_openstack_inventory.tar')

        tar_open.assert_called_with(backup_path, 'a')

        # This chain is present because of how tarfile.open is called to
        # make a context manager inside the make_backup function.

        tar_file.add.assert_called_with(inventory_file_path,
                                        arcname=backup_name)

    def test_recreating_files(self):
        # Deleting the files after the first run should cause the files to be
        # completely remade
        get_inventory()

        get_inventory()

        backup_path = path.join(TARGET_DIR, 'backup_openstack_inventory.tar')

        self.assertFalse(os.path.exists(backup_path))

    def test_rereading_files(self):
        # Generate the initial inventory files
        get_inventory(clean=False)

        inv, path = fs.load_inventory(TARGET_DIR)
        self.assertIsInstance(inv, dict)
        self.assertIn('_meta', inv)
        # This test is basically just making sure we get more than
        # INVENTORY_SKEL populated, so we're not going to do deep testing
        self.assertIn('log_hosts', inv)

    def tearDown(self):
        # Clean up here since get_inventory will not do it by design in
        # this test.
        cleanup()


if __name__ == '__main__':
    unittest.main(catchbreak=True)




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\test_inventory.py
===========File Type===========
.py
===========File Content===========
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import collections
import copy
import json
import mock
import os
from os import path
try:
    import Queue
except ImportError:
    import queue as Queue
import sys
import unittest
import warnings
import yaml

INV_DIR = 'inventory'

sys.path.append(path.join(os.getcwd(), INV_DIR))

from osa_toolkit import dictutils
import dynamic_inventory
from osa_toolkit import filesystem as fs
from osa_toolkit import generate as di
from osa_toolkit import tools

TARGET_DIR = path.join(os.getcwd(), 'tests', 'inventory')
BASE_ENV_DIR = INV_DIR
CONFIGS_DIR = path.join(os.getcwd(), 'etc', 'openstack_deploy')
CONFD = os.path.join(CONFIGS_DIR, 'conf.d')
AIO_CONFIG_FILE = path.join(CONFIGS_DIR, 'openstack_user_config.yml.aio')
USER_CONFIG_FILE = path.join(TARGET_DIR, 'openstack_user_config.yml')

# These files will be placed in TARGET_DIR by the inventory functions
# They should be cleaned up between each test.
CLEANUP = [
    'openstack_inventory.json',
    'openstack_hostnames_ips.yml',
    'backup_openstack_inventory.tar'
]

# Base config is a global configuration accessible for convenience.
# It should *not* be mutated outside of setUpModule, which populates it.
_BASE_CONFIG = {}


def get_config():
    """Return a copy of the original config so original isn't modified."""
    global _BASE_CONFIG
    return copy.deepcopy(_BASE_CONFIG)


def make_config():
    """Build an inventory configuration from the sample AIO files.

    Take any files specified as '.aio' and load their keys into a
    configuration dict  and write them out to a file for consumption by
    the tests.
    """
    # Allow access here so we can populate the dictionary.
    global _BASE_CONFIG

    _BASE_CONFIG = tools.make_example_config(AIO_CONFIG_FILE, CONFD)

    tools.write_example_config(USER_CONFIG_FILE, _BASE_CONFIG)


def setUpModule():
    # The setUpModule function is used by the unittest framework.
    make_config()


def tearDownModule():
    # This file should only be removed after all tests are run,
    # thus it is excluded from cleanup.
    os.remove(USER_CONFIG_FILE)


def cleanup():
    for f_name in CLEANUP:
        f_file = path.join(TARGET_DIR, f_name)
        if os.path.exists(f_file):
            os.remove(f_file)


def get_inventory(clean=True, extra_args=None):
    "Return the inventory mapping in a dict."
    # Use the list argument to more closely mirror
    # Ansible's use of the callable.
    args = {'config': TARGET_DIR, 'list': True,
            'environment': BASE_ENV_DIR}
    if extra_args:
        args.update(extra_args)
    try:
        inventory_string = di.main(**args)
        inventory = json.loads(inventory_string)
        return inventory
    finally:
        if clean:
            # Remove the file system artifacts since we want to force
            # fresh runs
            cleanup()


class TestArgParser(unittest.TestCase):
    def test_no_args(self):
        arg_dict = dynamic_inventory.args([])
        self.assertIsNone(arg_dict['config'])
        self.assertEqual(arg_dict['list'], False)

    def test_list_arg(self):
        arg_dict = dynamic_inventory.args(['--list'])
        self.assertEqual(arg_dict['list'], True)

    def test_config_arg(self):
        arg_dict = dynamic_inventory.args(['--config',
                                           '/etc/openstack_deploy'])
        self.assertEqual(arg_dict['config'], '/etc/openstack_deploy')


class TestAnsibleInventoryFormatConstraints(unittest.TestCase):
    inventory = None

    expected_groups = [
        'aio1-host_containers',
        'all',
        'all_containers',
        'aodh_alarm_evaluator',
        'aodh_alarm_notifier',
        'aodh_all',
        'aodh_api',
        'aodh_container',
        'aodh_listener',
        'barbican_all',
        'barbican_api',
        'barbican_container',
        'blazar_all',
        'blazar_container',
        'blazar_api',
        'blazar_manager',
        'ceilometer_all',
        'ceilometer_agent_central',
        'ceilometer_agent_compute',
        'ceilometer_agent_notification',
        'ceilometer_central_container',
        'ceph_all',
        'ceph-mon_all',
        'ceph-mon_containers',
        'ceph-mon_container',
        'ceph-mon_hosts',
        'ceph-mon',
        'ceph-osd_all',
        'ceph-osd_containers',
        'ceph-osd_container',
        'ceph-osd_hosts',
        'ceph-osd',
        'ceph-rgw_all',
        'ceph-rgw_containers',
        'ceph-rgw_container',
        'ceph-rgw_hosts',
        'ceph-rgw',
        'cinder_all',
        'cinder_api',
        'cinder_api_container',
        'cinder_backup',
        'cinder_scheduler',
        'cinder_volume',
        'cinder_volumes_container',
        'compute-infra_all',
        'compute-infra_containers',
        'compute-infra_hosts',
        'compute_all',
        'compute_containers',
        'compute_hosts',
        'congress_all',
        'congress_container',
        'congress_server',
        'dashboard_all',
        'dashboard_containers',
        'dashboard_hosts',
        'database_containers',
        'database_hosts',
        'dnsaas_all',
        'dnsaas_containers',
        'dnsaas_hosts',
        'designate_all',
        'designate_container',
        'designate_api',
        'designate_central',
        'designate_mdns',
        'designate_worker',
        'designate_producer',
        'designate_sink',
        'etcd',
        'etcd_all',
        'etcd_container',
        'etcd_containers',
        'etcd_hosts',
        'galera',
        'galera_all',
        'galera_container',
        'glance_all',
        'glance_api',
        'glance_container',
        'glance_registry',
        'gnocchi_all',
        'gnocchi_api',
        'gnocchi_container',
        'gnocchi_metricd',
        'haproxy',
        'haproxy_all',
        'haproxy_container',
        'haproxy_containers',
        'haproxy_hosts',
        'heat_all',
        'heat_api',
        'heat_api_cfn',
        'heat_api_container',
        'heat_engine',
        'horizon',
        'horizon_all',
        'horizon_container',
        'hosts',
        'identity_all',
        'identity_containers',
        'identity_hosts',
        'image_all',
        'image_containers',
        'image_hosts',
        'ironic-infra_all',
        'ironic-infra_containers',
        'ironic-infra_hosts',
        'ironic-server_containers',
        'ironic-server_hosts',
        'ironic_all',
        'ironic_api',
        'ironic_api_container',
        'ironic_conductor',
        'ironic_server',
        'ironic_server_container',
        'ironic_servers',
        'ironic_compute',
        'ironic_compute_container',
        'ironic-compute_containers',
        'ironic-compute_all',
        'ironic-compute_hosts',
        'key-manager_containers',
        'key-manager_hosts',
        'key-manager_all',
        'keystone',
        'keystone_all',
        'keystone_container',
        'kvm-compute_containers',
        'kvm-compute_hosts',
        'log_all',
        'log_containers',
        'log_hosts',
        'lxc_hosts',
        'lxd-compute_containers',
        'lxd-compute_hosts',
        'magnum',
        'magnum-infra_all',
        'magnum-infra_containers',
        'magnum-infra_hosts',
        'magnum_all',
        'magnum_container',
        'mano_all',
        'mano_containers',
        'mano_hosts',
        'nspawn_hosts',
        'octavia-infra_hosts',
        'octavia_all',
        'octavia-api',
        'octavia_server_container',
        'octavia-worker',
        'octavia-housekeeping',
        'octavia-health-manager',
        'octavia-infra_containers',
        'octavia-infra_all',
        'policy_all',
        'policy_containers',
        'policy_hosts',
        'powervm-compute_containers',
        'powervm-compute_hosts',
        'qemu-compute_containers',
        'qemu-compute_hosts',
        'reservation_all',
        'reservation_containers',
        'reservation_hosts',
        'trove_all',
        'trove_api',
        'trove_conductor',
        'trove_taskmanager',
        'trove_api_container',
        'trove-infra_containers',
        'trove-infra_hosts',
        'trove-infra_all',
        'memcached',
        'memcached_all',
        'memcached_container',
        'memcaching_containers',
        'memcaching_hosts',
        'metering-alarm_all',
        'metering-alarm_containers',
        'metering-alarm_hosts',
        'metering-compute_all',
        'metering-compute_container',
        'metering-compute_containers',
        'metering-compute_hosts',
        'metering-infra_all',
        'metering-infra_containers',
        'metering-infra_hosts',
        'metrics_all',
        'metrics_containers',
        'metrics_hosts',
        'mq_containers',
        'mq_hosts',
        'network_all',
        'network_containers',
        'network_hosts',
        'neutron_agent',
        'neutron_agents_container',
        'neutron_all',
        'neutron_bgp_dragent',
        'neutron_dhcp_agent',
        'neutron_l3_agent',
        'neutron_lbaas_agent',
        'neutron_linuxbridge_agent',
        'neutron_metadata_agent',
        'neutron_metering_agent',
        'neutron_openvswitch_agent',
        'neutron_sriov_nic_agent',
        'neutron_server',
        'neutron_server_container',
        'nova_all',
        'nova_api_metadata',
        'nova_api_os_compute',
        'nova_api_container',
        'nova_api_placement',
        'nova_compute',
        'nova_compute_container',
        'nova_conductor',
        'nova_console',
        'nova_scheduler',
        'opendaylight',
        'operator_containers',
        'operator_hosts',
        'orchestration_all',
        'orchestration_containers',
        'orchestration_hosts',
        'os-infra_containers',
        'os-infra_hosts',
        'pkg_repo',
        'rabbit_mq_container',
        'rabbitmq',
        'rabbitmq_all',
        'remote',
        'remote_containers',
        'repo-infra_all',
        'repo-infra_containers',
        'repo-infra_hosts',
        'repo_all',
        'repo_container',
        'rsyslog',
        'rsyslog_all',
        'rsyslog_container',
        'sahara-infra_all',
        'sahara-infra_containers',
        'sahara-infra_hosts',
        'sahara_all',
        'sahara_api',
        'sahara_container',
        'sahara_engine',
        'shared-infra_all',
        'shared-infra_containers',
        'shared-infra_hosts',
        'storage-infra_all',
        'storage-infra_containers',
        'storage-infra_hosts',
        'storage_all',
        'storage_containers',
        'storage_hosts',
        'swift-proxy_all',
        'swift-proxy_containers',
        'swift-proxy_hosts',
        'swift-remote_containers',
        'swift-remote_hosts',
        'swift_acc',
        'swift_acc_container',
        'swift_all',
        'swift_cont',
        'swift_cont_container',
        'swift_containers',
        'swift_hosts',
        'swift_obj',
        'swift_obj_container',
        'swift_proxy',
        'swift_proxy_container',
        'swift_remote',
        'swift_remote_all',
        'swift_remote_container',
        'tacker_all',
        'tacker_container',
        'tacker_server',
        'unbound',
        'unbound_all',
        'unbound_container',
        'unbound_containers',
        'unbound_hosts',
        'utility',
        'utility_all',
        'utility_container'
    ]

    @classmethod
    def setUpClass(cls):
        cls.inventory = get_inventory()

    def test_meta(self):
        meta = self.inventory['_meta']
        self.assertIsNotNone(meta, "_meta missing from inventory")
        self.assertIsInstance(meta, dict, "_meta is not a dict")

    def test_hostvars(self):
        hostvars = self.inventory['_meta']['hostvars']
        self.assertIsNotNone(hostvars, "hostvars missing from _meta")
        self.assertIsInstance(hostvars, dict, "hostvars is not a dict")

    def test_group_vars_all(self):
        group_vars_all = self.inventory['all']
        self.assertIsNotNone(group_vars_all,
                             "group vars all missing from inventory")
        self.assertIsInstance(group_vars_all, dict,
                              "group vars all is not a dict")

        the_vars = group_vars_all['vars']
        self.assertIsNotNone(the_vars,
                             "vars missing from group vars all")
        self.assertIsInstance(the_vars, dict,
                              "vars in group vars all is not a dict")

    def test_expected_host_groups_present(self):

        for group in self.expected_groups:
            the_group = self.inventory[group]
            self.assertIsNotNone(the_group,
                                 "Required host group: %s is missing "
                                 "from inventory" % group)
            self.assertIsInstance(the_group, dict)

            if group != 'all':
                self.assertIn('hosts', the_group)
                self.assertIsInstance(the_group['hosts'], list)

    def test_only_expected_host_groups_present(self):
        all_keys = list(self.expected_groups)
        all_keys.append('_meta')
        self.assertEqual(set(all_keys), set(self.inventory.keys()))

    def test_configured_groups_have_hosts(self):
        config = get_config()
        groups = self.inventory.keys()
        for group in groups:
            if group in config.keys():
                self.assertTrue(0 < len(self.inventory[group]['hosts']))


class TestUserConfiguration(unittest.TestCase):
    def setUp(self):
        self.longMessage = True
        self.loaded_user_configuration = fs.load_user_configuration(TARGET_DIR)

    def test_loading_user_configuration(self):
        """Test that the user configuration can be loaded"""
        self.assertIsInstance(self.loaded_user_configuration, dict)


class TestEnvironments(unittest.TestCase):
    def setUp(self):
        self.longMessage = True
        self.loaded_environment = fs.load_environment(BASE_ENV_DIR, {})

    def test_loading_environment(self):
        """Test that the environment can be loaded"""
        self.assertIsInstance(self.loaded_environment, dict)

    def test_envd_read(self):
        """Test that the env.d contents are inserted into the environment"""
        expected_keys = [
            'component_skel',
            'container_skel',
            'physical_skel',
        ]
        for key in expected_keys:
            self.assertIn(key, self.loaded_environment)


class TestIps(unittest.TestCase):
    def setUp(self):
        # Allow custom assertion errors.
        self.longMessage = True
        self.env = fs.load_environment(BASE_ENV_DIR, {})

    @mock.patch('osa_toolkit.filesystem.load_environment')
    @mock.patch('osa_toolkit.filesystem.load_user_configuration')
    def test_duplicates(self, mock_load_config, mock_load_env):
        """Test that no duplicate IPs are made on any network."""

        # Grab our values read from the file system just once.
        mock_load_config.return_value = get_config()
        mock_load_env.return_value = self.env

        mock_open = mock.mock_open()

        for i in range(0, 99):
            # tearDown is ineffective for this loop, so clean the USED_IPs
            # on each run
            inventory = None
            di.ip.USED_IPS = set()

            # Mock out the context manager being used to write files.
            # We don't need to hit the file system for this test.
            with mock.patch('__main__.open', mock_open):
                inventory = get_inventory()
            ips = collections.defaultdict(int)
            hostvars = inventory['_meta']['hostvars']

            for host, var_dict in hostvars.items():
                nets = var_dict['container_networks']
                for net, vals in nets.items():
                    if 'address' in vals.keys():

                        addr = vals['address']
                        ips[addr] += 1

                        self.assertEqual(1, ips[addr],
                                         msg="IP %s duplicated." % addr)

    def test_empty_ip_queue(self):
        q = Queue.Queue()
        with self.assertRaises(SystemExit) as context:
            # TODO(nrb): import and use ip module directly
            di.ip.get_ip_address('test', q)
        expectedLog = ("Cannot retrieve requested amount of IP addresses. "
                       "Increase the test range in your "
                       "openstack_user_config.yml.")
        self.assertEqual(str(context.exception), expectedLog)

    def tearDown(self):
        # Since the get_ip_address function touches USED_IPS,
        # and USED_IPS is currently a global var, make sure we clean it out
        di.ip.USED_IPS = set()


class TestConfigCheckBase(unittest.TestCase):
    def setUp(self):
        self.config_changed = False
        self.user_defined_config = get_config()

    def delete_config_key(self, user_defined_config, key):
        try:
            if key in user_defined_config:
                del user_defined_config[key]
            elif key in user_defined_config['global_overrides']:
                del user_defined_config['global_overrides'][key]
            else:
                raise KeyError("can't find specified key in user config")
        finally:
            self.write_config()

    def add_config_key(self, key, value):
        self.user_defined_config[key] = value
        self.write_config()

    def add_provider_network(self, net_name, cidr):
        self.user_defined_config['cidr_networks'][net_name] = cidr
        self.write_config()

    def delete_provider_network(self, net_name):
        del self.user_defined_config['cidr_networks'][net_name]
        self.write_config()

    def add_provider_network_key(self, net_name, key, value):
        pns = self.user_defined_config['global_overrides']['provider_networks']
        for net in pns:
            if 'ip_from_q' in net['network']:
                if net['network']['ip_from_q'] == net_name:
                    net['network'][key] = value

    def delete_provider_network_key(self, net_name, key):
        pns = self.user_defined_config['global_overrides']['provider_networks']
        for net in pns:
            if 'ip_from_q' in net['network']:
                if net['network']['ip_from_q'] == net_name:
                    if key in net['network']:
                        del net['network'][key]

    def write_config(self):
        self.config_changed = True
        # Save new user_config_file
        with open(USER_CONFIG_FILE, 'wb') as f:
            f.write(yaml.dump(self.user_defined_config).encode('ascii'))

    def restore_config(self):
        # get back our initial user config file
        self.user_defined_config = get_config()
        self.write_config()

    def set_new_hostname(self, user_defined_config, group,
                         old_hostname, new_hostname):
        # set a new name for the specified hostname
        old_hostname_settings = user_defined_config[group].pop(old_hostname)
        user_defined_config[group][new_hostname] = old_hostname_settings
        self.write_config()

    def set_new_ip(self, user_defined_config, group, hostname, ip):
        # Sets an IP address for a specified host.
        user_defined_config[group][hostname]['ip'] = ip
        self.write_config()

    def add_host(self, group, host_name, ip):
        self.user_defined_config[group][host_name] = {'ip': ip}
        self.write_config()

    def tearDown(self):
        if self.config_changed:
            self.restore_config()


class TestConfigChecks(TestConfigCheckBase):
    def test_missing_container_cidr_network(self):
        self.delete_provider_network('container')
        with self.assertRaises(SystemExit) as context:
            get_inventory()
        expectedLog = ("No container or management network specified in "
                       "user config.")
        self.assertEqual(str(context.exception), expectedLog)

    def test_management_network_malformed(self):
        self.delete_provider_network_key('container', 'is_container_address')
        self.write_config()

        with self.assertRaises(di.ProviderNetworkMisconfiguration) as context:
            get_inventory()
        expectedLog = ("Provider network with queue 'container' "
                       "requires 'is_container_address' "
                       "to be set to True.")
        self.assertEqual(str(context.exception), expectedLog)
        self.restore_config()

    def test_missing_cidr_network_present_in_provider(self):
        self.delete_provider_network('storage')
        with self.assertRaises(SystemExit) as context:
            get_inventory()
        expectedLog = "can't find storage in cidr_networks"
        self.assertEqual(str(context.exception), expectedLog)

    def test_missing_cidr_networks_key(self):
        del self.user_defined_config['cidr_networks']
        self.write_config()
        with self.assertRaises(SystemExit) as context:
            get_inventory()
        expectedLog = "No container CIDR specified in user config"
        self.assertEqual(str(context.exception), expectedLog)

    def test_provider_networks_check(self):
        # create config file without provider networks
        self.delete_config_key(self.user_defined_config, 'provider_networks')
        # check if provider networks absence is Caught
        with self.assertRaises(SystemExit) as context:
            get_inventory()
        expectedLog = "provider networks can't be found under global_overrides"
        self.assertIn(expectedLog, str(context.exception))

    def test_global_overrides_check(self):
        # create config file without global_overrides
        self.delete_config_key(self.user_defined_config, 'global_overrides')
        # check if global_overrides absence is Caught
        with self.assertRaises(SystemExit) as context:
            get_inventory()
        expectedLog = "global_overrides can't be found in user config"
        self.assertEqual(str(context.exception), expectedLog)

    def test_two_hosts_same_ip(self):
        # Use an OrderedDict to be certain our testing order is preserved
        # Even with the same hash seed, different OSes get different results,
        # eg. local OS X vs gate's Linux
        config = collections.OrderedDict()
        config['shared-infra_hosts'] = {
            'host1': {
                'ip': '192.168.1.1'
            }
        }
        config['compute_hosts'] = {
            'host2': {
                'ip': '192.168.1.1'
            }
        }

        with self.assertRaises(di.MultipleHostsWithOneIPError) as context:
            di._check_same_ip_to_multiple_host(config)
        self.assertEqual(context.exception.ip, '192.168.1.1')
        self.assertEqual(context.exception.assigned_host, 'host1')
        self.assertEqual(context.exception.new_host, 'host2')

    def test_two_hosts_same_ip_externally(self):
        self.set_new_hostname(self.user_defined_config, "haproxy_hosts",
                              "aio1", "hap")
        with self.assertRaises(di.MultipleHostsWithOneIPError) as context:
            get_inventory()
        expectedLog = ("Both host:aio1 and host:hap have "
                       "address:172.29.236.100 assigned.  Cannot "
                       "assign same ip to both hosts")
        self.assertEqual(str(context.exception), expectedLog)

    def test_one_host_two_ips_externally(self):
        # haproxy chosen because it was last in the config file as of
        # writing
        self.set_new_ip(self.user_defined_config, 'haproxy_hosts', 'aio1',
                        '172.29.236.101')
        with self.assertRaises(di.MultipleIpForHostError) as context:
            get_inventory()
        expectedLog = ("Host aio1 has both 172.29.236.100 and 172.29.236.101 "
                       "assigned")
        self.assertEqual(str(context.exception), expectedLog)

    def test_two_ips(self):
        # Use an OrderedDict to be certain our testing order is preserved
        # Even with the same hash seed, different OSes get different results,
        # eg. local OS X vs gate's Linux
        config = collections.OrderedDict()
        config['shared-infra_hosts'] = {
            'host1': {
                'ip': '192.168.1.1'
            }
        }
        config['compute_hosts'] = {
            'host1': {
                'ip': '192.168.1.2'
            }
        }

        with self.assertRaises(di.MultipleIpForHostError) as context:
            di._check_multiple_ips_to_host(config)
        self.assertEqual(context.exception.current_ip, '192.168.1.1')
        self.assertEqual(context.exception.new_ip, '192.168.1.2')
        self.assertEqual(context.exception.hostname, 'host1')

    def test_correct_hostname_ip_map(self):
        config = {
            'shared-infra_hosts': {
                'host1': {
                    'ip': '192.168.1.1'
                }
            },
            'compute_hosts': {
                'host2': {
                    'ip': '192.168.1.2'
                }
            },
        }
        ret = di._check_multiple_ips_to_host(config)
        self.assertTrue(ret)


class TestStaticRouteConfig(TestConfigCheckBase):
    def setUp(self):
        super(TestStaticRouteConfig, self).setUp()
        self.expectedMsg = ("Static route provider network with queue "
                            "'container' needs both 'cidr' and 'gateway' "
                            "values.")

    def add_static_route(self, q_name, route_dict):
        """Adds a static route to a provider network."""
        pn = self.user_defined_config['global_overrides']['provider_networks']
        for net in pn:
            net_dict = net['network']
            q = net_dict.get('ip_from_q', None)
            if q == q_name:
                net_dict['static_routes'] = [route_dict]
        self.write_config()

    def test_setting_static_route(self):
        route_dict = {'cidr': '10.176.0.0/12',
                      'gateway': '172.29.248.1'}
        self.add_static_route('container', route_dict)
        inventory = get_inventory()

        # Use aio1 and 'container_address' since they're known keys.
        hostvars = inventory['_meta']['hostvars']['aio1']
        cont_add = hostvars['container_networks']['container_address']

        self.assertIn('static_routes', cont_add)

        first_route = cont_add['static_routes'][0]
        self.assertIn('cidr', first_route)
        self.assertIn('gateway', first_route)

    def test_setting_bad_static_route_only_cidr(self):
        route_dict = {'cidr': '10.176.0.0/12'}
        self.add_static_route('container', route_dict)

        with self.assertRaises(di.MissingStaticRouteInfo) as context:
            get_inventory()

        exception = context.exception

        self.assertEqual(str(exception), self.expectedMsg)

    def test_setting_bad_static_route_only_gateway(self):
        route_dict = {'gateway': '172.29.248.1'}
        self.add_static_route('container', route_dict)

        with self.assertRaises(di.MissingStaticRouteInfo) as context:
            get_inventory()

        exception = context.exception

        self.assertEqual(exception.message, self.expectedMsg)

    def test_setting_bad_gateway_value(self):
        route_dict = {'cidr': '10.176.0.0/12',
                      'gateway': None}
        self.add_static_route('container', route_dict)

        with self.assertRaises(di.MissingStaticRouteInfo) as context:
            get_inventory()

        exception = context.exception

        self.assertEqual(exception.message, self.expectedMsg)

    def test_setting_bad_cidr_value(self):
        route_dict = {'cidr': None,
                      'gateway': '172.29.248.1'}
        self.add_static_route('container', route_dict)

        with self.assertRaises(di.MissingStaticRouteInfo) as context:
            get_inventory()

        exception = context.exception

        self.assertEqual(exception.message, self.expectedMsg)

    def test_setting_bad_cidr_gateway_value(self):
        route_dict = {'cidr': None,
                      'gateway': None}
        self.add_static_route('container', route_dict)

        with self.assertRaises(di.MissingStaticRouteInfo) as context:
            get_inventory()

        exception = context.exception

        self.assertEqual(exception.message, self.expectedMsg)


class TestGlobalOverridesConfigDeletion(TestConfigCheckBase):
    def setUp(self):
        super(TestGlobalOverridesConfigDeletion, self).setUp()
        self.inventory = get_inventory()

    def add_global_override(self, var_name, var_value):
        """Adds an arbitrary name and value to the global_overrides dict."""
        overrides = self.user_defined_config['global_overrides']
        overrides[var_name] = var_value

    def remove_global_override(self, var_name):
        """Removes target key from the global_overrides dict."""
        overrides = self.user_defined_config['global_overrides']
        del overrides[var_name]

    def test_global_overrides_delete_when_merge(self):
        """Vars removed from global overrides are removed from inventory"""
        self.add_global_override('foo', 'bar')

        di._parse_global_variables({}, self.inventory,
                                   self.user_defined_config)

        self.remove_global_override('foo')

        di._parse_global_variables({}, self.inventory,
                                   self.user_defined_config)

        self.assertNotIn('foo', self.inventory['all']['vars'],
                         "foo var not removed from group_vars_all")

    def test_global_overrides_merge(self):
        self.add_global_override('foo', 'bar')

        di._parse_global_variables({}, self.inventory,
                                   self.user_defined_config)

        self.assertEqual('bar', self.inventory['all']['vars']['foo'])

    def test_container_cidr_key_retained(self):
        user_cidr = self.user_defined_config['cidr_networks']['container']
        di._parse_global_variables(user_cidr, self.inventory,
                                   self.user_defined_config)
        self.assertIn('container_cidr', self.inventory['all']['vars'])
        self.assertEqual(self.inventory['all']['vars']['container_cidr'],
                         user_cidr)

    def test_only_old_vars_deleted(self):
        self.inventory['all']['vars']['foo'] = 'bar'

        di._parse_global_variables('', self.inventory,
                                   self.user_defined_config)

        self.assertNotIn('foo', self.inventory['all']['vars'])

    def test_empty_vars(self):
        del self.inventory['all']

        di._parse_global_variables('', self.inventory,
                                   self.user_defined_config)

        self.assertIn('container_cidr', self.inventory['all']['vars'])

        for key in self.user_defined_config['global_overrides']:
            self.assertIn(key, self.inventory['all']['vars'])


class TestEnsureInventoryUptoDate(unittest.TestCase):
    def setUp(self):
        self.env = fs.load_environment(BASE_ENV_DIR, {})
        # Copy because we manipulate the structure in each test;
        # not copying would modify the global var in the target code
        self.inv = copy.deepcopy(di.INVENTORY_SKEL)
        # Since we're not running skel_setup, add necessary keys
        self.host_vars = self.inv['_meta']['hostvars']

        # The _ensure_inventory_uptodate function depends on values inserted
        # by the skel_setup function
        di.skel_setup(self.env, self.inv)

    def test_missing_required_host_vars(self):
        self.host_vars['host1'] = {}

        di._ensure_inventory_uptodate(self.inv, self.env['container_skel'])

        for required_key in di.REQUIRED_HOSTVARS:
            self.assertIn(required_key, self.host_vars['host1'])

    def test_missing_container_name(self):
        self.host_vars['host1'] = {}

        di._ensure_inventory_uptodate(self.inv, self.env['container_skel'])

        self.assertIn('container_name', self.host_vars['host1'])
        self.assertEqual(self.host_vars['host1']['container_name'], 'host1')

    def test_inserting_container_networks_is_dict(self):
        self.host_vars['host1'] = {}

        di._ensure_inventory_uptodate(self.inv, self.env['container_skel'])

        self.assertIsInstance(self.host_vars['host1']['container_networks'],
                              dict)

    def test_populating_inventory_info(self):
        skel = self.env['container_skel']

        di._ensure_inventory_uptodate(self.inv, skel)

        for container_type, type_vars in skel.items():
            hosts = self.inv[container_type]['hosts']
            if hosts:
                for host in hosts:
                    host_var_entries = self.inv['_meta']['hostvars'][host]
                    if 'properties' in type_vars:
                        self.assertEqual(host_var_entries['properties'],
                                         type_vars['properties'])

    def tearDown(self):
        self.env = None
        self.host_vars = None
        self.inv = None


class OverridingEnvBase(unittest.TestCase):
    def setUp(self):
        self.base_env = fs.load_environment(BASE_ENV_DIR, {})

        # Use the cinder configuration as our sample for override testing
        with open(path.join(BASE_ENV_DIR, 'env.d', 'cinder.yml'), 'r') as f:
            self.cinder_config = yaml.safe_load(f.read())

        self.override_path = path.join(TARGET_DIR, 'env.d')
        os.mkdir(self.override_path)

    def write_override_env(self):
        with open(path.join(self.override_path, 'cinder.yml'), 'w') as f:
            f.write(yaml.safe_dump(self.cinder_config))

    def tearDown(self):
        os.remove(path.join(self.override_path, 'cinder.yml'))
        os.rmdir(self.override_path)


class TestOverridingEnvVars(OverridingEnvBase):

    def test_cinder_metal_override(self):
        vol = self.cinder_config['container_skel']['cinder_volumes_container']
        vol['properties']['is_metal'] = False

        self.write_override_env()

        fs.load_environment(TARGET_DIR, self.base_env)

        test_vol = self.base_env['container_skel']['cinder_volumes_container']
        self.assertFalse(test_vol['properties']['is_metal'])

    def test_deleting_elements(self):
        # Leave only the 'properties' dictionary attached to simulate writing
        # a partial override file

        vol = self.cinder_config['container_skel']['cinder_volumes_container']
        keys = vol.keys()
        to_delete = []
        for key in vol.keys():
            if not key == 'properties':
                to_delete.append(key)

        for key in to_delete:
            del vol[key]

        self.write_override_env()

        fs.load_environment(TARGET_DIR, self.base_env)

        test_vol = self.base_env['container_skel']['cinder_volumes_container']

        self.assertIn('belongs_to', test_vol)

    def test_adding_new_keys(self):
        vol = self.cinder_config['container_skel']['cinder_volumes_container']
        vol['a_new_key'] = 'Added'

        self.write_override_env()

        fs.load_environment(TARGET_DIR, self.base_env)

        test_vol = self.base_env['container_skel']['cinder_volumes_container']

        self.assertIn('a_new_key', test_vol)
        self.assertEqual(test_vol['a_new_key'], 'Added')

    def test_emptying_dictionaries(self):
        self.cinder_config['container_skel']['cinder_volumes_container'] = {}

        self.write_override_env()

        fs.load_environment(TARGET_DIR, self.base_env)

        test_vol = self.base_env['container_skel']['cinder_volumes_container']

        self.assertNotIn('belongs_to', test_vol)

    def test_emptying_lists(self):
        vol = self.cinder_config['container_skel']['cinder_volumes_container']
        vol['belongs_to'] = []

        self.write_override_env()

        fs.load_environment(TARGET_DIR, self.base_env)

        test_vol = self.base_env['container_skel']['cinder_volumes_container']

        self.assertEqual(test_vol['belongs_to'], [])


class TestOverridingEnvIntegration(OverridingEnvBase):
    def setUp(self):
        super(TestOverridingEnvIntegration, self).setUp()
        self.user_defined_config = get_config()

        # Inventory is necessary since keys are assumed present
        self.inv, path = fs.load_inventory(TARGET_DIR, di.INVENTORY_SKEL)

    def skel_setup(self):
        self.environment = fs.load_environment(TARGET_DIR, self.base_env)

        di.skel_setup(self.environment, self.inv)

        di.skel_load(
            self.environment.get('physical_skel'),
            self.inv
        )

    def test_emptying_container_integration(self):
        self.cinder_config = {}
        self.cinder_config['container_skel'] = {'cinder_volumes_container': {}}

        self.write_override_env()
        self.skel_setup()

        di.container_skel_load(
            self.environment.get('container_skel'),
            self.inv,
            self.user_defined_config
        )

        test_vol = self.base_env['container_skel']['cinder_volumes_container']

        self.assertNotIn('belongs_to', test_vol)
        self.assertNotIn('contains', test_vol)

    def test_empty_contains(self):
        vol = self.cinder_config['container_skel']['cinder_volumes_container']
        vol['contains'] = []

        self.write_override_env()
        self.skel_setup()

        di.container_skel_load(
            self.environment.get('container_skel'),
            self.inv,
            self.user_defined_config
        )

        test_vol = self.base_env['container_skel']['cinder_volumes_container']

        self.assertEqual(test_vol['contains'], [])

    def test_empty_belongs_to(self):
        vol = self.cinder_config['container_skel']['cinder_volumes_container']
        vol['belongs_to'] = []

        self.write_override_env()
        self.skel_setup()

        di.container_skel_load(
            self.environment.get('container_skel'),
            self.inv,
            self.user_defined_config
        )

        test_vol = self.base_env['container_skel']['cinder_volumes_container']

        self.assertEqual(test_vol['belongs_to'], [])

    def tearDown(self):
        super(TestOverridingEnvIntegration, self).tearDown()
        self.user_defined_config = None
        self.inv = None


class TestSetUsedIPS(unittest.TestCase):
    def setUp(self):
        # Clean up the used ips in case other tests didn't.
        di.ip.USED_IPS = set()

        # Create a fake inventory just for this test.
        self.inventory = {'_meta': {'hostvars': {
            'host1': {'container_networks': {
                'net': {'address': '172.12.1.1'}
            }},
            'host2': {'container_networks': {
                'net': {'address': '172.12.1.2'}
            }},
        }}}

    def test_adding_inventory_used_ips(self):
        config = {'used_ips': None}

        # TODO(nrb): This is a smell, needs to set more directly

        di.ip.set_used_ips(config, self.inventory)

        self.assertEqual(len(di.ip.USED_IPS), 2)
        self.assertIn('172.12.1.1', di.ip.USED_IPS)
        self.assertIn('172.12.1.2', di.ip.USED_IPS)

    def tearDown(self):
        di.ip.USED_IPS = set()


class TestConfigCheckFunctional(TestConfigCheckBase):
    def duplicate_ip(self):
        ip = self.user_defined_config['log_hosts']['aio1']
        self.user_defined_config['log_hosts']['bogus'] = ip

    def test_checking_good_config(self):
        output = di.main(config=TARGET_DIR, check=True,
                         environment=BASE_ENV_DIR)
        self.assertEqual(output, 'Configuration ok!')

    def test_duplicated_ip(self):
        self.duplicate_ip()
        self.write_config()
        with self.assertRaises(di.MultipleHostsWithOneIPError) as context:
            di.main(config=TARGET_DIR, check=True, environment=BASE_ENV_DIR)
        self.assertEqual(context.exception.ip, '172.29.236.100')


class TestNetworkEntry(unittest.TestCase):
    def test_all_args_filled(self):
        entry = di.network_entry(True, 'eth1', 'br-mgmt', 'my_type', '1700')

        self.assertNotIn('interface', entry.keys())
        self.assertEqual(entry['bridge'], 'br-mgmt')
        self.assertEqual(entry['type'], 'my_type')
        self.assertEqual(entry['mtu'], '1700')

    def test_container_dict(self):
        entry = di.network_entry(False, 'eth1', 'br-mgmt', 'my_type', '1700')

        self.assertEqual(entry['interface'], 'eth1')


class TestDebugLogging(unittest.TestCase):
    @mock.patch('osa_toolkit.generate.logging')
    @mock.patch('osa_toolkit.generate.logger')
    def test_logging_enabled(self, mock_logger, mock_logging):
        # Shadow the real value so tests don't complain about it
        mock_logging.DEBUG = 10

        get_inventory(extra_args={"debug": True})

        self.assertTrue(mock_logging.basicConfig.called)
        self.assertTrue(mock_logger.info.called)
        self.assertTrue(mock_logger.debug.called)

    @mock.patch('osa_toolkit.generate.logging')
    @mock.patch('osa_toolkit.generate.logger')
    def test_logging_disabled(self, mock_logger, mock_logging):
        get_inventory(extra_args={"debug": False})

        self.assertFalse(mock_logging.basicConfig.called)
        # Even though logging is disabled, we still call these
        # all over the place; they just choose not to do anything.
        # NOTE: No info messages are published when debug is False
        self.assertTrue(mock_logger.debug.called)


class TestLxcHosts(TestConfigCheckBase):

    def test_lxc_hosts_group_present(self):
        inventory = get_inventory()
        self.assertIn('lxc_hosts', inventory)

    def test_lxc_hosts_only_inserted_once(self):
        inventory = get_inventory()
        self.assertEqual(1, len(inventory['lxc_hosts']['hosts']))

    def test_lxc_hosts_members(self):
        self.add_host('shared-infra_hosts', 'aio2', '172.29.236.101')
        inventory = get_inventory()
        self.assertIn('aio2', inventory['lxc_hosts']['hosts'])
        self.assertIn('aio1', inventory['lxc_hosts']['hosts'])

    def test_lxc_hosts_in_config_raises_error(self):
        self.add_config_key('lxc_hosts', {})
        with self.assertRaises(di.LxcHostsDefined):
            get_inventory()

    def test_host_without_containers(self):
        self.add_host('compute_hosts', 'compute1', '172.29.236.102')
        inventory = get_inventory()
        self.assertNotIn('compute1', inventory['lxc_hosts']['hosts'])

    def test_cleaning_bad_hosts(self):
        self.add_host('compute_hosts', 'compute1', '172.29.236.102')
        inventory = get_inventory()
        # insert compute1 into lxc_hosts, which mimicks bug behavior
        inventory['lxc_hosts']['hosts'].append('compute1')
        faked_path = INV_DIR

        with mock.patch('osa_toolkit.filesystem.load_inventory') as inv_mock:
            inv_mock.return_value = (inventory, faked_path)
            new_inventory = get_inventory()
        # host should no longer be in lxc_hosts

        self.assertNotIn('compute1', new_inventory['lxc_hosts']['hosts'])

    def test_emptying_lxc_hosts(self):
        """If lxc_hosts is deleted between runs, it should re-populate"""

        inventory = get_inventory()
        original_lxc_hosts = inventory.pop('lxc_hosts')

        self.assertNotIn('lxc_hosts', inventory.keys())

        faked_path = INV_DIR
        with mock.patch('osa_toolkit.filesystem.load_inventory') as inv_mock:
            inv_mock.return_value = (inventory, faked_path)
            new_inventory = get_inventory()

        self.assertEqual(original_lxc_hosts, new_inventory['lxc_hosts'])


class TestConfigMatchesEnvironment(unittest.TestCase):
    def setUp(self):
        self.env = fs.load_environment(BASE_ENV_DIR, {})

    def test_matching_keys(self):
        config = get_config()

        result = di._check_all_conf_groups_present(config, self.env)
        self.assertTrue(result)

    def test_failed_match(self):
        bad_config = get_config()
        bad_config['bogus_key'] = []

        result = di._check_all_conf_groups_present(bad_config, self.env)
        self.assertFalse(result)

    def test_extra_config_key_warning(self):
        bad_config = get_config()
        bad_config['bogus_key'] = []
        with warnings.catch_warnings(record=True) as wl:
            di._check_all_conf_groups_present(bad_config, self.env)
            self.assertEqual(1, len(wl))
            self.assertIn('bogus_key', str(wl[0].message))

    def test_multiple_extra_keys(self):
        bad_config = get_config()
        bad_config['bogus_key1'] = []
        bad_config['bogus_key2'] = []

        with warnings.catch_warnings(record=True) as wl:
            di._check_all_conf_groups_present(bad_config, self.env)
            self.assertEqual(2, len(wl))
            warn_msgs = [str(warn.message) for warn in wl]
            warn_msgs.sort()
            self.assertIn('bogus_key1', warn_msgs[0])
            self.assertIn('bogus_key2', warn_msgs[1])

    def test_confirm_exclusions(self):
        """Ensure the excluded keys in the function are present."""
        config = get_config()
        excluded_keys = ('global_overrides', 'cidr_networks', 'used_ips')

        for key in excluded_keys:
            config[key] = 'sentinel value'

        with warnings.catch_warnings(record=True) as wl:
            di._check_all_conf_groups_present(config, self.env)
            self.assertEqual(0, len(wl))

        for key in excluded_keys:
            self.assertIn(key, config.keys())


class TestInventoryGroupConstraints(unittest.TestCase):
    def setUp(self):
        self.env = fs.load_environment(BASE_ENV_DIR, {})

    def test_group_with_hosts_dont_have_children(self):
        """Require that groups have children groups or hosts, not both."""
        inventory = get_inventory()

        # This should only work on groups, but stuff like '_meta' and 'all'
        # are in here, too.
        for key, values in inventory.items():
            # The keys for children/hosts can exist, the important part is being empty lists.
            has_children = bool(inventory.get('children'))
            has_hosts = bool(inventory.get('hosts'))

            self.assertFalse(has_children and has_hosts)

    def _create_bad_env(self, env):
        # This environment setup is used because it was reported with
        # bug #1646136
        override = """
            physical_skel:
              local-compute_containers:
                belongs_to:
                - compute_containers
              local-compute_hosts:
                belongs_to:
                - compute_hosts
              rbd-compute_containers:
                belongs_to:
                - compute_containers
              rbd-compute_hosts:
                belongs_to:
                - compute_hosts
        """

        bad_env = yaml.safe_load(override)

        # This is essentially what load_environment does, after all the file
        # system walking
        dictutils.merge_dict(env, bad_env)

        return env

    def test_group_with_hosts_and_children_fails(self):
        """Integration test making sure the whole script fails."""
        env = self._create_bad_env(self.env)


        config = get_config()

        kwargs = {
            'load_environment': mock.DEFAULT,
            'load_user_configuration': mock.DEFAULT
        }

        with mock.patch.multiple('osa_toolkit.filesystem', **kwargs) as mocks:
            mocks['load_environment'].return_value = env
            mocks['load_user_configuration'].return_value = config

            with self.assertRaises(di.GroupConflict) as context:
                get_inventory()

    def test_group_validation_unit(self):
        env = self._create_bad_env(self.env)

        config = get_config()

        with self.assertRaises(di.GroupConflict):
            di._check_group_branches(config, env['physical_skel'])

    def test_group_validation_no_config(self):
        result = di._check_group_branches(None, self.env)
        self.assertTrue(result)

    def test_group_validation_passes_defaults(self):
        config = get_config()

        result = di._check_group_branches(config, self.env['physical_skel'])

        self.assertTrue(result)

class TestL3ProviderNetworkConfig(TestConfigCheckBase):
    def setUp(self):
        super(TestL3ProviderNetworkConfig, self).setUp()
        self.delete_provider_network('container')
        self.add_provider_network('pod1_container', '172.29.236.0/22')
        self.add_provider_network_key('container', 'ip_from_q',
                                      'pod1_container')
        self.add_provider_network_key('pod1_container', 'address_prefix',
                                      'management')
        self.add_provider_network_key('pod1_container', 'reference_group',
                                      'pod1_hosts')
        self.add_config_key('pod1_hosts', {})
        self.add_host('pod1_hosts', 'aio2', '172.29.236.101')
        self.add_host('compute_hosts', 'aio2', '172.29.236.101')
        self.write_config()
        self.inventory = get_inventory()

    def test_address_prefix_name_applied(self):
         aio2_host_vars = self.inventory['_meta']['hostvars']['aio2']
         aio2_container_networks = aio2_host_vars['container_networks']
         self.assertIsInstance(aio2_container_networks['management_address'],
                               dict)

    def test_host_outside_reference_group_excluded(self):
         aio1_host_vars = self.inventory['_meta']['hostvars']['aio1']
         aio1_container_networks = aio1_host_vars['container_networks']
         self.assertNotIn('management_address', aio1_container_networks)

if __name__ == '__main__':
    unittest.main(catchbreak=True)




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\test_ip.py
===========File Type===========
.py
===========File Content===========
#!/usr/bin/env python

# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import unittest

from osa_toolkit import ip


class TestIPManager(unittest.TestCase):
    def test_basic_instantiation(self):
        manager = ip.IPManager()

        self.assertEqual({}, manager.queues)
        self.assertEqual(set(), manager.used)

    def test_verbose_instantiation(self):
        manager = ip.IPManager(queues={'test': '192.168.0.0/24'},
                               used_ips=set(['192.168.0.0', '192.168.0.255']))
        self.assertEqual(2, len(manager.used))
        self.assertEqual(254, len(manager.queues['test']))

    def test_instantiation_with_used_list(self):
        manager = ip.IPManager(used_ips=['192.168.0.0', '192.168.0.255'])

        self.assertEqual(2, len(manager.used))

    def test_verbose_instantiation_duplicated_ips(self):
        manager = ip.IPManager(used_ips=['192.168.0.0', '192.168.0.0'])

        self.assertEqual(1, len(manager.used))

    def test_deleting_used(self):
        manager = ip.IPManager(used_ips=set(['192.168.1.1']))

        del manager.used

        self.assertEqual(set(), manager.used)

    def test_getitem(self):
        manager = ip.IPManager(queues={'test': '192.168.0.0/24'})

        self.assertEqual(manager.queues['test'], manager['test'])

    def test_loading_queue(self):
        manager = ip.IPManager()
        manager.load('test', '192.168.0.0/24')
        self.assertEqual(254, len(manager.queues['test']))

    def test_loading_network_excludes(self):
        manager = ip.IPManager()
        manager.load('test', '192.168.0.0/24')
        self.assertNotIn('192.168.0.0', manager.queues['test'])
        self.assertNotIn('192.168.0.255', manager.queues['test'])

    def test_loading_used_ips(self):
        manager = ip.IPManager()
        manager.load('test', '192.168.0.0/24')

        self.assertEqual(2, len(manager.used))
        self.assertIn('192.168.0.0', manager.used)
        self.assertIn('192.168.0.255', manager.used)

    def test_load_creates_networks(self):
        manager = ip.IPManager()
        manager.load('test', '192.168.0.0/24')

        self.assertIn('test', manager._networks)

    def test_loaded_randomly(self):
        manager = ip.IPManager()
        manager.load('test', '192.168.0.0/24')

        self.assertNotEqual(['192.168.0.1', '192.168.0.2', '192.168.0.3'],
                            manager.queues['test'][0:3])

    def test_getting_ip(self):
        manager = ip.IPManager(queues={'test': '192.168.0.0/24'})
        my_ip = manager.get('test')

        self.assertTrue(my_ip.startswith('192.168.0'))
        self.assertIn(my_ip, manager.used)
        self.assertNotIn(my_ip, manager.queues['test'])

    def test_getting_ip_from_empty_queue(self):
        manager = ip.IPManager(queues={'test': '192.168.0.0/31'})
        # There will only be 1 usable IP address in this range.
        manager.get('test')

        with self.assertRaises(ip.EmptyQueue):
            manager.get('test')

    def test_get_ip_from_missing_queue(self):
        manager = ip.IPManager()

        with self.assertRaises(ip.NoSuchQueue):
            manager.get('management')

    def test_release_used_ip(self):
        target_ip = '192.168.0.1'
        manager = ip.IPManager(queues={'test': '192.168.0.0/31'},
                               used_ips=[target_ip])

        manager.release(target_ip)

        # No broadcast address on this network, so only the network addr left
        self.assertEqual(1, len(manager.used))
        self.assertNotIn(target_ip, manager.used)
        self.assertIn(target_ip, manager['test'])

    def test_save_not_implemented(self):
        manager = ip.IPManager()

        with self.assertRaises(NotImplementedError):
            manager.save()

    def test_queue_dict_copied(self):
        manager = ip.IPManager(queues={'test': '192.168.0.0/31'})
        external = manager.queues
        self.assertIsNot(manager.queues, external)
        self.assertIsNot(manager.queues['test'], external['test'])

    def test_queue_list_copied(self):
        manager = ip.IPManager(queues={'test': '192.168.0.0/31'})
        external = manager['test']
        # test against the internal structure since .queues should
        # itself be making copies
        self.assertIsNot(manager._queues['test'], external)

    def test_used_ips_copies(self):
        manager = ip.IPManager(used_ips=['192.168.0.1'])
        external = manager.used
        self.assertIsNot(manager._used_ips, external)

    def test_deleting_used_ips_releases_to_queues(self):
        target_ip = '192.168.0.1'
        manager = ip.IPManager(queues={'test': '192.168.0.0/31'},
                               used_ips=[target_ip])

        del manager.used

        self.assertIn(target_ip, manager['test'])


if __name__ == "__main__":
    unittest.main()




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\test_manage.py
===========File Type===========
.py
===========File Content===========
#!/usr/bin/env python

# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import os
from os import path
from osa_toolkit import manage as mi
import test_inventory
import unittest

TARGET_DIR = path.join(os.getcwd(), 'tests', 'inventory')


def setUpModule():
    test_inventory.make_config()


def tearDownModule():
    os.remove(test_inventory.USER_CONFIG_FILE)


class TestExportFunction(unittest.TestCase):
    def setUp(self):
        self.inv = test_inventory.get_inventory()

    def tearDown(self):
        test_inventory.cleanup()

    def test_host_is_present(self):
        host_inv = mi.export_host_info(self.inv)['hosts']
        self.assertIn('aio1', host_inv.keys())

    def test_groups_added(self):
        host_inv = mi.export_host_info(self.inv)['hosts']
        self.assertIn('groups', host_inv['aio1'].keys())

    def test_variables_added(self):
        host_inv = mi.export_host_info(self.inv)['hosts']
        self.assertIn('hostvars', host_inv['aio1'].keys())

    def test_number_of_hosts(self):
        host_inv = mi.export_host_info(self.inv)['hosts']

        self.assertEqual(len(self.inv['_meta']['hostvars']),
                         len(host_inv))

    def test_all_information_added(self):
        all_info = mi.export_host_info(self.inv)['all']
        self.assertIn('provider_networks', all_info)

    def test_all_lb_information(self):
        all_info = mi.export_host_info(self.inv)['all']
        inv_all = self.inv['all']['vars']
        self.assertEqual(inv_all['internal_lb_vip_address'],
                         all_info['internal_lb_vip_address'])


class TestRemoveIpfunction(unittest.TestCase):
    def setUp(self):
        self.inv = test_inventory.get_inventory()

    def tearDown(self):
        test_inventory.cleanup()

    def test_ips_removed(self):
        mi.remove_ip_addresses(self.inv)
        mi.remove_ip_addresses(self.inv, TARGET_DIR)
        hostvars = self.inv['_meta']['hostvars']

        for host, variables in hostvars.items():
            has_networks = 'container_networks' in variables
            if variables.get('is_metal', False):
                continue
            self.assertFalse(has_networks)

    def test_inventory_item_removed(self):
        inventory = self.inv

        # Make sure we have log_hosts in the original inventory
        self.assertIn('log_hosts', inventory)

        mi.remove_inventory_item("log_hosts", inventory)
        mi.remove_inventory_item("log_hosts", inventory, TARGET_DIR)

        # Now make sure it's gone
        self.assertIn('log_hosts', inventory)

    def test_metal_ips_kept(self):
        mi.remove_ip_addresses(self.inv)
        hostvars = self.inv['_meta']['hostvars']

        for host, variables in hostvars.items():
            has_networks = 'container_networks' in variables
            if not variables.get('is_metal', False):
                continue
            self.assertTrue(has_networks)

    def test_ansible_host_vars_removed(self):
        mi.remove_ip_addresses(self.inv)
        hostvars = self.inv['_meta']['hostvars']

        for host, variables in hostvars.items():
            has_host = 'ansible_host' in variables
            if variables.get('is_metal', False):
                continue
            self.assertFalse(has_host)

    def test_multiple_calls(self):
        """Removal should fail silently if keys are absent."""
        mi.remove_ip_addresses(self.inv)
        mi.remove_ip_addresses(self.inv)


if __name__ == '__main__':
    unittest.main(catchbreak=True)




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\inventory\.gitkeep
===========File Type===========

===========File Content===========
Keep this file to keep the directory.




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\defaults\main.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2015, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

## AIO user-space configuration options
# Scenario used to bootstrap the host
bootstrap_host_scenario: "{{ lookup('env','SCENARIO') | default('aio_lxc', true) }}"
#
# Boolean option to implement OpenStack-Ansible configuration for an AIO
# Switch to no for a multi-node configuration
bootstrap_host_aio_config: yes
#
# Path to the location of the bootstrapping configuration files
bootstrap_host_aio_config_path: "{{ playbook_dir }}/../etc/openstack_deploy"
#
# Path to the location of the scripts the bootstrap scripts use
bootstrap_host_aio_script_path: "{{ playbook_dir }}/../scripts"
#
# The user space configuration file names to use
bootstrap_host_user_variables_filename: "user_variables.yml"
bootstrap_host_user_secrets_filename: "user_secrets.yml"
#
# Paths to configuration file targets that should be created by the bootstrap
bootstrap_host_target_config_paths:
  - /etc/openstack_deploy
  - /etc/openstack_deploy/conf.d
  - /etc/openstack_deploy/env.d

# The user variables template to use
bootstrap_user_variables_template: user_variables.aio.yml.j2

# Extra user variables files can be loaded into /etc/openstack_deploy by
# test scenarios. The dict uses scenario as the key to load a list of extra
# templates if necessary.
bootstrap_user_variables_extra_templates:
  ceph:
    - src: user_variables_ceph.yml.j2
      dest: user_variables_ceph.yml
  congress:
    - src: user_variables_congress.yml.j2
      dest: user_variables_congress.yml
  translations:
    - src: user_variables_translations.yml.j2
      dest: user_variables_translations.yml
  barbican:
    - src: user_variables_barbican.yml.j2
      dest: user_variables_barbican.yml

## Loopback volumes
# Sparse loopback disks are used for the containers if there is no secondary
# disk available to partition for btrfs. They are also used for Ceph, Cinder,
# Swift and Nova (instance storage).
# The size of the loopback volumes can be customized here (in gigabytes).
#
# Boolean option to deploy the loopback disk for Swap
bootstrap_host_loopback_swap: yes
# Size of the Swap loopback disk in gigabytes (GB).
bootstrap_host_loopback_swap_size: 4096
#
# Boolean option to deploy the loopback disk for Cinder
bootstrap_host_loopback_cinder: yes
# Size of the Cinder loopback disk in gigabytes (GB).
bootstrap_host_loopback_cinder_size: 1024
#
# Boolean option to deploy the loopback disk for Swift
bootstrap_host_loopback_swift: yes
# Size of the Swift loopback disk in gigabytes (GB).
bootstrap_host_loopback_swift_size: 1024
#
# Boolean option to deploy the loopback disk for Nova
bootstrap_host_loopback_nova: yes
# Size of the Nova loopback disk in gigabytes (GB).
bootstrap_host_loopback_nova_size: 1024
#
# Boolean option to deploy the loopback disk for machines
bootstrap_host_loopback_machines: yes
# Size of the machines loopback disk in gigabytes (GB).
bootstrap_host_loopback_machines_size: 128
#
# Boolean option to deploy the loopback disk for btrfs
bootstrap_host_loopback_btrfs: yes
# Size of the btrfs loopback disk in gigabytes (GB).
bootstrap_host_loopback_btrfs_size: 1024
#
# Boolean option to deploy the loopback disk for btrfs
bootstrap_host_loopback_zfs: yes
# Size of the btrfs loopback disk in gigabytes (GB).
bootstrap_host_loopback_zfs_size: 1024
#
# Boolean option to deploy the OSD loopback disks and cluster UUID for Ceph
bootstrap_host_ceph: "{{ (bootstrap_host_scenario == 'ceph') | bool }}"
# Size of the Ceph OSD loopbacks
bootstrap_host_loopback_ceph_size: 1024
# Ceph OSDs to create on the AIO host
ceph_osd_images:
  - 'ceph1'
  - 'ceph2'
  - 'ceph3'

## Network configuration
# Default network IP ranges
mgmt_range: "172.29.236"
vxlan_range: "172.29.240"
storage_range: "172.29.244"
vlan_range: "172.29.248"
netmask: "255.255.252.0"
#
# NICs
bootstrap_host_public_interface: "{{ ansible_default_ipv4.interface }}"
#
# Utility paths
bootstrap_host_network_utils:
  apt:
    iptables: /sbin/iptables
    ethtool: /sbin/ethtool
    ip: /sbin/ip
  yum:
    iptables: /usr/sbin/iptables
    ethtool: /usr/sbin/ethtool
    ip: /usr/sbin/ip
  zypper:
    iptables: /usr/sbin/iptables
    ethtool: /sbin/ethtool
    ip: /sbin/ip
#
bootstrap_host_iptables_path: "{{ bootstrap_host_network_utils[ansible_pkg_mgr]['iptables'] }}"
bootstrap_host_ethtool_path: "{{ bootstrap_host_network_utils[ansible_pkg_mgr]['ethtool'] }}"
bootstrap_host_ip_path: "{{ bootstrap_host_network_utils[ansible_pkg_mgr]['ip'] }}"

## Extra storage
# An AIO may optionally be built using a second storage device. If a
# secondary disk device to use is not specified, then the AIO will be
# built on any existing disk partitions.
#
# WARNING: The data on a secondary storage device specified here will
# be destroyed and repartitioned.
#
# Specify the secondary disk device to use. When the data disk is in use, no NOT
# set the full path to the device. IE: "/dev/xvde" should be "xvde".
bootstrap_host_data_disk_device: null
#
# Specify the default filesystem type
bootstrap_host_data_disk_fs_type: ext4
#
# Boolean value to force the repartitioning of the secondary device.
bootstrap_host_data_disk_device_force: no
#
# If the storage capacity on this device is greater than or equal to this
# size (in GB), the bootstrap process will use it.
bootstrap_host_data_disk_min_size: 50
#
# Set the data disk formats table. If the backing store is set to lvm the option
# the partition will not actually be formatted however for parted, ext2 is used.
bootstrap_host_data_disk2_formats:
  machinectl: btrfs
  zfs: zfs
  btrfs: btrfs
  xfs: xfs
  dir: ext4
  lvm: ext2

bootstrap_host_format_options:
  machinectl: '--metadata single --data single --mixed'
  btrfs: '--metadata single --data single --mixed'
  xfs: '-K -d agcount=64 -l size=128m'
  ext4: '-O dir_index'

#
# Set the data disk mount options.
bootstrap_host_data_mount_options:
  machinectl: "noatime,nodiratime,compress=lzo,commit=120,{{ (ansible_kernel is version_compare('4.5', '>=')) | ternary('space_cache=v2', 'space_cache') }}"
  zfs: "defaults"
  btrfs: "noatime,nodiratime,compress=lzo,commit=120,{{ (ansible_kernel is version_compare('4.5', '>=')) | ternary('space_cache=v2', 'space_cache') }}"
  xfs: "noatime,nodiratime,nobarrier,logbufs=8,logbsize=256k"
  ext4: "noatime,nobh,barrier=0,data=writeback"
  dir: "defaults"
  lvm: "defaults"
  swap: "%%"

bootstrap_host_data_disk2_fs: "{{ bootstrap_host_data_disk2_formats[((bootstrap_host_container_tech == 'nspawn') | ternary('btrfs', lxc_container_backing_store))] }}"
bootstrap_host_data_disk2_fs_mount_options: "{{ bootstrap_host_data_mount_options[((bootstrap_host_container_tech == 'nspawn') | ternary('btrfs', lxc_container_backing_store))] }}"
bootstrap_host_data_disk2_path: "{{ (lxc_container_backing_store == 'machinectl' or bootstrap_host_container_tech == 'nspawn') | ternary('/var/lib/machines', '/var/lib/lxc') }}"

### Optional Settings ###

# Specify the public IP address for the host.
# By default the address will be set to the ipv4 address of the
# host's network interface that has the default route on it.
#bootstrap_host_public_address: 0.0.0.0

# Set the install method for the deployment. Options are ['source', 'distro']
bootstrap_host_install_method: "{{ lookup('env', 'INSTALL_METHOD') | default('source', true)  }}"

# Set the container technology in service. Options are nspawn and lxc.
bootstrap_host_container_tech: "{{ (bootstrap_host_scenario is search('nspawn')) | ternary('nspawn', 'lxc') }}"

# Set the lxc backing store for the job
lxc_container_backing_store: dir




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\files\user_variables_proxy.yml
===========File Type===========
.yml
===========File Content===========
---
no_proxy_env: "localhost,127.0.0.1,{{ internal_lb_vip_address }},{{ external_lb_vip_address }},{% for host in groups['all_containers'] %}{{ hostvars[host]['container_address'] }}{% if not loop.last %},{% endif %}{% endfor %}"
http_proxy_env: "{{ lookup('env', 'http_proxy') }}"
https_proxy_env: "{{ lookup('env', 'https_proxy') }}"
global_environment_variables:
   HTTP_PROXY: "{{ http_proxy_env }}"
   HTTPS_PROXY: "{{ https_proxy_env }}"
   http_proxy: "{{ http_proxy_env }}"
   https_proxy: "{{ https_proxy_env }}"
   NO_PROXY: "{{ no_proxy_env }}"
   no_proxy: "{{ no_proxy_env }}"




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\check-requirements.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2015, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Check for a supported Operating System
  assert:
    that:
      - (ansible_distribution == 'Ubuntu' and ansible_distribution_release == 'xenial') or
        (ansible_distribution == 'Ubuntu' and ansible_distribution_release == 'bionic') or
        (ansible_os_family == 'RedHat' and ansible_distribution_major_version == '7') or
        (ansible_os_family == 'Suse' and ansible_distribution_major_version in ['42', '15'])
    msg: "The only supported platforms for this release are Ubuntu 16.04 LTS (Xenial), Ubuntu 18.04 LTS (Bionic), CentOS 7 (WIP), openSUSE Leap 42.X and openSUSE Leap 15.X"
  when: (check_operating_system | default(True))| bool
  tags:
    - check-operating-system

- name: Identify the space available in /
  # NOTE(hwoarang): df does not work reliably on btrfs filesystems
  # https://btrfs.wiki.kernel.org/index.php/FAQ#How_much_free_space_do_I_have.3F
  # As such, use the btrfs tools to determine the real available size on the
  # disk
  shell: |
    if [[ $(df -T / | tail -n 1 | awk '{print $2}') == "btrfs" ]]; then
        btrfs fi usage --kbytes / | awk '/^.*Free / {print $3}'| sed 's/\..*//'
    else
        df -BK / | awk '!/^Filesystem/ {print $4}' | sed 's/K//'
    fi
  when:
    - bootstrap_host_data_disk_device == None
  changed_when: false
  register: root_space_available
  tags:
    - check-disk-size

# Convert root_space_available to bytes.
- name: Set root disk facts
  set_fact:
    host_root_space_available_bytes: "{{ ( root_space_available.stdout | int) * 1024 | int }}"
  when:
    - bootstrap_host_data_disk_device == None
  tags:
    - check-disk-size

- name: Fail when disk can not be found
  fail:
    msg: |
      Can not find disk {{ bootstrap_host_data_disk_device }}
  when:
    - bootstrap_host_data_disk_device != None
    - ansible_devices.get(bootstrap_host_data_disk_device) == None
  tags:
    - check-disk-size

- name: Set data disk facts
  set_fact:
    host_data_disk_sectors: "{{ (ansible_devices[bootstrap_host_data_disk_device]['sectors'] | int) }}"
    host_data_disk_sectorsize: "{{ (ansible_devices[bootstrap_host_data_disk_device]['sectorsize'] | int) }}"
  when:
    - bootstrap_host_data_disk_device != None
  tags:
    - check-disk-size

# Calculate the size of the bootstrap_host_data_disk_device by muliplying sectors with sectorsize.
- name: Calculate data disk size
  set_fact:
    host_data_disk_size_bytes: "{{ ((host_data_disk_sectors | int) * (host_data_disk_sectorsize | int)) | int }}"
  when:
    - bootstrap_host_data_disk_device != None
  tags:
    - check-disk-size

# Convert bootstrap_host_data_disk_min_size to bytes.
- name: Set min size fact
  set_fact:
    host_data_disk_min_size_bytes: "{{ ((bootstrap_host_data_disk_min_size | int) * 1024**3) | int }}"
  tags:
    - check-disk-size

- name: Set size facts
  set_fact:
    root_gb_available: "{{ ((host_root_space_available_bytes | int ) / 1024**3) | round(2, 'floor') }}"
  when: bootstrap_host_data_disk_device == None
  tags:
    - check-disk-size

- name: Set disk size facts
  set_fact:
    disk_gb_available: "{{ ((host_data_disk_size_bytes | int ) / 1024**3) | round(2, 'floor') }}"
  when: bootstrap_host_data_disk_device != None
  tags:
    - check-disk-size

- name: Fail if there is not enough space available in /
  fail:
    msg: |
      Not enough space available in /.
      Found {{ root_gb_available }} GB, required {{ bootstrap_host_data_disk_min_size }} GB)
  when:
    - bootstrap_host_data_disk_device == None
    - (host_root_space_available_bytes | int) < (host_data_disk_min_size_bytes | int)
  tags:
    - check-disk-size

- name: Fail if there is not enough disk space available (disk specified)
  fail:
    msg: |
      Not enough disk space available.
      Found {{ disk_gb_available }} GB, required {{ bootstrap_host_data_disk_min_size }} GB)
  when:
    - bootstrap_host_data_disk_device != None
    - (host_data_disk_size_bytes | int) < (host_data_disk_min_size_bytes | int)
  tags:
    - check-disk-size

- name: Ensure that the kernel has VXLAN, VLAN, and bonding support
  modprobe:
    name: "{{ item }}"
    state: present
  with_items:
    - vxlan
    - bonding
    - 8021q




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\install_packages.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2015, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Remove known problem packages
  package:
    name: "{{ packages_remove }}"
    state: absent
  tags:
    - remove-packages

- name: Install packages
  package:
    name: "{{ packages_install }}"
    state: present
    update_cache: "{{ (ansible_pkg_mgr in ['apt', 'zypper']) | ternary('yes', omit) }}"
  tags:
    - install-packages





===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\main.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2015, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Before we do anything, check the minimum requirements
- include: check-requirements.yml
  tags:
    - check-requirements

# We will look for the most specific variable files first and eventually
# end up with the least-specific files.
- name: Gather variables for each operating system
  include_vars: "{{ item }}"
  with_first_found:
    - "{{ ansible_distribution | lower }}-{{ ansible_distribution_version | lower }}.yml"
    - "{{ ansible_distribution | lower }}-{{ ansible_distribution_major_version | lower }}.yml"
    - "{{ ansible_os_family | lower }}-{{ ansible_distribution_major_version | lower }}.yml"
    - "{{ ansible_distribution | lower }}.yml"
    - "{{ ansible_os_family | lower }}.yml"
  tags:
    - always

- name: Create the required directories
  file:
    path: "{{ item }}"
    state: directory
  with_items:
    - "/openstack"
  tags:
    - create-directories

- include: install_packages.yml
  tags:
    - install-packages

# Prepare the data disk, if one is provided
- include: prepare_data_disk.yml
  when:
    - bootstrap_host_data_disk_device != None
  tags:
    - prepare-data-disk

# Prepare the swap space loopback disk
# This is only necessary if there isn't swap already
- include: prepare_loopback_swap.yml
  static: no
  when:
    - bootstrap_host_loopback_swap | bool
    - ansible_swaptotal_mb < 1
  tags:
    - prepare-loopback

# Prepare the Machines storage loopback disk
- include: prepare_loopback_machines.yml
  when:
    - bootstrap_host_loopback_machines | bool
    - bootstrap_host_data_disk_device == None
    - lxc_container_backing_store == 'machinectl' or bootstrap_host_container_tech == 'nspawn'
  tags:
    - prepare-loopback

# Prepare the zfs storage loopback disk
- include: prepare_loopback_zfs.yml
  when:
    - bootstrap_host_loopback_zfs | bool
    - bootstrap_host_data_disk_device == None
    - lxc_container_backing_store == 'zfs'
  tags:
    - prepare-loopback

# Prepare the btrfs storage loopback disk
- include: prepare_loopback_btrfs.yml
  when:
    - bootstrap_host_loopback_btrfs | bool
    - bootstrap_host_data_disk_device == None
    - lxc_container_backing_store == 'btrfs'
  tags:
    - prepare-loopback

# Prepare the Cinder LVM VG loopback disk
# This is only necessary if bootstrap_host_loopback_cinder is set to yes
- include: prepare_loopback_cinder.yml
  when:
    - bootstrap_host_loopback_cinder | bool
  tags:
    - prepare-loopback

# Prepare the Nova instance storage loopback disk
- include: prepare_loopback_nova.yml
  when:
    - bootstrap_host_loopback_nova | bool
  tags:
    - prepare-loopback

# Prepare the Swift data storage loopback disks
- include: prepare_loopback_swift.yml
  when:
    - bootstrap_host_loopback_swift | bool
  tags:
    - prepare-loopback

# Prepare the Ceph cluster UUID and loopback disks
- include: prepare_ceph.yml
  when:
    - bootstrap_host_ceph | bool
  tags:
    - prepare-ceph

# Ensure hostname/ip is consistent with inventory
- include: prepare_hostname.yml
  tags:
    - prepare-hostname

# Prepare the network interfaces
- include: prepare_networking.yml
  when:
    - bootstrap_host_container_tech != 'nspawn'
  tags:
    - prepare-networking

# Ensure that there are both private and public ssh keys for root
- include: prepare_ssh_keys.yml
  tags:
    - prepare-ssh-keys

# Put the OpenStack-Ansible configuration for an All-In-One on the host
- include: prepare_aio_config.yml
  when:
    - bootstrap_host_aio_config | bool
  tags:
    - prepare-aio-config




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\prepare_aio_config.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2015, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Create the required deployment directories
  file:
    path: "{{ item }}"
    state: directory
  with_items: "{{ bootstrap_host_target_config_paths }}"
  tags:
    - create-directories

- name: Deploy user conf.d configuration
  config_template:
    src: "{{ item.path | default(bootstrap_host_aio_config_path ~ '/conf.d') }}/{{ item.name }}"
    dest: "/etc/openstack_deploy/conf.d/{{ item.name | regex_replace('.aio$', '') }}"
    config_overrides: "{{ item.override | default({}) }}"
    config_type: "yaml"
  with_items: "{{ confd_overrides[bootstrap_host_scenario] | default([]) }}"
  tags:
    - deploy-confd

- name: Deploy openstack_user_config
  config_template:
    src: "{{ bootstrap_host_aio_config_path }}/openstack_user_config.yml.{{ (bootstrap_host_container_tech == 'nspawn') | ternary('aio-nspawn', 'aio') }}.j2"
    dest: "/etc/openstack_deploy/openstack_user_config.yml"
    config_overrides: "{{ openstack_user_config_overrides | default({}) }}"
    config_type: "yaml"
    list_extend: false
  tags:
    - deploy-openstack-user-config

- name: Deploy user_secrets file
  config_template:
    src: "{{ bootstrap_host_aio_config_path }}/user_secrets.yml"
    dest: "/etc/openstack_deploy/{{ bootstrap_host_user_secrets_filename }}"
    config_overrides: "{{ user_secrets_overrides | default({}) }}"
    config_type: "yaml"
  tags:
    - deploy-user-secrets

- name: Generate any missing values in user_secrets
  command: "/opt/ansible-runtime/bin/python {{ bootstrap_host_aio_script_path }}/pw-token-gen.py --file /etc/openstack_deploy/{{ bootstrap_host_user_secrets_filename }}"
  changed_when: false
  tags:
    - generate_secrets

- name: Detect whether the host is an OpenStack-CI host
  stat:
    path: /etc/nodepool
  register: nodepool_dir

- name: Set facts when inside of OpenStack-Infra
  when:
    - nodepool_dir.stat.exists
  block:
    - name: Discover the OpenStack-Infra mirrors
      shell: |
        source /etc/ci/mirror_info.sh
        NODEPOOL_OVERRIDES="/etc/openstack_deploy/user_openstackci.yml"
        echo "uca_apt_repo_url: '${NODEPOOL_UCA_MIRROR}'" >> ${NODEPOOL_OVERRIDES}
        echo "openstack_hosts_centos_mirror_url: '${NODEPOOL_CENTOS_MIRROR}'" >> ${NODEPOOL_OVERRIDES}
        echo "opensuse_mirror: '${NODEPOOL_OPENSUSE_MIRROR}'" >> ${NODEPOOL_OVERRIDES}
        echo "centos_epel_mirror: '${NODEPOOL_EPEL_MIRROR}'" >> ${NODEPOOL_OVERRIDES}
        echo "galera_percona_xtrabackup_repo_host: '${NODEPOOL_PERCONA_PROXY}'" >> ${NODEPOOL_OVERRIDES}
        echo "galera_repo_host: '${NODEPOOL_MIRROR_HOST}:8080'" >> ${NODEPOOL_OVERRIDES}
        echo "lxc_centos_package_baseurl: 'http://${NODEPOOL_MIRROR_HOST}:8080/copr-lxc2/epel-7-x86_64/'" >> ${NODEPOOL_OVERRIDES}
        echo "lxc_centos_package_key: 'http://${NODEPOOL_MIRROR_HOST}:8080/copr-lxc2/pubkey.gpg'" >> ${NODEPOOL_OVERRIDES}
        echo "nova_virt_type: 'qemu'" >> ${NODEPOOL_OVERRIDES}
        echo "repo_build_pip_default_index: '${NODEPOOL_PYPI_MIRROR}'" >> ${NODEPOOL_OVERRIDES}

        # NOTE(mnaser): We need to make sure we pull the latest RDO mirror
        #               which is hashed to avoid cache going stale during CI.
        export DLRN_BASE=${DLRN_BASE:-centos7-master/puppet-passed-ci}
        rdo_dlrn=`curl --silent ${NODEPOOL_RDO_PROXY}/${DLRN_BASE}/delorean.repo | grep baseurl | cut -d= -f2`
        if [[ -z "$rdo_dlrn" ]]; then
            echo "Failed to parse dlrn hash"
            exit 1
        fi
        RDO_MIRROR_HOST=${rdo_dlrn/https:\/\/trunk.rdoproject.org/$NODEPOOL_RDO_PROXY}
        echo "openstack_hosts_rdo_repo_url: '${RDO_MIRROR_HOST}'" >> ${NODEPOOL_OVERRIDES}
      args:
        executable: /bin/bash
      tags:
        - skip_ansible_lint

    - name: Discover the OpenStack-Infra pypi/wheel mirror
      shell: |
        source /etc/ci/mirror_info.sh
        echo "${NODEPOOL_PYPI_MIRROR}"
        echo "${NODEPOOL_WHEEL_MIRROR}"
      args:
        executable: /bin/bash
      register: _pypi_wheel_mirror
      tags:
        - skip_ansible_lint

    - name: Discover the OpenStack-Infra LXC reverse proxy
      shell: |
        source /etc/ci/mirror_info.sh
        echo ${NODEPOOL_LXC_IMAGE_PROXY}
      register: _lxc_mirror
      args:
        executable: /bin/bash
      tags:
        - skip_ansible_lint

    - name: Set the package cache timeout to 60 mins in OpenStack-CI
      set_fact:
        cache_timeout: 3600
      when:
        - cache_timeout is not defined

    # This is a very dirty hack due to images.linuxcontainers.org
    # constantly failing to resolve in openstack-infra.
    - name: Implement hard-coded hosts entries for consistently failing name
      lineinfile:
        path: "/etc/hosts"
        line: "{{ item }}"
        state: present
      with_items:
        - "91.189.91.21 images.linuxcontainers.org us.images.linuxcontainers.org"
        - "91.189.88.37 images.linuxcontainers.org uk.images.linuxcontainers.org"

- name: Set facts when outside of OpenStack-Infra
  when:
    - not nodepool_dir.stat.exists
  block:
    - name: Determine the fastest available OpenStack-Infra wheel mirror
      command: "{{ bootstrap_host_aio_script_path }}/fastest-infra-wheel-mirror.py"
      register: fastest_wheel_mirror

    - name: Set repo_build_pip_extra_indexes fact
      set_fact:
        repo_build_pip_extra_indexes: "{{ fastest_wheel_mirror.stdout_lines }}"

# NOTE(mhayden): The OpenStack CI images for CentOS 7 recently set SELinux to
# Enforcing mode by default. While I am normally a supporter of this change,
# the SELinux policy work for CentOS 7 is not done yet.
- name: Set SELinux to permissive mode in OpenStack-CI
  selinux:
    policy: targeted
    state: permissive
  when:
    - ansible_selinux.status == "enabled"

- name: Set the user_variables
  config_template:
    src: "{{ bootstrap_user_variables_template }}"
    dest: "/etc/openstack_deploy/{{ bootstrap_host_user_variables_filename }}"
    config_overrides: "{{ user_variables_overrides | default({}) }}"
    config_type: yaml

- name: Set http proxy user variables
  copy:
    src: "user_variables_proxy.yml"
    dest: "/etc/openstack_deploy/user_variables_proxy.yml"
  when:
    - "lookup('env', 'http_proxy')|length > 0"

- name: Drop the extra user_variables files for this scenario
  config_template:
    src: "{{ item.src }}"
    dest: "/etc/openstack_deploy/{{ item.dest }}"
    config_overrides: "{{ item.config_overrides | default({}) }}"
    config_type: yaml
  with_items: "{{ bootstrap_user_variables_extra_templates[bootstrap_host_scenario] | default([]) }}"

- name: Copy modified cinder-volume env.d file for ceph scenario
  copy:
    src: "{{ playbook_dir }}/../etc/openstack_deploy/env.d/cinder-volume.yml.container.example"
    dest: "/etc/openstack_deploy/env.d/cinder-volume.yml"
  when:
    - "bootstrap_host_scenario == 'ceph'"

- name: Copy modified env.d file for metal scenario
  copy:
    src: "{{ playbook_dir }}/../etc/openstack_deploy/env.d/aio_metal.yml.example"
    dest: "/etc/openstack_deploy/env.d/aio_metal.yml"
  when:
    - "bootstrap_host_scenario == 'aio_metal'"

- name: Create vars override folders if we need to test them
  file:
    path: "{{ item }}"
    state: directory
  with_items:
    - /etc/openstack_deploy/group_vars
    - /etc/openstack_deploy/host_vars
  when: "(lookup('env','ACTION') | default(false,true)) == 'varstest'"

- name: Create user-space overrides
  lineinfile:
    path: "{{ item.path }}"
    state: present
    line:  "{{ item.line }}"
    create: yes
  with_items:
    - path: /etc/openstack_deploy/group_vars/hosts.yml
      line: 'babar: "elephant"'
    - path: /etc/openstack_deploy/group_vars/hosts.yml
      line: 'lxc_hosts_package_state: "present"'
    - path: /etc/openstack_deploy/host_vars/localhost.yml
      line: 'security_package_state: "present"'
    - path: /etc/openstack_deploy/host_vars/localhost.yml
      line: 'tintin: "milou"'
  when: "(lookup('env','ACTION') | default(false,true)) == 'varstest'"




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\prepare_ceph.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2016, Logan Vig <logan2211@gmail.com>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Create sparse ceph OSD files
  command: truncate -s {{ bootstrap_host_loopback_ceph_size }}G /openstack/{{ item }}.img
  args:
    creates: "/openstack/{{ item }}.img"
  with_items: "{{ ceph_osd_images }}"
  register: ceph_create
  tags:
    - ceph-file-create

- name: Run the systemd service role
  include_role:
    name: systemd_service
    private: true
  vars:
    systemd_services:
      - service_name: "loop-{{ loopback_var }}"
        config_overrides:
          Unit:
            Description: no
            After: systemd-udev-settle.service
          Service:
            RemainAfterExit: yes
        service_type: oneshot
        execstarts:
          - /bin/bash -c "/sbin/losetup $(/sbin/losetup -f) /openstack/{{ loopback_var }}.img"
        execstops:
          - /bin/bash -c "losetup -d $(losetup -l | awk '/{{ loopback_var }}.img/ {print $1}')"
        enabled: yes
        state: started
    systemd_tempd_prefix: openstack
  with_items: "{{ ceph_osd_images }}"
  loop_control:
    loop_var: loopback_var
  tags:
    - ceph-config

- name: Get loopback device
  shell: "losetup -l | awk '/{{ item }}.img/ {print $1}'"
  changed_when: false
  register: ceph_create_loopback
  with_items: "{{ ceph_osd_images }}"
  tags:
    - skip_ansible_lint

# TODO(logan): Move these vars to user_variables.ceph.yml.j2 once LP #1649381
# is fixed and eliminate this task.
- name: Write ceph cluster config
  copy:
    content: |
      ---
      devices: {{ ceph_create_loopback.results | map(attribute='stdout') | list | to_yaml | trim }}
      cinder_backends:
        "RBD":
          volume_driver: cinder.volume.drivers.rbd.RBDDriver
          rbd_pool: volumes
          rbd_ceph_conf: /etc/ceph/ceph.conf
          rbd_store_chunk_size: 8
          volume_backend_name: rbddriver
          rbd_user: cinder
          rbd_secret_uuid: "{% raw %}{{ cinder_ceph_client_uuid }}{% endraw %}"
          report_discard_supported: true
    dest: /etc/openstack_deploy/user_ceph_aio.yml
    force: no
  become: false
  when: not ceph_create_loopback|skipped
  tags:
    - skip_ansible_lint




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\prepare_data_disk.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2015, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# Only execute the disk partitioning process if a partition labeled
#  'openstack-data{1,2}' is not present and that partition is not
#  formatted as ext4. This is an attempt to achieve idempotency just
#  in case these tasks are executed multiple times.
- name: Determine whether partitions labeled openstack-data{1,2} are present
  shell: |
    parted --script -l -m | egrep -q ':{{ bootstrap_host_data_disk_fs_type }}:openstack-data[12]:;$'
  register: data_disk_partitions
  changed_when: false
  failed_when: false
  tags:
    - check-data-disk-partitions

- name: Set bootstrap host data disk fact
  set_fact:
    bootstrap_host_data_disk_device_force: true
    _bootstrap_host_data_disk_device: "{{ (bootstrap_host_data_disk_device | regex_replace('!','/')).strip() }}"
  when:
    - data_disk_partitions.rc == 1

- name: Dismount and remove fstab entries for anything on the data disk device
  mount:
    name: "{{ item.mount }}"
    src: "{{ item.device }}"
    fstype: "{{ bootstrap_host_data_disk_fs_type }}"
    state: absent
  when:
    - bootstrap_host_data_disk_device_force | bool
    - item.device | search(bootstrap_host_data_disk_device)
  with_items:
    - "{{ ansible_mounts }}"

- name: Partition the whole data disk for our usage
  command: "{{ item }}"
  when:
    - bootstrap_host_data_disk_device_force | bool
  with_items:
    - "parted --script /dev/{{ _bootstrap_host_data_disk_device }} mklabel gpt"
    - "parted --align optimal --script /dev/{{ _bootstrap_host_data_disk_device }} mkpart openstack-data1 {{ bootstrap_host_data_disk_fs_type }} 0% 40%"
    - "parted --align optimal --script /dev/{{ _bootstrap_host_data_disk_device }} mkpart openstack-data2 {{ bootstrap_host_data_disk2_fs }} 40% 100%"
  tags:
    - create-data-disk-partitions

- name: Format the partition 1
  filesystem:
    fstype: "{{ bootstrap_host_data_disk_fs_type }}"
    dev: "/dev/{{ _bootstrap_host_data_disk_device }}1"
    opts: "{{ bootstrap_host_format_options['ext4'] | default(omit) }}"
  when:
    - bootstrap_host_data_disk_device_force | bool
  tags:
    - format-data-partitions

- name: Format the partition 2
  filesystem:
    fstype: "{{ bootstrap_host_data_disk2_fs }}"
    dev: "/dev/{{ _bootstrap_host_data_disk_device }}2"
    opts: "{{ bootstrap_host_format_options[bootstrap_host_data_disk2_fs] | default(omit) }}"
  when:
    - bootstrap_host_data_disk_device_force | bool
    - lxc_container_backing_store != 'lvm'
    - lxc_container_backing_store != 'zfs'
  tags:
    - format-data-partitions

- name: Run the systemd mount role
  include_role:
    name: systemd_mount
    private: true
  vars:
    systemd_mounts:
      - what: "/dev/{{ _bootstrap_host_data_disk_device }}1"
        where: "/openstack"
        type: "{{ bootstrap_host_data_disk_fs_type }}"
        options: "{{ bootstrap_host_data_mount_options[bootstrap_host_data_disk_fs_type] }}"
        state: 'started'
        enabled: true
  tags:
    - data-config

- name: Run the systemd mount role
  include_role:
    name: systemd_mount
    private: true
  vars:
    systemd_mounts:
      - what: "/dev/{{ _bootstrap_host_data_disk_device }}2"
        where: "{{ bootstrap_host_data_disk2_path }}"
        type: "{{ bootstrap_host_data_disk2_fs }}"
        options: "{{ bootstrap_host_data_disk2_fs_mount_options }}"
        state: 'started'
        enabled: true
  when:
    - lxc_container_backing_store != 'lvm'
    - lxc_container_backing_store != 'zfs'
  tags:
    - data-config

- name: Create the ZFS pool
  command: zpool create pool "/dev/{{ _bootstrap_host_data_disk_device }}2"
  args:
    creates: /pool
  when:
    - bootstrap_host_data_disk_device_force | bool
    - lxc_container_backing_store == 'zfs'

- name: Create the ZFS pool/lxc volume
  shell: "(zfs list | grep lxc) || zfs create -o mountpoint=/var/lib/lxc pool/lxc"
  when:
    - bootstrap_host_data_disk_device_force | bool
    - lxc_container_backing_store == 'zfs'
  tags:
    - skip_ansible_lint

- name: Make LVM physical volume on the cinder device
  shell: "pvcreate /dev/{{ _bootstrap_host_data_disk_device }}2 && touch /openstack/lxc.pvcreate"
  args:
    creates: "/openstack/lxc.pvcreate"
  when:
    - lxc_container_backing_store == 'lvm'
  tags:
    - skip_ansible_lint
    - data-config

- name: Run pvscan
  command: "pvscan"
  changed_when: false
  when:
    - lxc_container_backing_store == 'lvm'
  tags:
    - cinder-lvm-pv

- name: Add cinder-volumes volume group
  lvg:
    vg: lxc
    pvs: "/dev/{{ _bootstrap_host_data_disk_device }}2"
  when:
    - lxc_container_backing_store == 'lvm'
  tags:
    - data-config




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\prepare_hostname.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Ensure the hosts file is templated appropriately
  copy:
    content: |
      127.0.0.1 localhost aio1
      127.0.1.1 aio1.openstack.local aio1

      # The following lines are desirable for IPv6 capable hosts
      ::1 ip6-localhost ip6-loopback
      fe00::0 ip6-localnet
      ff00::0 ip6-mcastprefix
      ff02::1 ip6-allnodes
      ff02::2 ip6-allrouters
      ff02::3 ip6-allhosts
    dest: /etc/hosts
    backup: yes

- name: Ensure hostname is set
  block:
    - name: Set hostname using the Ansible module
      hostname:
        name: aio1
  # NOTE(hwoarang) The hostname module does not work on Leap 15 because of
  # https://bugzilla.novell.com/show_bug.cgi?id=997614
  # As such we need to fallback to using the command directly.
  rescue:
    - name: Set hostname using hostnamectl
      command: hostnamectl set-hostname aio1
  tags:
    - skip_ansible_lint




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\prepare_loopback_btrfs.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Create sparse lxc-btrfs file
  command: "truncate -s {{ bootstrap_host_loopback_btrfs_size }}G /openstack/lxc-btrfs.img"
  args:
    creates: /openstack/lxc-btrfs.img

- name: Format the lxc-btrfs file
  filesystem:
    fstype: btrfs
    opts: "{{ bootstrap_host_format_options['btrfs'] | default(omit) }}"
    dev: /openstack/lxc-btrfs.img

- name: Run the systemd mount role
  include_role:
    name: systemd_mount
    private: true
  vars:
    systemd_mounts:
      - what: "/openstack/lxc-btrfs.img"
        where: "/var/lib/lxc"
        options: "loop,{{ bootstrap_host_data_mount_options['btrfs'] }}"
        type: "btrfs"
        state: 'started'
        enabled: true
  tags:
    - lxc-config




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\prepare_loopback_cinder.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2015, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Create sparse Cinder file
  command: "truncate -s {{ bootstrap_host_loopback_cinder_size }}G /openstack/cinder.img"
  args:
    creates: /openstack/cinder.img
  register: cinder_create
  tags:
    - cinder-file-create

- name: Run the systemd service role
  include_role:
    name: systemd_service
    private: true
  vars:
    systemd_services:
      - service_name: "loop-cinder"
        config_overrides:
          Unit:
            Description: no
            After: systemd-udev-settle.service
            Before: lvm2-activation-early.service
            Wants: systemd-udev-settle.service
          Service:
            RemainAfterExit: yes
        service_type: oneshot
        execstarts:
          - /bin/bash -c "/sbin/losetup $(/sbin/losetup -f) /openstack/cinder.img"
          - /sbin/pvscan
        execstops:
          - /bin/bash -c "losetup -d $(losetup -l | awk '/cinder.img/ {print $1}')"
        enabled: yes
        state: started
    systemd_tempd_prefix: openstack
  tags:
    - cinder-config

- name: Get loopback device
  shell: "losetup -l | awk '/cinder.img/ {print $1}'"
  changed_when: false
  register: cinder_losetup
  tags:
    - skip_ansible_lint

- name: Make LVM physical volume on the cinder device
  shell: "pvcreate {{ cinder_losetup.stdout }} && touch /openstack/cinder.pvcreate"
  args:
    creates: "/openstack/cinder.pvcreate"
  tags:
    - skip_ansible_lint
    - cinder-lvm-pv

- name: Run pvscan
  command: "pvscan"
  changed_when: false
  tags:
    - cinder-lvm-pv

- name: Add cinder-volumes volume group
  lvg:
    vg: cinder-volumes
    pvs: "{{ cinder_losetup.stdout }}"
  tags:
    - cinder-lvm-vg




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\prepare_loopback_machines.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Create sparse machines file
  command: "truncate -s {{ bootstrap_host_loopback_machines_size }}G /openstack/machines.img"
  args:
    creates: /openstack/machines.img
  tags:
    - machines-file-create

- name: Format the machines file
  filesystem:
    fstype: btrfs
    opts: "{{ bootstrap_host_format_options['btrfs'] | default(omit) }}"
    dev: /openstack/machines.img
  tags:
    - machines-format-file

- name: Run the systemd mount role
  include_role:
    name: systemd_mount
    private: true
  vars:
    systemd_mounts:
      - what: "/openstack/machines.img"
        where: "/var/lib/machines"
        options: "loop,{{ bootstrap_host_data_mount_options['btrfs'] }}"
        type: "btrfs"
        state: 'started'
        enabled: true
  tags:
    - machines-config




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\prepare_loopback_nova.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2015, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Create sparse Nova file
  command: "truncate -s {{ bootstrap_host_loopback_nova_size }}G /openstack/nova.img"
  args:
    creates: /openstack/nova.img
  tags:
    - nova-file-create

- name: Format the Nova file
  filesystem:
    fstype: xfs
    dev: /openstack/nova.img
    opts: "{{ bootstrap_host_format_options['xfs'] | default(omit) }}"
  tags:
    - nova-format-file

- name: Run the systemd mount role
  include_role:
    name: systemd_mount
    private: true
  vars:
    systemd_mounts:
      - what: "/openstack/nova.img"
        where: "/var/lib/nova/instances"
        options: "loop,{{ bootstrap_host_data_mount_options['xfs'] }}"
        type: "xfs"
        state: 'started'
        enabled: true
  tags:
    - nova-config




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\prepare_loopback_swap.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2015, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Create swap file
  command: "dd if=/dev/zero of=/openstack/swap.img bs=1M count={{ bootstrap_host_loopback_swap_size }}"
  args:
    creates: /openstack/swap.img
  register: swap_create
  tags:
    - swap-file-create

- name: Format the swap file
  command: mkswap /openstack/swap.img
  when:
    - swap_create  is changed
  tags:
    - swap-format
    - skip_ansible_lint

- name: Run the systemd mount role
  include_role:
    name: systemd_mount
    private: true
  vars:
    systemd_mounts:
      - what: "/openstack/swap.img"
        priority: "0"
        options: "{{ bootstrap_host_data_mount_options['swap'] }}"
        type: "swap"
        state: 'started'
        enabled: true
  tags:
    - swap-config

- name: Set system swappiness
  sysctl:
    name: vm.swappiness
    value: 10
    state: present
  tags:
    - swap-sysctl




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\prepare_loopback_swift.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2015, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Create sparse Swift files
  command: "truncate -s {{ bootstrap_host_loopback_swift_size }}G /openstack/{{ item }}.img"
  args:
    creates: "/openstack/{{ item }}.img"
  with_items:
    - 'swift1'
    - 'swift2'
    - 'swift3'
  tags:
    - swift-file-create

- name: Format the Swift files
  filesystem:
    fstype: xfs
    opts: "{{ bootstrap_host_format_options['xfs'] | default(omit) }}"
    dev: "/openstack/{{ item }}.img"
  with_items:
    - 'swift1'
    - 'swift2'
    - 'swift3'
  tags:
    - swift-format-file

- name: Run the systemd mount role
  include_role:
    name: systemd_mount
    private: true
  vars:
    systemd_mounts:
      - what: "/openstack/swift1.img"
        where: "/srv/swift1.img"
        options: "loop,{{ bootstrap_host_data_mount_options['xfs'] }}"
        type: "xfs"
        state: 'started'
        enabled: true
      - what: "/openstack/swift2.img"
        where: "/srv/swift2.img"
        options: "loop,{{ bootstrap_host_data_mount_options['xfs'] }}"
        type: "xfs"
        state: 'started'
        enabled: true
      - what: "/openstack/swift3.img"
        where: "/srv/swift3.img"
        options: "loop,{{ bootstrap_host_data_mount_options['xfs'] }}"
        type: "xfs"
        state: 'started'
        enabled: true
  tags:
    - swift-config




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\prepare_loopback_zfs.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Install zfs packages
  package:
    name: "{{ packages_install_zfs }}"
    state: present
    update_cache: "{{ (ansible_pkg_mgr in ['apt', 'zypper']) | ternary('yes', omit) }}"
  tags:
    - install-packages

- name: Create sparse ZFS backing file
  command: "truncate -s {{ bootstrap_host_loopback_zfs_size }}G /openstack/lxc-zfs.img"
  args:
    creates: /openstack/lxc-zfs.img

- name: Create the ZFS pool
  command: zpool create osa-test-pool /openstack/lxc-zfs.img
  args:
    creates: /osa-test-pool

- name: Create the ZFS pool/lxc volume
  shell: "(zfs list | grep lxc) || zfs create -o mountpoint=/var/lib/lxc osa-test-pool/lxc"
  tags:
    - skip_ansible_lint




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\prepare_networking.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2015, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Run the systemd-networkd role
  include_role:
    name: systemd_networkd
    private: true
  vars:
    systemd_networkd_prefix: "osa_testing"
    systemd_interface_cleanup: true
    systemd_run_networkd: true
    systemd_netdevs:

      - NetDev:
          Name: dummy-mgmt
          Kind: dummy
      - NetDev:
          Name: dummy-vxlan
          Kind: dummy
      - NetDev:
          Name: dummy-storage
          Kind: dummy
      - NetDev:
          Name: dummy-vlan
          Kind: dummy
      - NetDev:
          Name: dummy-dbaas
          Kind: dummy
      - NetDev:
          Name: dummy-lbaas
          Kind: dummy

      - NetDev:
          Name: br-mgmt
          Kind: bridge
      - NetDev:
          Name: br-vxlan
          Kind: bridge
      - NetDev:
          Name: br-storage
          Kind: bridge
      - NetDev:
          Name: br-vlan
          Kind: bridge
      - NetDev:
          Name: br-dbaas
          Kind: bridge
      - NetDev:
          Name: br-lbaas
          Kind: bridge

      - NetDev:
          Name: br-vlan-veth
          Kind: veth
        Peer:
          Name: eth12
      - NetDev:
          Name: br-dbaas-veth
          Kind: veth
        Peer:
          Name: eth13
      - NetDev:
          Name: br-lbaas-veth
          Kind: veth
        Peer:
          Name: eth14

    systemd_networks:

      - interface: "dummy-mgmt"
        bridge: "br-mgmt"
        mtu: 9000
      - interface: "br-mgmt"
        address: "172.29.236.100"
        netmask: "255.255.252.0"

      - interface: "dummy-storage"
        bridge: "br-storage"
        mtu: 9000
      - interface: "br-storage"
        address: "172.29.244.100"
        netmask: "255.255.252.0"

      - interface: "dummy-dbaas"
        bridge: "br-dbaas"
        mtu: 9000
      - interface: "br-dbaas"
        address: "172.29.232.100"
        netmask: "255.255.252.0"
      - interface: "br-dbaas-veth"
        bridge: "br-dbaas"
        mtu: 9000

      - interface: "dummy-lbaas"
        bridge: "br-lbaas"
        mtu: 9000
      - interface: "br-lbaas"
        address: "172.29.252.100"
        netmask: "255.255.252.0"
      - interface: "br-lbaas-veth"
        bridge: "br-lbaas"
        mtu: 9000

      - interface: "dummy-vxlan"
        bridge: "br-vxlan"
        mtu: 9000
      - interface: "br-vxlan"
        address: "172.29.240.100"
        netmask: "255.255.252.0"

      - interface: "dummy-vlan"
        bridge: "br-vlan"
        mtu: 9000
      - interface: "br-vlan"
        config_overrides:
          Network:
            Address:
              ? "172.29.248.100/22"
              ? "172.29.248.1/22"
      - interface: "br-vlan-veth"
        bridge: "br-vlan"
        mtu: 9000

  tags:
    - network-config

# NOTE(jrosser) The systemd_networkd role uses a handler to restart the networking service
# This will normally not run until the end of the play, so we must force it here
- name: Force systemd_networkd hander to run
  meta: flush_handlers

# NOTE(jrosser) The intention here is not to proceed further until the network bridges are up
# This ensures there will be no race between the bridges coming up and subsequent tasks which
# require functional network interfaces
- name: Check that network bridges are up
  wait_for:
    port: 22
    timeout: 30
    host: "{{ item }}"
  with_items:
    - 172.29.236.100  # br-mgmt
    - 172.29.244.100  # br-storage
    - 172.29.232.100  # br-dbaas
    - 172.29.252.100  # br-lbaas
    - 172.29.240.100  # br-vxlan

- name: Run the systemd service role
  include_role:
    name: systemd_service
    private: true
  vars:
    systemd_services:
      - service_name: "networking-post-up"
        config_overrides:
          Unit:
            Description: networking-post-up
            After: network-online.target
            Wants: network-online.target
          Service:
            RemainAfterExit: yes
        service_type: oneshot
        execstarts:
          - "-{{ bootstrap_host_iptables_path }} -t nat -A POSTROUTING -o {{ bootstrap_host_public_interface }} -j MASQUERADE"
          - "-{{ bootstrap_host_ethtool_path }} -K br-mgmt gso off sg off tso off tx off"
          - "-{{ bootstrap_host_ethtool_path }} -K br-vxlan gso off sg off tso off tx off"
          - "-{{ bootstrap_host_ethtool_path }} -K br-storage gso off sg off tso off tx off"
          - "-{{ bootstrap_host_ethtool_path }} -K br-vlan gso off sg off tso off tx off"
          - "-{{ bootstrap_host_ethtool_path }} -K br-dbaas gso off sg off tso off tx off"
          - "-{{ bootstrap_host_ethtool_path }} -K br-lbaas gso off sg off tso off tx off"
          - "-{{ bootstrap_host_ip_path }} link set eth12 up"
          - "-{{ bootstrap_host_ip_path }} link set br-vlan-veth up"
          - "-{{ bootstrap_host_ethtool_path }} -K eth12 gso off sg off tso off tx off"
          - "-{{ bootstrap_host_ip_path }} link set eth13 up"
          - "-{{ bootstrap_host_ip_path }} link set br-dbaas-veth up"
          - "-{{ bootstrap_host_ethtool_path }} -K eth13 gso off sg off tso off tx off"
          - "-{{ bootstrap_host_ip_path }} link set eth14 up"
          - "-{{ bootstrap_host_ip_path }} link set br-lbaas-veth up"
          - "-{{ bootstrap_host_ethtool_path }} -K eth14 gso off sg off tso off tx off"
        execstops:
          - "{{ bootstrap_host_iptables_path }} -t nat -D POSTROUTING -o {{ bootstrap_host_public_interface }} -j MASQUERADE"
        enabled: yes
        state: started
    systemd_tempd_prefix: openstack
  tags:
    - network-config

- name: Updating the facts due to net changes
  setup:
    gather_subset: network
  tags:
    - networking




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\tasks\prepare_ssh_keys.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2015, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Ensure root has a .ssh directory
  file:
    path: /root/.ssh
    state: directory
    owner: root
    group: root
    mode: 0700
  tags:
    - ssh-key-dir

- name: Check for existing ssh private key file
  stat:
    path: /root/.ssh/id_rsa
  register: ssh_key_private
  tags:
    - ssh-key-check

- name: Check for existing ssh public key file
  stat:
    path: /root/.ssh/id_rsa.pub
  register: ssh_key_public
  tags:
    - ssh-key-check

- name: Remove an existing private/public ssh keys if one is missing
  file:
    path: "/root/.ssh/{{ item }}"
    state: absent
  when: not ssh_key_public.stat.exists or not ssh_key_private.stat.exists
  with_items:
    - 'id_rsa'
    - 'id_rsa.pub'
  tags:
    - ssh-key-clean

- name: Create ssh key pair for root
  user:
    name: root
    generate_ssh_key: yes
    ssh_key_bits: 2048
    ssh_key_file: /root/.ssh/id_rsa
  tags:
    - ssh-key-generate

- name: Fetch the generated public ssh key
  fetch:
    src: "/root/.ssh/id_rsa.pub"
    dest: "/tmp/id_rsa.pub"
    flat: yes
  when: inventory_hostname == groups['all'][0]
  tags:
    - ssh-key-authorized

- name: Ensure root's new public ssh key is in authorized_keys
  authorized_key:
    user: root
    key: "{{ lookup('file','/tmp/id_rsa.pub') }}"
    manage_dir: no
  tags:
    - ssh-key-authorized



===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\templates\user_variables.aio.yml.j2
===========File Type===========
.j2
===========File Content===========
---
# Copyright 2014, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

## General options
debug: True

## Installation method for OpenStack services
install_method: "{{ bootstrap_host_install_method }}"

## Tempest settings
{% if bootstrap_host_container_tech == 'nspawn' %}
tempest_public_subnet_cidr: "172.29.236.0/22"
tempest_public_subnet_allocation_pools: "172.29.239.110-172.29.239.200"
{% else %}
## Tempest settings
tempest_public_subnet_cidr: "172.29.248.0/22"
tempest_public_subnet_allocation_pools: "172.29.249.110-172.29.249.200"
{% endif %}

## Galera settings
galera_monitoring_allowed_source: "0.0.0.0/0"
galera_innodb_buffer_pool_size: 16M
galera_innodb_log_buffer_size: 4M
galera_wsrep_provider_options:
 - { option: "gcache.size", value: "4M" }

### Set workers for all services to optimise memory usage

## Repo
repo_nginx_threads: 2

## Keystone
keystone_httpd_mpm_start_servers: 2
keystone_httpd_mpm_min_spare_threads: 1
keystone_httpd_mpm_max_spare_threads: 2
keystone_httpd_mpm_thread_limit: 2
keystone_httpd_mpm_thread_child: 1
keystone_wsgi_threads: 1
keystone_wsgi_processes_max: 2

## Barbican
barbican_wsgi_processes: 2
barbican_wsgi_threads: 1

## Cinder
cinder_wsgi_processes_max: 2
cinder_wsgi_threads: 1
cinder_wsgi_buffer_size: 16384
cinder_osapi_volume_workers_max: 2

## Glance
glance_api_threads_max: 2
glance_api_threads: 1
glance_api_workers: 1
glance_registry_workers: 1
glance_wsgi_threads: 1
glance_wsgi_processes_max: 2
glance_wsgi_processes: 2

## Nova
nova_wsgi_threads: 1
nova_wsgi_processes_max: 2
nova_wsgi_processes: 2
nova_wsgi_buffer_size: 16384
nova_api_threads_max: 2
nova_api_threads: 1
nova_osapi_compute_workers: 1
nova_conductor_workers: 1
nova_metadata_workers: 1
nova_scheduler_workers: 1

## Neutron
neutron_rpc_workers: 1
neutron_metadata_workers: 1
neutron_api_workers: 1
neutron_api_threads_max: 2
neutron_api_threads: 2
neutron_num_sync_threads: 1

## Heat
heat_api_workers: 1
heat_api_threads_max: 2
heat_api_threads: 1
heat_wsgi_threads: 1
heat_wsgi_processes_max: 2
heat_wsgi_processes: 1
heat_wsgi_buffer_size: 16384

## Horizon
horizon_wsgi_processes: 1
horizon_wsgi_threads: 1
horizon_wsgi_threads_max: 2

## Ceilometer
ceilometer_notification_workers_max: 2
ceilometer_notification_workers: 1

## AODH
aodh_wsgi_threads: 1
aodh_wsgi_processes_max: 2
aodh_wsgi_processes: 1

## Gnocchi
gnocchi_wsgi_threads: 1
gnocchi_wsgi_processes_max: 2
gnocchi_wsgi_processes: 1

## Swift
swift_account_server_replicator_workers: 1
swift_server_replicator_workers: 1
swift_object_replicator_workers: 1
swift_account_server_workers: 1
swift_container_server_workers: 1
swift_object_server_workers: 1
swift_proxy_server_workers_max: 2
swift_proxy_server_workers_not_capped: 1
swift_proxy_server_workers_capped: 1
swift_proxy_server_workers: 1

## Ironic
ironic_wsgi_threads: 1
ironic_wsgi_processes_max: 2
ironic_wsgi_processes: 1

## Trove
trove_api_workers_max: 2
trove_api_workers: 1
trove_conductor_workers_max: 2
trove_conductor_workers: 1
trove_wsgi_threads: 1
trove_wsgi_processes_max: 2
trove_wsgi_processes: 1

## Sahara
sahara_api_workers_max: 2
sahara_api_workers: 1

# NOTE: hpcloud-b4's eth0 uses 10.0.3.0/24, which overlaps with the
#       lxc_net_address default
# TODO: We'll need to implement a mechanism to determine valid lxc_net_address
#       value which will not overlap with an IP already assigned to the host.
lxc_net_address: 10.255.255.1
lxc_net_netmask: 255.255.255.0
lxc_net_dhcp_range: 10.255.255.2,10.255.255.253

{% if repo_build_pip_extra_indexes is defined and repo_build_pip_extra_indexes | length > 0 %}
## Wheel mirrors for the repo_build to use
repo_build_pip_extra_indexes:
{{ repo_build_pip_extra_indexes | to_nice_yaml }}
{% endif %}

{% if _lxc_mirror is defined and _lxc_mirror.stdout_lines is defined %}
## images.linuxcontainers.org reverse proxy
lxc_image_cache_server_mirrors:
  - "http://{{ _lxc_mirror.stdout_lines[0] }}"
{% endif %}

{% if cache_timeout is defined %}
## Package cache timeout
cache_timeout: {{ cache_timeout }}
{% endif %}

# The container backing store is set to 'machinectl' to speed up the
# AIO build time. Options are: [machinectl, overlayfs, btrfs, zfs, dir, lvm]
lxc_container_backing_store: "{{ lxc_container_backing_store }}"

## Always setup tempest, the resources for it, then execute tests
tempest_install: yes
tempest_run: yes

{% if nodepool_dir.stat.exists %}
# Disable chronyd in OpenStack CI
security_rhel7_enable_chrony: no
{% endif %}

# For testing purposes in public clouds, we need to ignore these
# services when trying to do a reload of nova services.
nova_service_negate:
  - "nova-agent.service"
  - "nova-resetnetwork.service"

{% if _pypi_wheel_mirror is defined and _pypi_wheel_mirror.stdout_lines is defined %}
repo_build_pip_extra_indexes:
 - "{{ _pypi_wheel_mirror.stdout_lines[1] }}"
{% endif %}

# Set all the distros to the same value: a "quiet" print
# of kernel log messages.
openstack_user_kernel_options:
  - key: 'kernel.printk'
    value: '4 1 7 4'

{% if bootstrap_host_scenario in ['octavia'] %}
# Octavia specific stuff
neutron_lbaas_octavia: True
octavia_management_net_subnet_cidr: "{{ (bootstrap_host_container_tech == 'nspawn') | ternary('172.29.240.0/22', '172.29.252.0/22') }}"
{% endif %}




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\templates\user_variables_barbican.yml.j2
===========File Type===========
.j2
===========File Content===========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

tempest_plugins:
  - name: barbican-tempest-plugin
    repo: https://git.openstack.org/openstack/barbican-tempest-plugin
    branch: master

tempest_test_whitelist:
  - barbican_tempest_plugin.tests.api

tempest_roles:
  - key-manager:service-admin




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\templates\user_variables_ceph.yml.j2
===========File Type===========
.j2
===========File Content===========
---
# Copyright 2017, Logan Vig <logan2211@gmail.com>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

## ceph-ansible AIO settings
common_single_host_mode: true
monitor_interface: eth1 # Management network in the AIO
public_network: "{{ (mgmt_range ~ '.0/' ~ netmask) | ipaddr('net') }}"
journal_size: 100
osd_scenario: collocated
pool_default_pg_num: 16
openstack_config: true # Ceph ansible automatically creates pools & keys
cinder_ceph_client: cinder
cinder_default_volume_type: RBD
glance_ceph_client: glance
glance_default_store: rbd
glance_rbd_store_pool: images
nova_libvirt_images_rbd_pool: vms




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\templates\user_variables_congress.yml.j2
===========File Type===========
.j2
===========File Content===========
# Copyright 2018, Taseer Ahmed <taseer94@gmail.com>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

tempest_plugins:
  - name: congress-tempest-plugin
    repo: https://github.com/openstack/congress-tempest-plugin
    branch: e8d68f8da9380aacdf05693aaf8bb9f8e570dd93 # HEAD of "master" as of 19.04.2018

tempest_whitelist:
  - congress_tempest_plugin.tests.scenario.test_congress_basic_ops



===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\templates\user_variables_translations.yml.j2
===========File Type===========
.j2
===========File Content===========
---
# Copyright 2017, Logan Vig <logan2211@gmail.com>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Trove settings for translations site.
trove_provider_net_name: flat-db
trove_service_net_phys_net: flat-db
trove_service_net_setup: True

{% if bootstrap_host_container_tech == 'nspawn' %}
trove_service_net_subnet_cidr: "172.29.236.0/22"
trove_service_net_allocation_pool_start: "172.29.237.110"
trove_service_net_allocation_pool_end: "172.29.237.200"
{% else %}
trove_service_net_subnet_cidr: "172.29.232.0/22"
trove_service_net_allocation_pool_start: "172.29.233.110"
trove_service_net_allocation_pool_end: "172.29.233.200"
{% endif %}




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\vars\redhat.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2017, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

packages_install:
  - bridge-utils
  - btrfs-progs
  - curl
  - dbus
  - ethtool
  - git
  - iputils
  - lvm2
  - python
  - python-devel
  - sshpass
  - systemd-networkd
  - vim
  - xfsprogs

packages_remove: []

rc_local: /etc/rc.d/rc.local
rc_local_insert_before: "^touch /var/lock/subsys/local$"




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\vars\suse.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2015, Rackspace US, Inc.
# Copyright 2017, SUSE LINUX GmbH.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

packages_install:
  - bridge-utils
  - btrfsprogs
  - curl
  - dbus-1
  - ethtool
  - git-core
  - lvm2
  - python
  - python-devel
  - vim
  - vlan
  - xfsprogs

packages_remove: []

rc_local: /etc/rc.d/boot.local
rc_local_insert_before: EOF




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\roles\bootstrap-host\vars\ubuntu.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2015, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

packages_install:
  - apt-transport-https
  - bridge-utils
  - btrfs-tools
  - build-essential
  - curl
  - dbus
  - ethtool
  - git-core
  - iptables
  - iputils-tracepath
  - ipython
  - lvm2
  - parted
  - python2.7
  - python-dev
  - sshpass
  - vim
  - vlan
  - xfsprogs

packages_install_zfs:
  - zfsutils-linux

packages_remove:
  - libmysqlclient18
  - mysql-common

rc_local: /etc/rc.local
rc_local_insert_before: "^exit 0$"




===========Repository Name===========
openstack-ansible
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible\tests\vars\bootstrap-aio-vars.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2017, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

confd_overrides:
  aio_lxc:
    - name: cinder.yml.aio
    - name: glance.yml.aio
    - name: haproxy.yml.aio
    - name: horizon.yml.aio
    - name: keystone.yml.aio
    - name: neutron.yml.aio
    - name: nova.yml.aio
  aio_metal:
    - name: cinder.yml.aio
    - name: glance.yml.aio
    - name: keystone.yml.aio
    - name: neutron.yml.aio
    - name: nova.yml.aio
  aio_nspawn:
    - name: cinder.yml.aio
    - name: glance.yml.aio
    - name: haproxy.yml.aio
    - name: horizon.yml.aio
    - name: keystone.yml.aio
    - name: neutron.yml.aio
    - name: nova.yml.aio
  blazar:
    - name: haproxy.yml.aio
    - name: nova.yml.aio
    - name: neutron.yml.aio
    - name: keystone.yml.aio
    - name: glance.yml.aio
    - name: blazar.yml.aio
  ceph:
    - name: haproxy.yml.aio
    - name: ceph.yml.aio
    - name: cinder.yml.aio
    - name: glance.yml.aio
    - name: keystone.yml.aio
    - name: neutron.yml.aio
    - name: nova.yml.aio
  congress:
    - name: congress.yml.aio
    - name: haproxy.yml.aio
    - name: glance.yml.aio
    - name: keystone.yml.aio
    - name: nova.yml.aio
    - name: neutron.yml.aio
  translations:
    - name: cinder.yml.aio
    - name: designate.yml.aio
    - name: glance.yml.aio
    - name: haproxy.yml.aio
    - name: heat.yml.aio
    - name: horizon.yml.aio
    - name: keystone.yml.aio
    - name: magnum.yml.aio
    - name: neutron.yml.aio
    - name: nova.yml.aio
    - name: sahara.yml.aio
    - name: swift.yml.aio
    - name: trove.yml.aio
  octavia:
    - name: glance.yml.aio
    - name: haproxy.yml.aio
    - name: keystone.yml.aio
    - name: neutron.yml.aio
    - name: nova.yml.aio
    - name: octavia.yml.aio
  tacker:
    - name: haproxy.yml.aio
    - name: heat.yml.aio
    - name: keystone.yml.aio
    - name: tacker.yml.aio
  barbican:
    - name: haproxy.yml.aio
    - name: keystone.yml.aio
    - name: barbican.yml.aio




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\tox.ini
===========File Type===========
.ini
===========File Content===========
[tox]
minversion = 2.0
skipsdist = True
envlist = docs,linters,functional


[testenv]
usedevelop = True
install_command =
    pip install -c{env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt} {opts} {packages}
commands =
    /usr/bin/find . -type f -name "*.pyc" -delete
passenv =
    COMMON_TESTS_PATH
    HOME
    http_proxy
    HTTP_PROXY
    https_proxy
    HTTPS_PROXY
    no_proxy
    NO_PROXY
    TESTING_BRANCH
    TESTING_HOME
    USER
whitelist_externals =
    bash
setenv =
    VIRTUAL_ENV={envdir}
    WORKING_DIR={toxinidir}


[testenv:docs]
basepython = python3
deps = -r{toxinidir}/doc/requirements.txt
commands=
    bash -c "rm -rf doc/build"
    doc8 doc
    sphinx-build -b html doc/source doc/build/html


[doc8]
# Settings for doc8:
extensions = .rst


[testenv:releasenotes]
basepython = python3
deps = -r{toxinidir}/doc/requirements.txt
commands =
    sphinx-build -a -E -W -d releasenotes/build/doctrees -b html releasenotes/source releasenotes/build/html


# environment used by the -infra templated docs job
[testenv:venv]
basepython = python3
commands =
    {posargs}


[testenv:pep8]
basepython = python3
commands =
    bash -c "{toxinidir}/tests/common/test-pep8.sh"


[flake8]
# Ignores the following rules due to how ansible modules work in general
#     F403 'from ansible.module_utils.basic import *' used;
#          unable to detect undefined names
ignore=F403


[testenv:bashate]
commands =
    bash -c "{toxinidir}/tests/common/test-bashate.sh"


[testenv:linters]
basepython = python3
commands =
    bash -c "{toxinidir}/tests/common/test-ansible-env-prep.sh"
    {[testenv:pep8]commands}
    {[testenv:bashate]commands}




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\tests\.placeholder
===========File Type===========

===========File Content===========
# This placeholder is here to ensure that git
# does not remove this directory. The directory
# is used by run_tests.sh to symlink the common
# tests folder.




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\elk_metrics_6x\tests\ansible-role-requirements.yml
===========File Type===========
.yml
===========File Content===========
---
- name: apt_package_pinning
  scm: git
  src: https://git.openstack.org/openstack/openstack-ansible-apt_package_pinning
  version: master
- name: config_template
  scm: git
  src: https://git.openstack.org/openstack/ansible-config_template
  version: master
- name: nspawn_container_create
  scm: git
  src: https://git.openstack.org/openstack/openstack-ansible-nspawn_container_create
  version: master
- name: nspawn_hosts
  scm: git
  src: https://git.openstack.org/openstack/openstack-ansible-nspawn_hosts
  version: master
- name: plugins
  scm: git
  src: https://git.openstack.org/openstack/openstack-ansible-plugins
  version: master
- name: systemd_mount
  scm: git
  src: https://git.openstack.org/openstack/ansible-role-systemd_mount
  version: master
- name: systemd_networkd
  scm: git
  src: https://git.openstack.org/openstack/ansible-role-systemd_networkd
  version: master
- name: systemd_service
  scm: git
  src: https://git.openstack.org/openstack/ansible-role-systemd_service
  version: master




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\elk_metrics_6x\tests\functional.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- import_playbook: run-setup.yml

- name: Basic setup
  hosts: "all"
  become: true

  environment:
    # ZUUL_PROJECT is used by tests/get-ansible-role-requirements to
    # determine when CI provided repos should be used.
    ZUUL_PROJECT: "{{ zuul.project.short_name }}"
    ANSIBLE_PACKAGE: "{{ ansible_package | default('') }}"
    ANSIBLE_HOST_KEY_CHECKING: "False"
    ANSIBLE_LOG_PATH: "/tmp/elk-metrics-6x-logs/ansible-elk-test.log"

  vars:
    inventory_file: "inventory/test-{{ (contianer_inventory | bool) | ternary('container', 'metal') }}-inventory.yml"

  pre_tasks:
    - name: Create swap file
      command: "dd if=/dev/zero of=/swap.img bs=1M count=4096"
      args:
        creates: /swap.img
      register: swap_create

    - name: Format the swap file
      command: mkswap /swap.img
      when:
        - swap_create is changed
      tags:
        - swap-format
        - skip_ansible_lint

    - name: Enable swap file
      command: swapon /swap.img
      failed_when: false
      tags:
        - swap-format
        - skip_ansible_lint

    - name: Set system swappiness
      sysctl:
        name: vm.swappiness
        value: 10
        state: present
        reload: "yes"
        sysctl_file: /etc/sysctl.d/99-elasticsearch.conf

    - name: Create tmp elk_metrics_6x dir
      file:
        path: "/tmp/elk-metrics-6x-logs"
        state: directory

    - name: Flush iptables rules
      command: "{{ item }}"
      args:
        creates: "/tmp/elk-metrics-6x-logs/iptables.flushed"
      with_items:
        - "iptables -F"
        - "iptables -X"
        - "iptables -t nat -F"
        - "iptables -t nat -X"
        - "iptables -t mangle -F"
        - "iptables -t mangle -X"
        - "iptables -P INPUT ACCEPT"
        - "iptables -P FORWARD ACCEPT"
        - "iptables -P OUTPUT ACCEPT"
        - "touch /tmp/elk-metrics-6x-logs/iptables.flushed"

    - name: First ensure apt cache is always refreshed
      apt:
        update_cache: yes
      when:
        - ansible_pkg_mgr == 'apt'

  tasks:
    - name: Run embedded ansible installation
      become: yes
      become_user: root
      command: "./bootstrap-embedded-ansible.sh"
      args:
        chdir: "src/{{ current_test_repo }}/elk_metrics_6x"

    - name: Run ansible-galaxy (tests)
      become: yes
      become_user: root
      command: "${HOME}/ansible_venv/bin/ansible-galaxy install --force --ignore-errors --roles-path=${HOME}/ansible_venv/repositories/roles -r ansible-role-requirements.yml"
      args:
        chdir: "src/{{ current_test_repo }}/elk_metrics_6x/tests"

    - name: Run ansible-galaxy (elk_metrics_6x)
      become: yes
      become_user: root
      command: "${HOME}/ansible_venv/bin/ansible-galaxy install --force --ignore-errors --roles-path=${HOME}/ansible_venv/repositories/roles -r ansible-role-requirements.yml"
      args:
        chdir: "src/{{ current_test_repo }}/elk_metrics_6x"

    - name: Run environment setup
      become: yes
      become_user: root
      command: "${HOME}/ansible_venv/bin/ansible-playbook -i {{ inventory_file }} -e @test-vars.yml _key-setup.yml"
      environment:
        ANSIBLE_ACTION_PLUGINS: "${HOME}/ansible_venv/repositories/ansible-config_template/action"
        ANSIBLE_CONNECTION_PLUGINS: "${HOME}/ansible_venv/repositories/openstack-ansible-plugins/connection"
        ANSIBLE_LOG_PATH: "/tmp/elk-metrics-6x-logs/ansible-elk-test-container-setup.log"
        ANSIBLE_ROLES_PATH: "${HOME}/ansible_venv/repositories/roles"
      args:
        chdir: "src/{{ current_test_repo }}/elk_metrics_6x/tests"
      when:
        - ansible_service_mgr != 'systemd' or
          not (contianer_inventory | bool)

    - name: Run environment setup
      become: yes
      become_user: root
      command: "${HOME}/ansible_venv/bin/ansible-playbook -i {{ inventory_file }} -e @test-vars.yml _container-setup.yml"
      environment:
        ANSIBLE_ACTION_PLUGINS: "${HOME}/ansible_venv/repositories/ansible-config_template/action"
        ANSIBLE_CONNECTION_PLUGINS: "${HOME}/ansible_venv/repositories/openstack-ansible-plugins/connection"
        ANSIBLE_LOG_PATH: "/tmp/elk-metrics-6x-logs/ansible-elk-test-container-setup.log"
        ANSIBLE_ROLES_PATH: "${HOME}/ansible_venv/repositories/roles"
      args:
        chdir: "src/{{ current_test_repo }}/elk_metrics_6x/tests"
      when:
        - ansible_service_mgr == 'systemd'
        - contianer_inventory | bool

    - name: Wait 15 seconds
      command: "sleep 15"
      changed_when: false
      when:
        - ansible_service_mgr == 'systemd'

    - name: Run functional test
      become: yes
      become_user: root
      command: "${HOME}/ansible_venv/bin/ansible-playbook -i tests/{{ inventory_file }} -e @tests/test-vars.yml site.yml"
      environment:
        ANSIBLE_ACTION_PLUGINS: "${HOME}/ansible_venv/repositories/ansible-config_template/action"
        ANSIBLE_CONNECTION_PLUGINS: "${HOME}/ansible_venv/repositories/openstack-ansible-plugins/connection"
        ANSIBLE_LOG_PATH: "/tmp/elk-metrics-6x-logs/ansible-elk-test-deployment.log"
        ANSIBLE_ROLES_PATH: "${HOME}/ansible_venv/repositories/roles"
      args:
        chdir: "src/{{ current_test_repo }}/elk_metrics_6x"

    - name: Show cluster state
      become: yes
      become_user: root
      command: "${HOME}/ansible_venv/bin/ansible-playbook -i tests/{{ inventory_file }} -e @tests/test-vars.yml showElasticCluster.yml"
      environment:
        ANSIBLE_ACTION_PLUGINS: "${HOME}/ansible_venv/repositories/ansible-config_template/action"
        ANSIBLE_CONNECTION_PLUGINS: "${HOME}/ansible_venv/repositories/openstack-ansible-plugins/connection"
        ANSIBLE_LOG_PATH: "/tmp/elk-metrics-6x-logs/ansible-elk-test-show-cluster.log"
        ANSIBLE_ROLES_PATH: "${HOME}/ansible_venv/repositories/roles"
      args:
        chdir: "src/{{ current_test_repo }}/elk_metrics_6x"




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\elk_metrics_6x\tests\manual-test.rc
===========File Type===========
.rc
===========File Content===========
export ANSIBLE_HOST_KEY_CHECKING="False"
export ANSIBLE_ROLES_PATH="${HOME}/ansible_venv/repositories/roles"
export ANSIBLE_ACTION_PLUGINS="${HOME}/ansible_venv/repositories/roles/config_template/action"
export ANSIBLE_CONNECTION_PLUGINS="${HOME}/ansible_venv/repositories/roles/plugins/connection"
export ANSIBLE_LOG_PATH="/tmp/elk-metrics-6x-logs/ansible-elk-test.log"

if [[ ! -d "/tmp/elk-metrics-6x-logs" ]]; then
  mkdir -pv "/tmp/elk-metrics-6x-logs"
  chmod 0777 "/tmp/elk-metrics-6x-logs"
fi

echo "To build a test environment run the following:"
echo -e "# ${HOME}/ansible_venv/bin/ansible-playbook -i tests/inventory/test-container-inventory.yml tests/test.yml --limit localhost\n"

echo "Run manual functional tests by executing the following:"
echo -e "# ${HOME}/ansible_venv/bin/ansible-playbook -i tests/inventory/test-container-inventory.yml site.yml\n"




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\elk_metrics_6x\tests\post-run.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- import_playbook: run-setup.yml

- name: Run post tasks
  hosts: "all"
  tasks:
    - name: Copy logs back to the executor
      synchronize:
        src: "/tmp/elk-metrics-6x-logs"
        dest: "{{ zuul.executor.log_root }}/"
        mode: pull
        rsync_opts:
          - "--quiet"




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\elk_metrics_6x\tests\run-cleanup.sh
===========File Type===========
.sh
===========File Content===========
#!/usr/bin/env bash
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

set -e

export TEST_DIR="$(readlink -f $(dirname ${0})/../../)"

# Stop beat processes
pushd "${TEST_DIR}/elk_metrics_6x"
  for i in $(ls -1 install*beat.yml); do
    LOWER_BEAT="$(echo "${i}" | tr '[:upper:]' '[:lower:]')"
    BEAT_PARTIAL="$(echo ${LOWER_BEAT} | awk -F'.' '{print $1}')"
    BEAT="$(echo ${BEAT_PARTIAL} | awk -F'install' '{print $2}')"
    echo "Stopping ${BEAT}"
    (systemctl stop "${BEAT}" || true) &
    apt remove --purge -y "${BEAT}" || true
    if [[ -d "/etc/${BEAT}" ]]; then
      rm -rf "/etc/${BEAT}"
    fi
    if [[ -d "/var/lib/${BEAT}" ]]; then
      rm -rf "/var/lib/${BEAT}"
    fi
    if [[ -d "/etc/systemd/system/${BEAT}.service.d" ]]; then
      rm -rf "/etc/systemd/system/${BEAT}.service.d"
    fi
  done
popd

for i in $(grep -lri elastic /etc/apt/sources.list.d/); do
  rm "${i}"
done

# Stop and remove containers
for i in {1..3}; do
  if machinectl list-images | grep -v ubuntu | awk '/sub/ {print $1}' | xargs -n 1 machinectl kill; then
    sleep 1
  fi
done

for i in {1..3}; do
  if machinectl list-images | grep -v ubuntu | awk '/sub/ {print $1}' | xargs -n 1 machinectl remove; then
    sleep 1
  fi
done




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\elk_metrics_6x\tests\run-setup.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Zuul facts
  hosts: "all"
  tasks:
    - name: Set zuul fact
      set_fact:
        zuul:
          project:
            canonical_name: "openstack-ansible-ops"
            short_name: "ops"
          executor:
            log_root: "{{ ansible_env.HOME }}/elk-test-logs"
      when:
        - zuul is not defined

    - name: Print zuul fact
      debug: var=zuul

    - name: Set current test repo (cross-repo)
      set_fact:
        current_test_repo: "git.openstack.org/{{ osa_test_repo }}"
      when:
        - osa_test_repo is defined

    - name: Set current test repo (non-cross-repo)
      set_fact:
        current_test_repo: "{{ zuul.project.canonical_name }}"
      when:
        - osa_test_repo is not defined

    - name: Set inventory for test
      set_fact:
        contianer_inventory: "{{ test_clustered_elk | default(false) | bool }}"

  post_tasks:
    - name: Ensure the log directory exists
      file:
        path: "/tmp/elk-metrics-6x-logs"
        state: directory




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\elk_metrics_6x\tests\run-tests.sh
===========File Type===========
.sh
===========File Content===========
#!/usr/bin/env bash
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

set -ve

export TEST_DIR="$(readlink -f $(dirname ${0})/../../)"

pushd "${HOME}"
  if [[ ! -d "src" ]]; then
    mkdir src
  fi
  pushd src
    ln -sf "${TEST_DIR}"
  popd
popd

source "${TEST_DIR}/elk_metrics_6x/tests/manual-test.rc"

source "${TEST_DIR}/elk_metrics_6x/bootstrap-embedded-ansible.sh"
deactivate

${HOME}/ansible_venv/bin/ansible-galaxy install --force \
                                             --roles-path="${HOME}/ansible_venv/repositories/roles" \
                                             --role-file="${TEST_DIR}/elk_metrics_6x/tests/ansible-role-requirements.yml"

if [[ ! -e "${TEST_DIR}/elk_metrics_6x/tests/src" ]]; then
  ln -s ${TEST_DIR}/../ ${TEST_DIR}/elk_metrics_6x/tests/src
fi

${HOME}/ansible_venv/bin/ansible-playbook -i 'localhost,' \
                                       -vv \
                                       -e ansible_connection=local \
                                       -e test_clustered_elk=${CLUSTERED:-no} \
                                       ${TEST_DIR}/elk_metrics_6x/tests/test.yml




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\elk_metrics_6x\tests\test-vars.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

physical_host: localhost

# NOTE(cloudnull): Test configs used to minimize the impact of a
#                  multi-node install with limited resources.
q_storage: 1
q_mem: 512
h_mem: 512

osa_test_repo: "openstack/openstack-ansible-ops"




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\elk_metrics_6x\tests\test.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- import_playbook: functional.yml
- import_playbook: testLayout.yml
- import_playbook: testAPI.yml




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\elk_metrics_6x\tests\testAPI.yml
===========File Type===========
.yml
===========File Content===========
---
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Test apm api
  hosts: apm-server
  gather_facts: false
  become: true

  environment:
    ANSIBLE_LOG_PATH: "/tmp/elk-metrics-6x-logs/ansible-elk-test.log"

  tasks:
    - name: Check for open TCP
      wait_for:
        port: "{{ item.port }}"
        host: "{{ item.host }}"
        timeout: 120
      with_items:
        - port: 8200
          host: 127.0.0.1


- name: Test elasticsearch api
  hosts: elastic-logstash
  gather_facts: false
  become: true

  environment:
    ANSIBLE_LOG_PATH: "/tmp/elk-metrics-6x-logs/ansible-elk-test.log"

  tasks:
    - name: Check http
      uri:
        url: http://127.0.0.1:{{ item.port }}{{ item.path }}
        method: "{{ item.method }}"
        status_code: 200
      register: elk_test
      until: elk_test is success
      retries: 3
      delay: 10
      with_items:
        - port: 9200
          path: "/_nodes/stats"
          method: "GET"

    - name: Check for open TCP
      wait_for:
        port: "{{ item.port }}"
        host: "{{ item.host }}"
        timeout: 120
      with_items:
        - port: 9300
          host: 127.0.0.1


- name: Test kibana api
  hosts: elastic-logstash
  gather_facts: false
  become: true

  environment:
    ANSIBLE_LOG_PATH: "/tmp/elk-metrics-6x-logs/ansible-elk-test.log"

  tasks:
    - name: Check http
      uri:
        url: http://127.0.0.1:{{ item.port }}{{ item.path }}
        method: "{{ item.method }}"
        status_code: 200
      register: elk_test
      until: elk_test is success
      retries: 3
      delay: 10
      with_items:
        - port: 5601
          path: "/status"
          method: "HEAD"
        - port: 81
          path: "/status"
          method: "HEAD"


- name: Test logstash api
  hosts: elastic-logstash
  gather_facts: false
  become: true

  environment:
    ANSIBLE_LOG_PATH: "/tmp/elk-metrics-6x-logs/ansible-elk-test.log"

  tasks:
    - name: Check http
      uri:
        url: http://127.0.0.1:{{ item.port }}{{ item.path }}
        method: "{{ item.method }}"
        status_code: 200
      register: elk_test
      until: elk_test is success
      retries: 3
      delay: 10
      with_items:
        - port: 9600
          path: "/_node"
          method: "HEAD"

    - name: Check for open TCP
      wait_for:
        port: "{{ item.port }}"
        host: "{{ item.host }}"
        timeout: 120
      with_items:
        - port: 5044
          host: 127.0.0.1




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\elk_metrics_6x\tests\testLayout.yml
===========File Type===========
.yml
===========File Content===========
---
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- import_playbook: run-setup.yml

- name: Test host layout
  hosts: localhost
  become: true

  environment:
    ANSIBLE_LOG_PATH: "/tmp/elk-metrics-6x-logs/ansible-elk-test.log"

  vars:
    storage_node_count: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
    expected_masters:   [1, 1, 3, 3, 3, 3, 3, 5, 5,  5,  5,  7,  7,  7,  7,  9,  9,  9,  9, 11]

  tasks:

    # the elasticserch cluster elects one master from all those which are marked as master-eligible
    # 1 node cluster can only have one master
    # 2 node clusters have 1 master-eligable nodes to avoid split-brain
    # 3 node clusters have 3 master-eligable nodes
    # >3 node clusters have (nodes // 2) eligable masters rounded up to the next odd number
    - name: Master node count fact
      set_fact:
        master_node_count: |-
          {% set masters = 0 %}
          {% if (item | int) < 3 %}
          {%   set masters = 1 %}
          {% elif (item | int) == 3 %}
          {%   set masters = 3 %}
          {% else %}
          {%   set masters = (item | int ) // 2 %}
          {%   if ((masters | int) % 2 == 0) %}
          {%     set masters = (masters | int) + 1 %}
          {%   endif %}
          {% endif %}
          {{ masters }}
      with_items: "{{ storage_node_count }}"
      register: computed_masters

    - name: Gather results
      set_fact:
        masters: "{{ masters|default([]) + [ { 'nodes': item.item, 'masters': (item.ansible_facts.master_node_count|int) } ] }}"
      with_items: "{{ computed_masters.results }}"

    - name: Check results
      assert:
        that: item.0 == item.1.masters
      with_together:
        - "{{ expected_masters }}"
        - "{{ masters }}"




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\elk_metrics_6x\tests\_container-setup.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Setup host for nspawn
  hosts: localhost
  connection: local
  become: true
  vars:
    nspawn_networks:
      nspawn_address:
        bridge: "nspawn0"
        private_device: true
        enable_dhcp: true
        dhcp_range: 10.100.101.2,10.100.101.129
        address: 10.100.101.1
        netmask: 255.255.255.0
        macvlan_mode: bridge

  pre_tasks:
    - name: Ensure root ssh key
      user:
        name: "{{ ansible_env.USER | default('root') }}"
        generate_ssh_key: "yes"
        ssh_key_bits: 2048
        ssh_key_file: ".ssh/id_rsa"

    - name: Get root ssh key
      slurp:
        src: '~/.ssh/id_rsa.pub'
      register: _root_ssh_key

    - name: Prepare container ssh key fact
      set_fact:
        nspawn_container_ssh_key: "{{ _root_ssh_key['content'] | b64decode }}"

    - name: Ensure public ssh key is in authorized_keys
      authorized_key:
        user: "{{ ansible_env.USER | default('root') }}"
        key: "{{ nspawn_container_ssh_key }}"
        manage_dir: no

  roles:
    - role: "nspawn_hosts"


- name: Create container(s)
  hosts: all_containers
  gather_facts: false
  become: true
  pre_tasks:
    - name: Show container facts
      debug:
        var: hostvars

  roles:
    - role: "nspawn_container_create"

  post_tasks:
    - name: Rescan quotas
      command: "btrfs quota rescan -w /var/lib/machines"
      delegate_to: "{{ physical_host }}"




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\elk_metrics_6x\tests\_key-setup.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Setup host keys
  hosts: physical_hosts
  connection: local
  become: true
  tasks:
    - name: Ensure root ssh key
      user:
        name: "{{ ansible_env.USER | default('root') }}"
        generate_ssh_key: "yes"
        ssh_key_bits: 2048
        ssh_key_file: ".ssh/id_rsa"

    - name: Get root ssh key
      slurp:
        src: '~/.ssh/id_rsa.pub'
      register: _root_ssh_key

    - name: Prepare container ssh key fact
      set_fact:
        nspawn_container_ssh_key: "{{ _root_ssh_key['content'] | b64decode }}"

    - name: Ensure public ssh key is in authorized_keys
      authorized_key:
        user: "{{ ansible_env.USER | default('root') }}"
        key: "{{ nspawn_container_ssh_key }}"
        manage_dir: no




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\elk_metrics_6x\tests\inventory\test-container-inventory.yml
===========File Type===========
.yml
===========File Content===========
---
# The hosts group is used to target physical host machines. Enter all physical
# host machines here.
hosts:
  children:
    physical_hosts:
      hosts:
        localhost:
          ansible_host: 127.0.0.1
          ansible_user: root
      vars:
        physical_host: localhost
        management_cidr: "172.29.236.0/24"
        container_networks:
          management_address:
            address: "172.29.236.1"
            netmask: "255.255.255.0"
            bridge: "{{ hostvars[physical_host]['ansible_default_ipv4']['alias'] }}"


all_containers:
  vars:
    physical_host: localhost
    container_tech: nspawn
    container_networks:
      management_address:
        address: "{{ ansible_host }}"
        netmask: "255.255.255.0"
        bridge: "{{ hostvars[physical_host]['ansible_default_ipv4']['alias'] }}"
    # CI nodes havee limited resources, locking the memory is impossible.
    elastic_memory_lock: false

  children:
    elastic-logstash:
      children:
        kibana:
          hosts:
            elastic0:
              ansible_host: 172.29.236.100
              ansible_user: root

            elastic1:
              ansible_host: 172.29.236.101
              ansible_user: root

            elastic2:
              ansible_host: 172.29.236.102
              ansible_user: root

    apm-server:
      hosts:
        apm0:
          ansible_host: 172.29.236.120
          ansible_user: root




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\elk_metrics_6x\tests\inventory\test-metal-inventory.yml
===========File Type===========
.yml
===========File Content===========
---
# The hosts group is used to target physical host machines. Enter all physical
# host machines here.
hosts:
  children:
    physical_hosts:
      hosts:
        localhost:
          ansible_host: 127.0.0.1
          ansible_user: root
      vars:
        physical_host: localhost

    elastic-logstash:
      hosts:
        localhost: {}

    kibana:
      hosts:
        localhost: {}

    apm-server:
      hosts:
        localhost: {}




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\osquery\tests\ansible-role-requirements.yml
===========File Type===========
.yml
===========File Content===========
---
- name: apt_package_pinning
  scm: git
  src: https://git.openstack.org/openstack/openstack-ansible-apt_package_pinning
  version: master
- name: config_template
  scm: git
  src: https://git.openstack.org/openstack/ansible-config_template
  version: master
- name: nspawn_container_create
  scm: git
  src: https://git.openstack.org/openstack/openstack-ansible-nspawn_container_create
  version: master
- name: nspawn_hosts
  scm: git
  src: https://git.openstack.org/openstack/openstack-ansible-nspawn_hosts
  version: master
- name: plugins
  scm: git
  src: https://git.openstack.org/openstack/openstack-ansible-plugins
  version: master
- name: systemd_mount
  scm: git
  src: https://git.openstack.org/openstack/ansible-role-systemd_mount
  version: master
- name: systemd_networkd
  scm: git
  src: https://git.openstack.org/openstack/ansible-role-systemd_networkd
  version: master
- name: systemd_service
  scm: git
  src: https://git.openstack.org/openstack/ansible-role-systemd_service
  version: master




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\osquery\tests\functional.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- import_playbook: run-setup.yml

- name: Basic setup
  hosts: "all"
  become: true

  environment:
    # ZUUL_PROJECT is used by tests/get-ansible-role-requirements to
    # determine when CI provided repos should be used.
    ZUUL_PROJECT: "{{ zuul.project.short_name }}"
    ANSIBLE_PACKAGE: "{{ ansible_package | default('') }}"
    ANSIBLE_HOST_KEY_CHECKING: "False"
    ANSIBLE_LOG_PATH: "/tmp/osquery-logs/ansible-osquery-test.log"

  vars:
    inventory_file: "inventory/test-{{ (contianer_inventory | bool) | ternary('container', 'metal') }}-inventory.yml"

  pre_tasks:
    - name: Create swap file
      command: "dd if=/dev/zero of=/swap.img bs=1M count=4096"
      args:
        creates: /swap.img
      register: swap_create

    - name: Format the swap file
      command: mkswap /swap.img
      when:
        - swap_create is changed
      tags:
        - swap-format
        - skip_ansible_lint

    - name: Enable swap file
      command: swapon /swap.img
      failed_when: false
      tags:
        - swap-format
        - skip_ansible_lint

    - name: Set system swappiness
      sysctl:
        name: vm.swappiness
        value: 10
        state: present
        reload: "yes"
        sysctl_file: /etc/sysctl.d/99-osquery.conf

    - name: Create tmp osquery dir
      file:
        path: "/tmp/osquery-logs"
        state: directory

    - name: Flush iptables rules
      command: "{{ item }}"
      args:
        creates: "/tmp/osquery-logs/iptables.flushed"
      with_items:
        - "iptables -F"
        - "iptables -X"
        - "iptables -t nat -F"
        - "iptables -t nat -X"
        - "iptables -t mangle -F"
        - "iptables -t mangle -X"
        - "iptables -P INPUT ACCEPT"
        - "iptables -P FORWARD ACCEPT"
        - "iptables -P OUTPUT ACCEPT"
        - "touch /tmp/osquery-logs/iptables.flushed"

    - name: First ensure apt cache is always refreshed
      apt:
        update_cache: yes
      when:
        - ansible_pkg_mgr == 'apt'

  tasks:
    - name: Run embedded ansible installation
      become: yes
      become_user: root
      command: "./bootstrap-embedded-ansible.sh"
      args:
        chdir: "src/{{ current_test_repo }}/osquery"

    - name: Run ansible-galaxy (tests)
      become: yes
      become_user: root
      command: "${HOME}/ansible_venv/bin/ansible-galaxy install --force --ignore-errors --roles-path=${HOME}/ansible_venv/repositories/roles -r ansible-role-requirements.yml"
      args:
        chdir: "src/{{ current_test_repo }}/osquery/tests"

    - name: Run ansible-galaxy (osquery)
      become: yes
      become_user: root
      command: "${HOME}/ansible_venv/bin/ansible-galaxy install --force --ignore-errors --roles-path=${HOME}/ansible_venv/repositories/roles -r ansible-role-requirements.yml"
      args:
        chdir: "src/{{ current_test_repo }}/osquery"

    - name: Run environment setup
      become: yes
      become_user: root
      command: "${HOME}/ansible_venv/bin/ansible-playbook -i {{ inventory_file }} -e @test-vars.yml _key-setup.yml"
      environment:
        ANSIBLE_ACTION_PLUGINS: "${HOME}/ansible_venv/repositories/ansible-config_template/action"
        ANSIBLE_CONNECTION_PLUGINS: "${HOME}/ansible_venv/repositories/openstack-ansible-plugins/connection"
        ANSIBLE_LOG_PATH: "/tmp/osquery-logs/ansible-osquery-test-container-setup.log"
        ANSIBLE_ROLES_PATH: "${HOME}/ansible_venv/repositories/roles"
      args:
        chdir: "src/{{ current_test_repo }}/osquery/tests"
      when:
        - ansible_service_mgr != 'systemd' or
          not (contianer_inventory | bool)

    - name: Run environment setup
      become: yes
      become_user: root
      command: "${HOME}/ansible_venv/bin/ansible-playbook -i {{ inventory_file }} -e @test-vars.yml _container-setup.yml"
      environment:
        ANSIBLE_ACTION_PLUGINS: "${HOME}/ansible_venv/repositories/ansible-config_template/action"
        ANSIBLE_CONNECTION_PLUGINS: "${HOME}/ansible_venv/repositories/openstack-ansible-plugins/connection"
        ANSIBLE_LOG_PATH: "/tmp/osquery-logs/ansible-osquery-test-container-setup.log"
        ANSIBLE_ROLES_PATH: "${HOME}/ansible_venv/repositories/roles"
      args:
        chdir: "src/{{ current_test_repo }}/osquery/tests"
      when:
        - ansible_service_mgr == 'systemd'
        - contianer_inventory | bool

    - name: Wait 15 seconds
      command: "sleep 15"
      changed_when: false
      when:
        - ansible_service_mgr == 'systemd'

    - name: Run functional test
      become: yes
      become_user: root
      command: "${HOME}/ansible_venv/bin/ansible-playbook -i tests/{{ inventory_file }} -e @tests/test-vars.yml site.yml"
      environment:
        ANSIBLE_ACTION_PLUGINS: "${HOME}/ansible_venv/repositories/ansible-config_template/action"
        ANSIBLE_CONNECTION_PLUGINS: "${HOME}/ansible_venv/repositories/openstack-ansible-plugins/connection"
        ANSIBLE_LOG_PATH: "/tmp/osquery-logs/ansible-osquery-test-deployment.log"
        ANSIBLE_ROLES_PATH: "${HOME}/ansible_venv/repositories/roles"
      args:
        chdir: "src/{{ current_test_repo }}/osquery"




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\osquery\tests\manual-test.rc
===========File Type===========
.rc
===========File Content===========
export ANSIBLE_HOST_KEY_CHECKING="False"
export ANSIBLE_ROLES_PATH="${HOME}/ansible_venv/repositories/roles"
export ANSIBLE_ACTION_PLUGINS="${HOME}/ansible_venv/repositories/roles/config_template/action"
export ANSIBLE_CONNECTION_PLUGINS="${HOME}/ansible_venv/repositories/roles/plugins/connection"
export ANSIBLE_LOG_PATH="/tmp/osquery-logs/ansible-elk-test.log"

if [[ ! -d "/tmp/osquery-logs" ]]; then
  mkdir -pv "/tmp/osquery-logs"
  chmod 0777 "/tmp/osquery-logs"
fi

echo "To build a test environment run the following:"
echo -e "# ${HOME}/ansible_venv/bin/ansible-playbook -i tests/inventory/test-container-inventory.yml tests/test.yml --limit localhost\n"

echo "Run manual functional tests by executing the following:"
echo -e "# ${HOME}/ansible_venv/bin/ansible-playbook -i tests/inventory/test-container-inventory.yml site.yml\n"




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\osquery\tests\post-run.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- import_playbook: run-setup.yml

- name: Run post tasks
  hosts: "all"
  tasks:
    - name: Copy logs back to the executor
      synchronize:
        src: "/tmp/osquery-logs"
        dest: "{{ zuul.executor.log_root }}/"
        mode: pull
        rsync_opts:
          - "--quiet"




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\osquery\tests\run-cleanup.sh
===========File Type===========
.sh
===========File Content===========
#!/usr/bin/env bash
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

set -e

export TEST_DIR="$(readlink -f $(dirname ${0})/../../)"

# Stop beat processes
(systemctl stop osqueryd.service || true) &

# Stop and remove containers
for i in {1..3}; do
  if machinectl list-images | grep -v ubuntu | awk '/sub/ {print $1}' | xargs -n 1 machinectl kill; then
    sleep 1
  fi
done

for i in {1..3}; do
  if machinectl list-images | grep -v ubuntu | awk '/sub/ {print $1}' | xargs -n 1 machinectl remove; then
    sleep 1
  fi
done




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\osquery\tests\run-setup.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Zuul facts
  hosts: "all"
  tasks:
    - name: Set zuul fact
      set_fact:
        zuul:
          project:
            canonical_name: "openstack-ansible-ops"
            short_name: "ops"
          executor:
            log_root: "{{ ansible_env.HOME }}/elk-test-logs"
      when:
        - zuul is not defined

    - name: Print zuul fact
      debug: var=zuul

    - name: Set current test repo (cross-repo)
      set_fact:
        current_test_repo: "git.openstack.org/{{ osa_test_repo }}"
      when:
        - osa_test_repo is defined

    - name: Set current test repo (non-cross-repo)
      set_fact:
        current_test_repo: "{{ zuul.project.canonical_name }}"
      when:
        - osa_test_repo is not defined

    - name: Set inventory for test
      set_fact:
        contianer_inventory: "{{ test_clustered_kolide | default(false) | bool }}"

  post_tasks:
    - name: Ensure the log directory exists
      file:
        path: "/tmp/osquery-logs"
        state: directory




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\osquery\tests\run-tests.sh
===========File Type===========
.sh
===========File Content===========
#!/usr/bin/env bash
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

set -ve

export TEST_DIR="$(readlink -f $(dirname ${0})/../../)"

pushd "${HOME}"
  if [[ ! -d "src" ]]; then
    mkdir src
  fi
  pushd src
    ln -sf "${TEST_DIR}"
  popd
popd

source "${TEST_DIR}/osquery/tests/manual-test.rc"

source "${TEST_DIR}/osquery/bootstrap-embedded-ansible.sh"
deactivate

${HOME}/ansible_venv/bin/ansible-galaxy install --force \
                                             --roles-path="${HOME}/ansible_venv/repositories/roles" \
                                             --role-file="${TEST_DIR}/osquery/tests/ansible-role-requirements.yml"

if [[ ! -e "${TEST_DIR}/osquery/tests/src" ]]; then
  ln -s ${TEST_DIR}/../ ${TEST_DIR}/osquery/tests/src
fi

${HOME}/ansible_venv/bin/ansible-playbook -i 'localhost,' \
                                       -vv \
                                       -e ansible_connection=local \
                                       -e test_clustered_kolide=${CLUSTERED:-no} \
                                       ${TEST_DIR}/osquery/tests/test.yml




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\osquery\tests\test-vars.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

physical_host: localhost

galera_root_password: secrete
kolide_fleet_db_password: secrete
kolide_fleet_admin_password: secrete
kolide_fleet_jwt_key: secrete

osa_test_repo: "openstack/openstack-ansible-ops"




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\osquery\tests\test.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- import_playbook: functional.yml




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\osquery\tests\_container-setup.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Setup host for nspawn
  hosts: localhost
  connection: local
  become: true
  vars:
    nspawn_networks:
      nspawn_address:
        bridge: "nspawn0"
        private_device: true
        enable_dhcp: true
        dhcp_range: 10.100.101.2,10.100.101.129
        address: 10.100.101.1
        netmask: 255.255.255.0
        macvlan_mode: bridge

  pre_tasks:
    - name: Ensure root ssh key
      user:
        name: "{{ ansible_env.USER | default('root') }}"
        generate_ssh_key: "yes"
        ssh_key_bits: 2048
        ssh_key_file: ".ssh/id_rsa"

    - name: Get root ssh key
      slurp:
        src: '~/.ssh/id_rsa.pub'
      register: _root_ssh_key

    - name: Prepare container ssh key fact
      set_fact:
        nspawn_container_ssh_key: "{{ _root_ssh_key['content'] | b64decode }}"

    - name: Ensure public ssh key is in authorized_keys
      authorized_key:
        user: "{{ ansible_env.USER | default('root') }}"
        key: "{{ nspawn_container_ssh_key }}"
        manage_dir: no

  roles:
    - role: "nspawn_hosts"


- name: Create container(s)
  hosts: all_containers
  gather_facts: false
  become: true
  pre_tasks:
    - name: Show container facts
      debug:
        var: hostvars

  roles:
    - role: "nspawn_container_create"

  post_tasks:
    - name: Rescan quotas
      command: "btrfs quota rescan -w /var/lib/machines"
      delegate_to: "{{ physical_host }}"




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\osquery\tests\_key-setup.yml
===========File Type===========
.yml
===========File Content===========
---
# Copyright 2018, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Setup host keys
  hosts: physical_hosts
  connection: local
  become: true
  tasks:
    - name: Ensure root ssh key
      user:
        name: "{{ ansible_env.USER | default('root') }}"
        generate_ssh_key: "yes"
        ssh_key_bits: 2048
        ssh_key_file: ".ssh/id_rsa"

    - name: Get root ssh key
      slurp:
        src: '~/.ssh/id_rsa.pub'
      register: _root_ssh_key

    - name: Prepare container ssh key fact
      set_fact:
        nspawn_container_ssh_key: "{{ _root_ssh_key['content'] | b64decode }}"

    - name: Ensure public ssh key is in authorized_keys
      authorized_key:
        user: "{{ ansible_env.USER | default('root') }}"
        key: "{{ nspawn_container_ssh_key }}"
        manage_dir: no




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\osquery\tests\inventory\test-container-inventory.yml
===========File Type===========
.yml
===========File Content===========
---
all:
  hosts:
    # Local host
    localhost:
      ansible_connection: local
      ansible_host: 127.0.0.1
      ansible_user: root

    kolide-fleet0:
      ansible_host: 172.29.236.100
      ansible_user: root

    kolide-fleet1:
      ansible_host: 172.29.236.101
      ansible_user: root

    kolide-fleet2:
      ansible_host: 172.29.236.102
      ansible_user: root


hosts:
  vars:
    physical_host: localhost
    management_cidr: "172.29.236.0/24"
    container_networks:
      management_address:
        address: "172.29.236.1"
        netmask: "255.255.255.0"
        bridge: "{{ hostvars[physical_host]['ansible_default_ipv4']['alias'] }}"

  hosts:
    localhost: {}


all_containers:
  vars:
    physical_host: localhost
    container_tech: nspawn
    container_networks:
      management_address:
        address: "{{ ansible_host }}"
        netmask: "255.255.255.0"
        bridge: "{{ hostvars[physical_host]['ansible_default_ipv4']['alias'] }}"

  children:
    mariadb_all:
      children:
        mariadb:
          hosts:
            kolide-fleet0: {}
            kolide-fleet1: {}
            kolide-fleet2: {}

    fleet_all:
      children:
        kolide-fleet_all:
          children:
            kolide-fleet:
              hosts:
                kolide-fleet0: {}
                kolide-fleet1: {}
                kolide-fleet2: {}




===========Repository Name===========
openstack-ansible-ops
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\openstack-ansible-ops\osquery\tests\inventory\test-metal-inventory.yml
===========File Type===========
.yml
===========File Content===========
---
all:
  hosts:
    # Local host
    localhost:
      ansible_connection: local
      ansible_host: 127.0.0.1
      ansible_user: root

hosts:
  hosts:
    localhost: {}


mariadb_all:
  children:
    mariadb:
      hosts:
        localhost: {}


fleet_all:
  children:
    kolide-fleet_all:
      children:
        kolide-fleet:
          hosts:
            localhost: {}




===========Repository Name===========
tripleo-quickstart
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\tripleo-quickstart\tox.ini
===========File Type===========
.ini
===========File Content===========
[tox]
minversion = 1.6
envlist = docs, linters
skipdist = True

[testenv]
usedevelop = True
install_command = pip install -c{env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt} {opts} {packages}
setenv = VIRTUAL_ENV={envdir}
deps = -r{toxinidir}/test-requirements.txt
whitelist_externals =
    bash
    echo

[testenv:bindep]
basepython = python3
# Do not install any requirements. We want this to be fast and work even if
# system dependencies are missing, since it's used to tell you what system
# dependencies are missing! This also means that bindep must be installed
# separately, outside of the requirements files.
deps = bindep
commands = bindep test

[testenv:docs]
basepython = python3
commands = python setup.py build_sphinx

[testenv:bashate]
commands =
# Run bashate check for all bash scripts
# Ignores the following rules:
# E006: Line longer than 79 columns (as many scripts use jinja
#       templating, this is very difficult)
# E040: Syntax error determined using `bash -n` (as many scripts
#       use jinja templating, this will often fail and the syntax
#       error will be discovered in execution anyway)
    bash -c "git ls-files | xargs grep --binary-files=without-match \
        --files-with-match '^.!.*\(ba\)\?sh$' \
        --exclude-dir .tox \
        --exclude-dir .git \
        | xargs bashate --error . --verbose --ignore=E006,E040"

[testenv:pep8]
basepython = python3
commands =
    # Run hacking/flake8 check for all python files
    bash -c "git ls-files | xargs grep --binary-files=without-match \
        --files-with-match '^.!.*python$' \
        --exclude-dir .tox \
        --exclude-dir .git \
        --exclude-dir .eggs \
        --exclude-dir *.egg-info \
        --exclude-dir dist \
        --exclude-dir *lib/python* \
        --exclude-dir doc \
        | xargs flake8 --verbose"

[testenv:ansible-lint]
basepython=python2
commands =
  bash ci-scripts/ansible-lint.sh

[testenv:linters]
basepython = python3
commands =
    python -m yamllint .
    {[testenv:bashate]commands}
    {[testenv:pep8]commands}
    {[testenv:ansible-lint]commands}

[testenv:releasenotes]
basepython = python3
whitelist_externals = bash
commands = bash -c ci-scripts/releasenotes_tox.sh

[testenv:venv]
changedir = {toxinidir}
commands =
    {posargs:echo done}

[flake8]
# E123, E125 skipped as they are invalid PEP-8.
show-source = True
ignore = E123,E125
builtins = _




===========Repository Name===========
tripleo-quickstart
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\tripleo-quickstart\roles\tripleo-inventory\tests\inventory
===========File Type===========

===========File Content===========
localhost




===========Repository Name===========
tripleo-quickstart
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\tripleo-quickstart\roles\tripleo-inventory\tests\test.yml
===========File Type===========
.yml
===========File Content===========
---
- hosts: localhost
  remote_user: root
  roles:
    - tripleo-inventory





===========Repository Name===========
tripleo-quickstart
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\tripleo-quickstart\roles\tripleo-inventory\tests\playbooks\quickstart-usb.yml
===========File Type===========
.yml
===========File Content===========
# This is the playbook used by the `quickstart.sh` script.

# The [provision.yml](provision.yml.html) playbook is responsible for
# creating an inventory entry for our `virthost` and for creating an
# unprivileged user on that host for use by our virtual environment.
- include_tasks: provision.yml
  tags:
    - provision

# The `environment/setup` role performs any tasks that require `root`
# access on the target host.
- name: Install libvirt packages and configure networks
  hosts: virthost
  tags:
    - environment
  roles:
    - environment/setup

# The `libvirt/setup` role creates the undercloud and overcloud
# virtual machines.
- name:  Setup undercloud and overcloud vms
  hosts: virthost
  gather_facts: yes
  roles:
    - libvirt/teardown
    - libvirt/setup

# Add the undercloud node to the generated
# inventory.
- name:  Inventory the undercloud
  hosts: undercloud
  gather_facts: no
  vars:
      inventory: undercloud
  roles:
    - tripleo-inventory

# DEPLOY ALL THE THINGS!  Depending on the currently selected set of
# tags, this will deploy the undercloud, deploy the overcloud, and
# perform some validation tests on the overcloud.
- name:  Install undercloud and deploy overcloud
  hosts: undercloud
  gather_facts: no
  roles:
    - tripleo/undercloud
    - tripleo/overcloud





===========Repository Name===========
windmill
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\windmill\tox.ini
===========File Type===========
.ini
===========File Content===========
[tox]
minversion = 1.4.2
envlist = docs,linters
skipsdist = True

[testenv]
deps = -r{toxinidir}/requirements.txt
       -r{toxinidir}/test-requirements.txt

[testenv:bindep]
basepython = python3
# Do not install any requirements. We want this to be fast and work even if
# system dependencies are missing, since it's used to tell you what system
# dependencies are missing! This also means that bindep must be installed
# separately, outside of the requirements files.
deps = bindep
commands =
  {toxinidir}/tools/install_bindep.sh

[testenv:docs]
basepython = python3
commands = python setup.py build_sphinx

[testenv:linters]
basepython = python3
whitelist_externals = bash
commands =
  flake8
  bash -c "cd playbooks; find . -type f -regex '.*.y[a]?ml' -print0 | xargs -t -n1 -0 \
    ansible-lint"

[testenv:venv]
basepython = python3
commands = {posargs}
passenv =
  HOME
  SSH_AUTH_SOCK
  TERM
  USER
setenv =
  ANSIBLE_CALLBACK_PLUGINS = {envsitepackagesdir}/ara/plugins/callbacks
  ANSIBLE_CONFIG = {toxinidir}/tests/ansible.cfg
  PYTHONUNBUFFERED = 1

[flake8]
# E123, E125 skipped as they are invalid PEP-8.

show-source = True
ignore = E123,E125
builtins = _
exclude=.venv,.git,.tox,dist,doc,*openstack/common*,*lib/python*,*egg,build




===========Repository Name===========
windmill
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\windmill\tests\ansible.cfg
===========File Type===========
.cfg
===========File Content===========
[defaults]
callback_whitelist = profile_tasks, timer

[ssh_connection]
# NOTE(pabelanger): Enable pipelining to deal with becomes issues:
# http://docs.ansible.com/ansible/become.html#becoming-an-unprivileged-user
pipelining = True




===========Repository Name===========
windmill
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\windmill\tests\collect-logs.yaml
===========File Type===========
.yaml
===========File Content===========
- hosts: all,!bastion
  tasks:
    - name: Ensure journald logs directory exists
      file:
        path: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal"
        state: directory

- hosts: statsd01
  tasks:
    - name: Collect journald logs
      shell: "sudo journalctl -u {{ item }}.service | tee {{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      args:
        creates: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      with_items:
        - statsd

    - name: Collect statsd log files
      become: yes
      synchronize:
        dest: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}"
        rsync_opts:
          - "--relative"
          - "--chown={{ ansible_user_id }}:{{ ansible_user_id }}"
        src: "{{ item }}"
        verify_host: true
      delegate_to: "{{ inventory_hostname }}"
      with_items:
        - /etc/statsd
        - /var/log/statsd

- hosts: gear01
  tasks:
    - name: Collect journald logs
      shell: "sudo journalctl -u {{ item }}.service | tee {{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      args:
        creates: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      with_items:
        - gear

    - name: Collect gear log files
      become: yes
      synchronize:
        dest: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}"
        rsync_opts:
          - "--relative"
          - "--chown={{ ansible_user_id }}:{{ ansible_user_id }}"
        src: "{{ item }}"
        verify_host: true
      delegate_to: "{{ inventory_hostname }}"
      with_items:
        - /etc/gear/logging.conf
        - /var/log/gear

- hosts: nb01
  tasks:
    - name: Collect journald logs
      shell: "sudo journalctl -u {{ item }}.service | tee {{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      args:
        creates: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      with_items:
        - nodepool-builder

    - name: Collect nodepool-builder log files
      become: yes
      synchronize:
        dest: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}"
        rsync_opts:
          - "--relative"
          - "--chown={{ ansible_user_id }}:{{ ansible_user_id }}"
        src: "{{ item }}"
        verify_host: true
      delegate_to: "{{ inventory_hostname }}"
      with_items:
        - /etc/nodepool/builder-logging.conf
        - /etc/nodepool/nodepool.yaml
        - /var/log/nodepool/builds
        - /var/log/nodepool/builder-debug.log
        - /var/log/nodepool/nodepool-builder.log

- hosts: nl01
  tasks:
    - name: Collect journald logs
      shell: "sudo journalctl -u {{ item }}.service | tee {{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      args:
        creates: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      with_items:
        - nodepool-launcher

    - name: Collect nodepool-launcher log files
      become: yes
      synchronize:
        dest: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}"
        rsync_opts:
          - "--relative"
          - "--chown={{ ansible_user_id }}:{{ ansible_user_id }}"
        src: "{{ item }}"
        verify_host: true
      delegate_to: "{{ inventory_hostname }}"
      with_items:
        - /etc/nodepool/launcher-logging.conf
        - /etc/nodepool/nodepool.yaml
        - /var/log/nodepool/launcher-debug.log
        - /var/log/nodepool/nodepool-launcher.log

- hosts: zs01
  tasks:
    - name: Collect journald logs
      shell: "sudo journalctl -u {{ item }}.service | tee {{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      args:
        creates: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      with_items:
        - zuul-scheduler

    - name: Collect zuul-scheduler log files
      become: yes
      synchronize:
        dest: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}"
        rsync_opts:
          - "--relative"
          - "--chown={{ ansible_user_id }}:{{ ansible_user_id }}"
        src: "{{ item }}"
        verify_host: true
      delegate_to: "{{ inventory_hostname }}"
      with_items:
        - /etc/zuul/main.yaml
        - /etc/zuul/scheduler-logging.conf
        - /etc/zuul/zuul.conf
        - /var/log/zuul/scheduler-debug.log
        - /var/log/zuul/scheduler.log

- hosts: ze01
  tasks:
    - name: Collect journald logs
      shell: "sudo journalctl -u {{ item }}.service | tee {{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      args:
        creates: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      with_items:
        - zuul-executor

    - name: Collect zuul-executor log files
      become: yes
      synchronize:
        dest: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}"
        rsync_opts:
          - "--relative"
          - "--chown={{ ansible_user_id }}:{{ ansible_user_id }}"
        src: "{{ item }}"
        verify_host: true
      delegate_to: "{{ inventory_hostname }}"
      with_items:
        - /etc/zuul/executor-logging.conf
        - /etc/zuul/zuul.conf
        - /var/log/zuul/executor-debug.log
        - /var/log/zuul/executor.log

- hosts: zf01
  tasks:
    - name: Collect journald logs
      shell: "sudo journalctl -u {{ item }}.service | tee {{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      args:
        creates: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      with_items:
        - zuul-fingergw

    - name: Collect zuul-fingergw log files
      become: yes
      synchronize:
        dest: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}"
        rsync_opts:
          - "--relative"
          - "--chown={{ ansible_user_id }}:{{ ansible_user_id }}"
        src: "{{ item }}"
        verify_host: true
      delegate_to: "{{ inventory_hostname }}"
      with_items:
        - /etc/zuul/fingergw-logging.conf
        - /etc/zuul/zuul.conf
        - /var/log/zuul/fingergw-debug.log
        - /var/log/zuul/fingergw.log

- hosts: zm01
  tasks:
    - name: Collect journald logs
      shell: "sudo journalctl -u {{ item }}.service | tee {{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      args:
        creates: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      with_items:
        - zuul-merger

    - name: Collect zuul-merger log files
      become: yes
      synchronize:
        dest: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}"
        rsync_opts:
          - "--relative"
          - "--chown={{ ansible_user_id }}:{{ ansible_user_id }}"
        src: "{{ item }}"
        verify_host: true
      delegate_to: "{{ inventory_hostname }}"
      with_items:
        - /etc/zuul/merger-logging.conf
        - /etc/zuul/zuul.conf
        - /var/log/zuul/merger-debug.log
        - /var/log/zuul/merger.log

- hosts: zw01
  tasks:
    - name: Collect journald logs
      shell: "sudo journalctl -u {{ item }}.service | tee {{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      args:
        creates: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}/var/log/journal/{{ item }}.service.log"
      with_items:
        - zuul-web

    - name: Collect zuul-web log files
      become: yes
      synchronize:
        dest: "{{ zuul_output_dir }}/logs/logs/{{ inventory_hostname }}"
        rsync_opts:
          - "--relative"
          - "--chown={{ ansible_user_id }}:{{ ansible_user_id }}"
        src: "{{ item }}"
        verify_host: true
      delegate_to: "{{ inventory_hostname }}"
      with_items:
        - /etc/zuul/web-logging.conf
        - /etc/zuul/zuul.conf
        - /var/log/zuul/web-debug.log
        - /var/log/zuul/web.log




===========Repository Name===========
windmill
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\windmill\tests\extra-vars\git.yaml
===========File Type===========
.yaml
===========File Content===========
# NOTE(pabelanger): Because we want to test Depends-On patches in the
# gate, we don't want our roles to update git after we pushed the repos
# from zuul-executors.
diskimage_builder_git_update: false
diskimage_builder_install_method: git

gear_git_update: false
gear_install_method: git

nodepool_git_update: false
nodepool_install_method: git

openstacksdk_git_update: false
openstacksdk_install_method: git

zuul_git_update: false
zuul_install_method: git

# NOTE(pabelanger): In the gate, we want to create a zuul-test user /
# group as zuul already exists on our images.
zuul_user_name: zuul-test
zuul_user_group: zuul-test
zuul_user_home: /var/lib/zuul




===========Repository Name===========
windmill
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\windmill\tests\extra-vars\pip.yaml
===========File Type===========
.yaml
===========File Content===========
# NOTE(pabelanger): In the gate, we want to create a zuul-test user /
# group as zuul already exists on our images.
zuul_user_name: zuul-test
zuul_user_group: zuul-test
zuul_user_home: /var/lib/zuul




===========Repository Name===========
windmill
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\windmill\tests\playbooks\post.yaml
===========File Type===========
.yaml
===========File Content===========
- hosts: all
  tasks:
    - name: Collect tox logs
      synchronize:
        dest: "{{ zuul.executor.log_root }}/tox"
        mode: pull
        src: "{{ windmill_src_dir }}/.tox/venv/log/"
        verify_host: true

    - name: Collect logs from hosts
      block:
        - name: Run ansible-playbook for collect-logs.yaml
          args:
            chdir: "{{ windmill_src_dir }}"
          shell: "tox -evenv -- ansible-playbook -i inventory/testing/hosts -ezuul_output_dir={{ zuul_output_dir }} tests/collect-logs.yaml"

      always:
        - name: Ensure ara-report directory exists
          file:
            path: "{{ zuul_output_dir }}/logs/logs/ara-report"
            state: directory

        - name: Copy ARA database to ara-report directory
          shell: "cp ~/.ara/ansible.sqlite {{ zuul_output_dir }}/logs/logs/ara-report"

        # TODO: Migrate to fetch-zuul-logs when
        # https://review.openstack.org/#/c/583346/ is merged.
        - name: Collect log output
          synchronize:
            dest: "{{ zuul.executor.log_root }}/"
            mode: pull
            src: "{{ zuul_output_dir }}/logs/"
            verify_host: true




===========Repository Name===========
windmill
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\windmill\tests\playbooks\pre.yaml
===========File Type===========
.yaml
===========File Content===========
- hosts: all
  tasks:
    # TODO(pabelanger): Remove once this lands in our base job in
    # project-config.
    - name: Execute ensure-output-dirs role
      include_role:
        name: ensure-output-dirs

    - name: Disable extra wheels mirror
      become: yes
      lineinfile:
        dest: /etc/pip.conf
        regexp: ^extra-index-url
        state: absent

    - name: Bootstrap bindep environment
      args:
        chdir: "{{ windmill_src_dir }}"
      command: tox -ebindep

    - name: Bootstrap tox environment
      args:
        chdir: "{{ windmill_src_dir }}"
      command: tox -evenv --notest

    - name: Install ansible roles via galaxy
      args:
        chdir: "{{ windmill_src_dir }}"
        executable: /bin/bash
      shell: source .tox/venv/bin/activate; ./tools/install_roles.sh




===========Repository Name===========
windmill
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\windmill\tests\playbooks\run.yaml
===========File Type===========
.yaml
===========File Content===========
- hosts: all
  tasks:
    - name: Bootstrap bastion node using ansible
      args:
        chdir: "{{ windmill_src_dir }}"
      shell: tox -evenv -- ansible-playbook -i inventory/testing/hosts playbooks/bastion.yaml

    - name: Run ansible-playbook for site.yaml
      args:
        chdir: "{{ windmill_src_dir }}"
      shell: "tox -evenv -- ansible-playbook -i inventory/testing/hosts playbooks/site.yaml -e @{{ windmill_extra_vars_file }}"

    - name: Run ansible-playbook for prove.yaml
      args:
        chdir: "{{ windmill_src_dir }}"
      shell: tox -evenv -- ansible-playbook -i inventory/testing/hosts playbooks/prove.yaml




===========Repository Name===========
windmill
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\windmill\tests\playbooks\group_vars\all.yaml
===========File Type===========
.yaml
===========File Content===========
---
windmill_src_dir: "{{ zuul.projects['git.openstack.org/openstack/windmill'].src_dir }}"




===========Repository Name===========
windmill
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\windmill\tests\playbooks\molecule\post.yaml
===========File Type===========
.yaml
===========File Content===========
- hosts: all
  tasks:
    - name: Ensure ara-report directory exists
      file:
        path: "{{ zuul_output_dir }}/logs/logs/ara-report"
        state: directory

    - name: Copy ARA database to ara-report directory
      shell: "cp ~/.ara/ansible.sqlite {{ zuul_output_dir }}/logs/logs/ara-report"

    # TODO: Migrate to fetch-zuul-logs when
    # https://review.openstack.org/#/c/583346/ is merged.
    - name: Collect log output
      synchronize:
        dest: "{{ zuul.executor.log_root }}/"
        mode: pull
        src: "{{ zuul_output_dir }}/logs/"
        verify_host: true




===========Repository Name===========
windmill
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\windmill\tests\playbooks\molecule\pre.yaml
===========File Type===========
.yaml
===========File Content===========
- hosts: all
  tasks:
    # TODO(pabelanger): Remove once this lands in our base job in
    # project-config.
    - name: Execute ensure-output-dirs role
      include_role:
        name: ensure-output-dirs

    - name: Reset SSH connection for new group
      meta: reset_connection




===========Repository Name===========
windmill
===========File Path===========
C:\Personal\Online Courses\IaC\OSTK_ANSI.tar\ostk-ansi\windmill\tests\playbooks\windmill-tox-with-sudo\run.yaml
===========File Type===========
.yaml
===========File Content===========
- hosts: all
  roles:
    - tox




